{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a CNN architecture\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        # Input: 3 channels (RGB), Output: 16 feature maps, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        # Input: 16 feature maps, Output: 32 feature maps, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Assuming input images are 32x32, after two 2x2 pooling layers, we have 32/2/2 = 8\n",
    "        # So the feature maps are 8x8 with 32 channels: 32 * 8 * 8 = 2048\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes (e.g., for CIFAR-10 dataset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the feature maps\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.fc2(self.relu4(self.fc1(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2. Create an instance of the model\n",
    "model = BasicCNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.04, momentum=0.5)\n",
    "\n",
    "# 4. Load and preprocess data (example with CIFAR-10)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "    \"../../data/processed/train_augmented_rotated\", transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.ImageFolder(\"../../data/raw/valid/\", transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=32, shuffle=True, num_workers=2\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=32, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, max_iter: int = 10, patience: int = 2, min_delta: float = 0):\n",
    "        self.max_iter = max_iter\n",
    "        self.min_error = float(\"Inf\")\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "\n",
    "    def check(self, val_error: float, model):\n",
    "\n",
    "        if self.counter >= self.max_iter:\n",
    "            return True\n",
    "\n",
    "        if val_error < self.min_error:\n",
    "            self.min_error = val_error\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            self.counter = self.counter + 1\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES = EarlyStopper()\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train(epochs=5):\n",
    "    time_started = time.time()\n",
    "\n",
    "    # Initialize JSON structure\n",
    "    log_data = {\n",
    "        \"metadata\": {\n",
    "            \"model\": str(model),\n",
    "            \"freq_bins\": 64,\n",
    "            \"time_steps\": 64,\n",
    "            \"batch_size\": trainloader.batch_size,\n",
    "            \"train_set_size\": len(trainloader.dataset),\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"loss_function\": str(criterion),\n",
    "            \"num_epochs\": epochs,\n",
    "        },\n",
    "        \"data\": {},\n",
    "    }\n",
    "\n",
    "    # Setup filenames and paths\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    arch = f\"{log_data['metadata']['freq_bins']}x{log_data['metadata']['time_steps']}\"\n",
    "    bsz = log_data[\"metadata\"][\"batch_size\"]\n",
    "    opt = log_data[\"metadata\"][\"optimizer\"]\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    size = log_data[\"metadata\"][\"train_set_size\"]\n",
    "    filename = f\"{arch}x{bsz}x{lr}x{opt}x{size}_{timestamp}.json\"\n",
    "\n",
    "    logs_dir = \"../logs/model_on_rotated_images\"\n",
    "    models_dir = \"../saved_models\"\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(logs_dir, filename)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        log_data[\"data\"][str(epoch + 1)] = {\"batches\": {}}\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            running_loss += batch_loss\n",
    "            correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            total += len(outputs)\n",
    "\n",
    "            if i % 200 == 199:\n",
    "                print(\n",
    "                    f\"[{epoch + 1}, {i + 1}], time {time.time() - time_started:.1f}s, loss: {running_loss / 200:.3f}, acc: {correct/total * 100:.2f}%\"\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_error = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_error += criterion(outputs, labels) * images.size(0)\n",
    "                correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "        acc_val = correct / len(valloader.dataset)\n",
    "\n",
    "        # Save epoch summary\n",
    "        log_data[\"data\"][str(epoch + 1)][\"summary\"] = {\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_error.item(),\n",
    "            \"val_acc\": acc_val,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        }\n",
    "\n",
    "        # Save model for this epoch\n",
    "        model_path = os.path.join(models_dir, f\"model_epoch_{epoch + 1}.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Save updated log file\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(log_data, f, indent=4)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | val loss: {val_error:.4f}, val acc: {acc_val:.4f} | model saved to {model_path}\"\n",
    "        )\n",
    "\n",
    "        if ES.check(val_error, model):\n",
    "            print(\"Early stopping triggered. Finished Training.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Final training log written to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200], time 29.8s, loss: 2.238, acc: 16.08%\n",
      "[1, 400], time 65.0s, loss: 1.973, acc: 21.34%\n",
      "[1, 600], time 96.4s, loss: 1.833, acc: 24.43%\n",
      "[1, 800], time 116.5s, loss: 1.757, acc: 26.93%\n",
      "[1, 1000], time 136.1s, loss: 1.696, acc: 28.96%\n",
      "[1, 1200], time 156.3s, loss: 1.645, acc: 30.65%\n",
      "[1, 1400], time 176.8s, loss: 1.634, acc: 31.88%\n",
      "[1, 1600], time 196.8s, loss: 1.602, acc: 33.01%\n",
      "[1, 1800], time 217.2s, loss: 1.585, acc: 33.86%\n",
      "[1, 2000], time 237.7s, loss: 1.542, acc: 34.79%\n",
      "[1, 2200], time 259.1s, loss: 1.536, acc: 35.55%\n",
      "[1, 2400], time 283.0s, loss: 1.479, acc: 36.40%\n",
      "[1, 2600], time 305.3s, loss: 1.482, acc: 37.06%\n",
      "[1, 2800], time 326.7s, loss: 1.461, acc: 37.71%\n",
      "[1, 3000], time 346.4s, loss: 1.430, acc: 38.32%\n",
      "[1, 3200], time 365.1s, loss: 1.419, acc: 38.95%\n",
      "[1, 3400], time 383.8s, loss: 1.426, acc: 39.43%\n",
      "[1, 3600], time 402.8s, loss: 1.393, acc: 39.98%\n",
      "[1, 3800], time 422.8s, loss: 1.392, acc: 40.45%\n",
      "[1, 4000], time 441.7s, loss: 1.369, acc: 40.95%\n",
      "[1, 4200], time 461.2s, loss: 1.369, acc: 41.35%\n",
      "[1, 4400], time 481.7s, loss: 1.336, acc: 41.82%\n",
      "[1, 4600], time 501.0s, loss: 1.325, acc: 42.23%\n",
      "[1, 4800], time 520.1s, loss: 1.315, acc: 42.64%\n",
      "[1, 5000], time 543.2s, loss: 1.313, acc: 43.02%\n",
      "[1, 5200], time 563.7s, loss: 1.313, acc: 43.33%\n",
      "[1, 5400], time 583.4s, loss: 1.300, acc: 43.69%\n",
      "[1, 5600], time 602.5s, loss: 1.299, acc: 44.01%\n",
      "Epoch 1 | val loss: 1.3121, val acc: 0.5290 | model saved to ../saved_models\\model_epoch_1.pt\n",
      "[2, 200], time 1055.8s, loss: 1.252, acc: 54.66%\n",
      "[2, 400], time 1072.7s, loss: 1.257, acc: 54.59%\n",
      "[2, 600], time 1089.9s, loss: 1.235, acc: 54.90%\n",
      "[2, 800], time 1106.8s, loss: 1.239, acc: 54.87%\n",
      "[2, 1000], time 1125.5s, loss: 1.252, acc: 54.77%\n",
      "[2, 1200], time 1145.6s, loss: 1.249, acc: 54.56%\n",
      "[2, 1400], time 1163.6s, loss: 1.251, acc: 54.52%\n",
      "[2, 1600], time 1180.4s, loss: 1.233, acc: 54.54%\n",
      "[2, 1800], time 1197.6s, loss: 1.228, acc: 54.72%\n",
      "[2, 2000], time 1215.0s, loss: 1.230, acc: 54.74%\n",
      "[2, 2200], time 1232.3s, loss: 1.211, acc: 54.83%\n",
      "[2, 2400], time 1250.6s, loss: 1.203, acc: 54.96%\n",
      "[2, 2600], time 1270.1s, loss: 1.197, acc: 55.09%\n",
      "[2, 2800], time 1289.4s, loss: 1.212, acc: 55.19%\n",
      "[2, 3000], time 1308.6s, loss: 1.213, acc: 55.26%\n",
      "[2, 3200], time 1327.8s, loss: 1.170, acc: 55.45%\n",
      "[2, 3400], time 1347.1s, loss: 1.200, acc: 55.52%\n",
      "[2, 3600], time 1366.9s, loss: 1.184, acc: 55.60%\n",
      "[2, 3800], time 1385.9s, loss: 1.177, acc: 55.65%\n",
      "[2, 4000], time 1405.2s, loss: 1.182, acc: 55.74%\n",
      "[2, 4200], time 1424.4s, loss: 1.178, acc: 55.78%\n",
      "[2, 4400], time 1443.6s, loss: 1.195, acc: 55.82%\n",
      "[2, 4600], time 1463.7s, loss: 1.210, acc: 55.84%\n",
      "[2, 4800], time 1483.6s, loss: 1.177, acc: 55.94%\n",
      "[2, 5000], time 1502.6s, loss: 1.173, acc: 55.98%\n",
      "[2, 5200], time 1521.9s, loss: 1.166, acc: 56.04%\n",
      "[2, 5400], time 1541.0s, loss: 1.138, acc: 56.15%\n",
      "[2, 5600], time 1560.8s, loss: 1.180, acc: 56.20%\n",
      "Epoch 2 | val loss: 1.1498, val acc: 0.5857 | model saved to ../saved_models\\model_epoch_2.pt\n",
      "[3, 200], time 2009.1s, loss: 1.114, acc: 60.00%\n",
      "[3, 400], time 2018.2s, loss: 1.105, acc: 59.93%\n",
      "[3, 600], time 2026.9s, loss: 1.087, acc: 60.05%\n",
      "[3, 800], time 2035.5s, loss: 1.098, acc: 60.08%\n",
      "[3, 1000], time 2043.9s, loss: 1.104, acc: 60.12%\n",
      "[3, 1200], time 2052.6s, loss: 1.112, acc: 59.91%\n",
      "[3, 1400], time 2061.0s, loss: 1.098, acc: 60.02%\n",
      "[3, 1600], time 2070.0s, loss: 1.095, acc: 60.14%\n",
      "[3, 1800], time 2080.7s, loss: 1.105, acc: 60.16%\n",
      "[3, 2000], time 2091.6s, loss: 1.085, acc: 60.26%\n",
      "[3, 2200], time 2102.1s, loss: 1.115, acc: 60.20%\n",
      "[3, 2400], time 2113.6s, loss: 1.105, acc: 60.24%\n",
      "[3, 2600], time 2126.0s, loss: 1.103, acc: 60.22%\n",
      "[3, 2800], time 2138.9s, loss: 1.099, acc: 60.25%\n",
      "[3, 3000], time 2151.9s, loss: 1.090, acc: 60.28%\n",
      "[3, 3200], time 2164.9s, loss: 1.099, acc: 60.30%\n",
      "[3, 3400], time 2178.8s, loss: 1.109, acc: 60.28%\n",
      "[3, 3600], time 2192.9s, loss: 1.111, acc: 60.25%\n",
      "[3, 3800], time 2207.6s, loss: 1.102, acc: 60.23%\n",
      "[3, 4000], time 2222.8s, loss: 1.094, acc: 60.25%\n",
      "[3, 4200], time 2238.2s, loss: 1.115, acc: 60.24%\n",
      "[3, 4400], time 2254.7s, loss: 1.079, acc: 60.27%\n",
      "[3, 4600], time 2273.4s, loss: 1.104, acc: 60.28%\n",
      "[3, 4800], time 2292.8s, loss: 1.076, acc: 60.30%\n",
      "[3, 5000], time 2311.8s, loss: 1.093, acc: 60.28%\n",
      "[3, 5200], time 2331.3s, loss: 1.083, acc: 60.29%\n",
      "[3, 5400], time 2351.2s, loss: 1.076, acc: 60.32%\n",
      "[3, 5600], time 2370.6s, loss: 1.074, acc: 60.35%\n",
      "Epoch 3 | val loss: 1.1513, val acc: 0.5891 | model saved to ../saved_models\\model_epoch_3.pt\n",
      "[4, 200], time 2819.9s, loss: 0.989, acc: 63.50%\n",
      "[4, 400], time 2829.3s, loss: 1.004, acc: 63.58%\n",
      "[4, 600], time 2838.5s, loss: 1.021, acc: 63.45%\n",
      "[4, 800], time 2848.3s, loss: 1.021, acc: 63.31%\n",
      "[4, 1000], time 2858.5s, loss: 1.029, acc: 63.20%\n",
      "[4, 1200], time 2869.0s, loss: 1.036, acc: 63.09%\n",
      "[4, 1400], time 2879.8s, loss: 1.014, acc: 63.15%\n",
      "[4, 1600], time 2890.4s, loss: 1.018, acc: 63.13%\n",
      "[4, 1800], time 2901.3s, loss: 1.020, acc: 63.12%\n",
      "[4, 2000], time 2912.9s, loss: 1.021, acc: 63.15%\n",
      "[4, 2200], time 2924.6s, loss: 1.058, acc: 63.04%\n",
      "[4, 2400], time 2936.0s, loss: 1.021, acc: 63.08%\n",
      "[4, 2600], time 2948.7s, loss: 1.036, acc: 63.09%\n",
      "[4, 2800], time 2960.9s, loss: 1.013, acc: 63.08%\n",
      "[4, 3000], time 2974.8s, loss: 1.022, acc: 63.09%\n",
      "[4, 3200], time 2989.0s, loss: 1.034, acc: 63.08%\n",
      "[4, 3400], time 3003.3s, loss: 1.045, acc: 63.06%\n",
      "[4, 3600], time 3019.2s, loss: 1.040, acc: 63.01%\n",
      "[4, 3800], time 3034.8s, loss: 1.033, acc: 62.98%\n",
      "[4, 4000], time 3050.4s, loss: 1.036, acc: 62.96%\n",
      "[4, 4200], time 3066.0s, loss: 1.003, acc: 62.99%\n",
      "[4, 4400], time 3083.8s, loss: 1.051, acc: 62.96%\n",
      "[4, 4600], time 3102.2s, loss: 1.047, acc: 62.94%\n",
      "[4, 4800], time 3120.9s, loss: 1.013, acc: 62.97%\n",
      "[4, 5000], time 3139.4s, loss: 1.056, acc: 62.93%\n",
      "[4, 5200], time 3158.4s, loss: 1.035, acc: 62.92%\n",
      "[4, 5400], time 3176.5s, loss: 1.014, acc: 62.91%\n",
      "[4, 5600], time 3195.2s, loss: 1.033, acc: 62.91%\n",
      "Epoch 4 | val loss: 1.1412, val acc: 0.5999 | model saved to ../saved_models\\model_epoch_4.pt\n",
      "[5, 200], time 3650.0s, loss: 0.942, acc: 65.69%\n",
      "[5, 400], time 3661.9s, loss: 0.938, acc: 66.19%\n",
      "[5, 600], time 3673.6s, loss: 0.943, acc: 65.91%\n",
      "[5, 800], time 3685.2s, loss: 0.946, acc: 65.79%\n",
      "[5, 1000], time 3696.3s, loss: 0.958, acc: 65.65%\n",
      "[5, 1200], time 3707.3s, loss: 0.962, acc: 65.54%\n",
      "[5, 1400], time 3718.9s, loss: 0.959, acc: 65.52%\n",
      "[5, 1600], time 3730.1s, loss: 0.952, acc: 65.54%\n",
      "[5, 1800], time 3741.8s, loss: 0.985, acc: 65.43%\n",
      "[5, 2000], time 3753.1s, loss: 0.980, acc: 65.35%\n",
      "[5, 2200], time 3764.3s, loss: 0.993, acc: 65.23%\n",
      "[5, 2400], time 3775.3s, loss: 0.970, acc: 65.22%\n",
      "[5, 2600], time 3786.4s, loss: 0.986, acc: 65.20%\n",
      "[5, 2800], time 3797.6s, loss: 1.002, acc: 65.10%\n",
      "[5, 3000], time 3809.6s, loss: 0.972, acc: 65.09%\n",
      "[5, 3200], time 3821.5s, loss: 0.952, acc: 65.14%\n",
      "[5, 3400], time 3834.5s, loss: 1.006, acc: 65.03%\n",
      "[5, 3600], time 3847.4s, loss: 0.987, acc: 65.01%\n",
      "[5, 3800], time 3859.8s, loss: 1.005, acc: 64.93%\n",
      "[5, 4000], time 3874.2s, loss: 0.968, acc: 64.94%\n",
      "[5, 4200], time 3888.7s, loss: 1.021, acc: 64.88%\n",
      "[5, 4400], time 3903.8s, loss: 0.995, acc: 64.87%\n",
      "[5, 4600], time 3918.8s, loss: 0.984, acc: 64.88%\n",
      "[5, 4800], time 3937.8s, loss: 0.984, acc: 64.87%\n",
      "[5, 5000], time 3957.2s, loss: 1.008, acc: 64.79%\n",
      "[5, 5200], time 3976.3s, loss: 0.990, acc: 64.77%\n",
      "[5, 5400], time 3995.0s, loss: 0.986, acc: 64.78%\n",
      "[5, 5600], time 4014.5s, loss: 0.991, acc: 64.77%\n",
      "Epoch 5 | val loss: 1.1497, val acc: 0.6008 | model saved to ../saved_models\\model_epoch_5.pt\n",
      "[6, 200], time 4484.3s, loss: 0.876, acc: 68.20%\n",
      "[6, 400], time 4503.6s, loss: 0.899, acc: 67.88%\n",
      "[6, 600], time 4523.1s, loss: 0.892, acc: 67.84%\n",
      "[6, 800], time 4543.2s, loss: 0.938, acc: 67.43%\n",
      "[6, 1000], time 4562.8s, loss: 0.926, acc: 67.30%\n",
      "[6, 1200], time 4582.4s, loss: 0.917, acc: 67.33%\n",
      "[6, 1400], time 4602.2s, loss: 0.917, acc: 67.27%\n",
      "[6, 1600], time 4621.2s, loss: 0.934, acc: 67.14%\n",
      "[6, 1800], time 4640.4s, loss: 0.926, acc: 67.09%\n",
      "[6, 2000], time 4659.2s, loss: 0.935, acc: 66.98%\n",
      "[6, 2200], time 4678.6s, loss: 0.941, acc: 66.95%\n",
      "[6, 2400], time 4697.9s, loss: 0.958, acc: 66.83%\n",
      "[6, 2600], time 4716.9s, loss: 0.923, acc: 66.81%\n",
      "[6, 2800], time 4736.6s, loss: 0.955, acc: 66.78%\n",
      "[6, 3000], time 4755.7s, loss: 0.920, acc: 66.78%\n",
      "[6, 3200], time 4774.8s, loss: 0.953, acc: 66.67%\n",
      "[6, 3400], time 4793.9s, loss: 0.948, acc: 66.62%\n",
      "[6, 3600], time 4813.5s, loss: 0.935, acc: 66.60%\n",
      "[6, 3800], time 4833.3s, loss: 0.934, acc: 66.60%\n",
      "[6, 4000], time 4853.2s, loss: 0.966, acc: 66.54%\n",
      "[6, 4200], time 4873.1s, loss: 0.945, acc: 66.52%\n",
      "[6, 4400], time 4892.3s, loss: 0.950, acc: 66.52%\n",
      "[6, 4600], time 4911.8s, loss: 0.951, acc: 66.51%\n",
      "[6, 4800], time 4930.8s, loss: 0.976, acc: 66.44%\n",
      "[6, 5000], time 4949.8s, loss: 0.958, acc: 66.41%\n",
      "[6, 5200], time 4969.4s, loss: 0.943, acc: 66.39%\n",
      "[6, 5400], time 4988.4s, loss: 0.969, acc: 66.32%\n",
      "[6, 5600], time 5008.2s, loss: 0.971, acc: 66.28%\n",
      "Epoch 6 | val loss: 1.1684, val acc: 0.5981 | model saved to ../saved_models\\model_epoch_6.pt\n",
      "[7, 200], time 5460.4s, loss: 0.840, acc: 69.42%\n",
      "[7, 400], time 5469.7s, loss: 0.860, acc: 68.94%\n",
      "[7, 600], time 5478.7s, loss: 0.869, acc: 68.98%\n",
      "[7, 800], time 5488.0s, loss: 0.896, acc: 68.56%\n",
      "[7, 1000], time 5497.5s, loss: 0.888, acc: 68.49%\n",
      "[7, 1200], time 5507.0s, loss: 0.873, acc: 68.34%\n",
      "[7, 1400], time 5516.4s, loss: 0.896, acc: 68.25%\n",
      "[7, 1600], time 5526.0s, loss: 0.893, acc: 68.25%\n",
      "[7, 1800], time 5536.1s, loss: 0.865, acc: 68.33%\n",
      "[7, 2000], time 5545.9s, loss: 0.891, acc: 68.33%\n",
      "[7, 2200], time 5556.6s, loss: 0.908, acc: 68.17%\n",
      "[7, 2400], time 5568.3s, loss: 0.909, acc: 68.06%\n",
      "[7, 2600], time 5580.3s, loss: 0.921, acc: 67.98%\n",
      "[7, 2800], time 5592.0s, loss: 0.926, acc: 67.86%\n",
      "[7, 3000], time 5604.7s, loss: 0.897, acc: 67.80%\n",
      "[7, 3200], time 5617.6s, loss: 0.930, acc: 67.70%\n",
      "[7, 3400], time 5631.6s, loss: 0.937, acc: 67.58%\n",
      "[7, 3600], time 5644.6s, loss: 0.908, acc: 67.58%\n",
      "[7, 3800], time 5658.6s, loss: 0.927, acc: 67.55%\n",
      "[7, 4000], time 5674.1s, loss: 0.926, acc: 67.50%\n",
      "[7, 4200], time 5690.1s, loss: 0.915, acc: 67.48%\n",
      "[7, 4400], time 5706.1s, loss: 0.921, acc: 67.42%\n",
      "[7, 4600], time 5724.7s, loss: 0.965, acc: 67.38%\n",
      "[7, 4800], time 5745.0s, loss: 0.918, acc: 67.36%\n",
      "[7, 5000], time 5764.1s, loss: 0.930, acc: 67.32%\n",
      "[7, 5200], time 5784.0s, loss: 0.933, acc: 67.27%\n",
      "[7, 5400], time 5804.3s, loss: 0.925, acc: 67.28%\n",
      "[7, 5600], time 5823.2s, loss: 0.932, acc: 67.25%\n",
      "Epoch 7 | val loss: 1.2270, val acc: 0.5961 | model saved to ../saved_models\\model_epoch_7.pt\n",
      "Early stopping triggered. Finished Training.\n",
      "Final training log written to: ../logs/model_on_rotated_images\\64x64x32x0.04xSGDx180000_25-03-2025.json\n"
     ]
    }
   ],
   "source": [
    "train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cypri\\AppData\\Local\\Temp\\ipykernel_35460\\2625688927.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../saved_models/model_epoch_7.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../saved_models/model_epoch_7.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch NONE val error: 0.0002579218416940421, acc: 0.5961222222222222\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f\"epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_Intro_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
