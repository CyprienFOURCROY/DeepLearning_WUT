{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c14ac1a-e963-4163-8866-f490d55b255c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b7eaf53-deba-43f2-86d6-39ac6d6134c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39ca280-ac28-4405-bc54-7bf93833175c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfbbe06-21dd-4bef-9110-43b009ed06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across multiple libraries.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    seed (int): \n",
    "        Seed value to use\n",
    "    \"\"\"\n",
    "    # Set PyTorch seed\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Set CUDA seed (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "        \n",
    "    \n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set Python's random seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set environment variable for some PyTorch operations\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Example usage\n",
    "set_seed(42)  # or any other seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abf976d-cf04-49e3-8e46-15f41107c619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=2, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_error = float('inf')\n",
    "    \n",
    "    def check(self, val_error, model):\n",
    "        if val_error < self.min_error - self.min_delta:\n",
    "            self.min_error = val_error\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e933ef2-c561-412a-afd7-f573629ace2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        # Input: 3 channels (RGB), Output: 16 feature maps, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop1 = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        # Input: 16 feature maps, Output: 32 feature maps, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Assuming input images are 32x32, after two 2x2 pooling layers, we have 32/2/2 = 8\n",
    "        # So the feature maps are 8x8 with 32 channels: 32 * 8 * 8 = 2048\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes (e.g., for CIFAR-10 dataset)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers\n",
    "        x = self.drop1(self.pool1(self.relu1(self.bn1(self.conv1(x)))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten the feature maps\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = self.fc2(self.drop4(self.relu4(self.bn4(self.fc1(x)))))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 2. Create an instance of the model\n",
    "model = BasicCNN()\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95, weight_decay=0.0005)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# 4. Load and preprocess data (example with CIFAR-10)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder('../../data/processed/train_augmented_rotated/', transform=transform)\n",
    "valset = torchvision.datasets.ImageFolder('../../data/raw/valid/', transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "ES = EarlyStopper(patience=4, min_delta=0)\n",
    "\n",
    "def train(epochs=5):\n",
    "    \n",
    "    time_started = time.time()\n",
    "    _train_errors = []\n",
    "    _val_errors = []\n",
    "    for epoch in range(epochs):\n",
    "        correct_train = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        total_batches = 0\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct_train += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            total += len(outputs)\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            \n",
    "            if i % 200 == 199:    # Print every 200 mini-batches\n",
    "                acc = correct_train / len(trainloader)\n",
    "                print(f'[{epoch + 1}, {i + 1}], time {time.time() - time_started} loss: {running_loss / 200:.3f}, acc: {correct_train/total * 100}%')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        _train_errors.append(epoch_loss / total_batches)\n",
    "        val_error = 0\n",
    "        correct_val = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "                correct_val += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            val_error = val_error / len(valloader.dataset)\n",
    "            _val_errors.append(val_error)\n",
    "            print(f'epoch {epoch + 1} TRAIN error: {epoch_loss / total_batches}, acc: {correct_train/total}')\n",
    "            print(f'epoch {epoch + 1} VALIDATION error: {val_error}, acc: {correct_val/len(valloader.dataset)}')\n",
    "\n",
    "        if ES.check(val_error, model):\n",
    "            print('Early stopping triggered!')\n",
    "            return _train_errors, _val_errors\n",
    "    \n",
    "    return _train_errors, _val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b01fad4-efcd-43c6-bd23-21621008de27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200], time 5.581929445266724 loss: 2.061, acc: 24.171875%\n",
      "[1, 400], time 7.184532403945923 loss: 1.851, acc: 28.0390625%\n",
      "[1, 600], time 8.797238826751709 loss: 1.766, acc: 30.307291666666668%\n",
      "[1, 800], time 10.42084002494812 loss: 1.689, acc: 31.863281249999996%\n",
      "[1, 1000], time 12.000942468643188 loss: 1.654, acc: 33.25%\n",
      "[1, 1200], time 13.599980115890503 loss: 1.635, acc: 34.20052083333333%\n",
      "[1, 1400], time 15.205082178115845 loss: 1.637, acc: 34.89732142857142%\n",
      "[1, 1600], time 16.788684606552124 loss: 1.599, acc: 35.599609375%\n",
      "[1, 1800], time 18.397432327270508 loss: 1.587, acc: 36.328125%\n",
      "[1, 2000], time 19.970037937164307 loss: 1.595, acc: 36.828125%\n",
      "[1, 2200], time 21.5666446685791 loss: 1.558, acc: 37.34659090909091%\n",
      "[1, 2400], time 23.149752140045166 loss: 1.562, acc: 37.8203125%\n",
      "[1, 2600], time 24.751858472824097 loss: 1.534, acc: 38.32451923076923%\n",
      "[1, 2800], time 26.376966953277588 loss: 1.528, acc: 38.699776785714285%\n",
      "[1, 3000], time 27.974573850631714 loss: 1.538, acc: 39.04166666666667%\n",
      "[1, 3200], time 29.542690753936768 loss: 1.495, acc: 39.427734375%\n",
      "[1, 3400], time 31.139800310134888 loss: 1.497, acc: 39.76746323529412%\n",
      "[1, 3600], time 32.73380780220032 loss: 1.507, acc: 40.07378472222222%\n",
      "[1, 3800], time 34.3274188041687 loss: 1.481, acc: 40.39309210526316%\n",
      "[1, 4000], time 35.92152214050293 loss: 1.467, acc: 40.678906250000004%\n",
      "[1, 4200], time 37.50312924385071 loss: 1.483, acc: 40.89657738095238%\n",
      "[1, 4400], time 39.065953493118286 loss: 1.456, acc: 41.18678977272727%\n",
      "[1, 4600], time 40.6670606136322 loss: 1.460, acc: 41.44361413043478%\n",
      "[1, 4800], time 42.264169454574585 loss: 1.486, acc: 41.630208333333336%\n",
      "[1, 5000], time 43.83977174758911 loss: 1.437, acc: 41.8725%\n",
      "[1, 5200], time 45.4348828792572 loss: 1.447, acc: 42.07451923076923%\n",
      "[1, 5400], time 47.02412557601929 loss: 1.426, acc: 42.277199074074076%\n",
      "[1, 5600], time 48.61273431777954 loss: 1.439, acc: 42.46316964285714%\n",
      "epoch 1 TRAIN error: 1.5666429207695856, acc: 0.4247888888888889\n",
      "epoch 1 VALIDATION error: 1.2897075414657593, acc: 0.5340444444444444\n",
      "[2, 200], time 80.42563724517822 loss: 1.405, acc: 48.546875%\n",
      "[2, 400], time 81.95174384117126 loss: 1.439, acc: 48.2890625%\n",
      "[2, 600], time 83.4833493232727 loss: 1.387, acc: 48.72916666666667%\n",
      "[2, 800], time 85.01645517349243 loss: 1.412, acc: 48.57421875%\n",
      "[2, 1000], time 86.55457019805908 loss: 1.413, acc: 48.65625%\n",
      "[2, 1200], time 88.07552886009216 loss: 1.408, acc: 48.661458333333336%\n",
      "[2, 1400], time 89.5996356010437 loss: 1.400, acc: 48.732142857142854%\n",
      "[2, 1600], time 91.14024114608765 loss: 1.388, acc: 48.765625%\n",
      "[2, 1800], time 92.6503484249115 loss: 1.364, acc: 48.90625%\n",
      "[2, 2000], time 94.15996026992798 loss: 1.369, acc: 49.065625000000004%\n",
      "[2, 2200], time 95.68765330314636 loss: 1.383, acc: 49.09943181818181%\n",
      "[2, 2400], time 97.19876074790955 loss: 1.387, acc: 49.169270833333336%\n",
      "[2, 2600], time 98.71236801147461 loss: 1.362, acc: 49.29326923076923%\n",
      "[2, 2800], time 100.22646903991699 loss: 1.377, acc: 49.323660714285715%\n",
      "[2, 3000], time 101.71207118034363 loss: 1.343, acc: 49.45%\n",
      "[2, 3200], time 103.28268623352051 loss: 1.384, acc: 49.4755859375%\n",
      "[2, 3400], time 104.8161141872406 loss: 1.348, acc: 49.58547794117647%\n",
      "[2, 3600], time 106.34021639823914 loss: 1.358, acc: 49.67447916666667%\n",
      "[2, 3800], time 107.86482048034668 loss: 1.355, acc: 49.73848684210526%\n",
      "[2, 4000], time 109.37791967391968 loss: 1.334, acc: 49.85546875%\n",
      "[2, 4200], time 110.9025182723999 loss: 1.321, acc: 50.00818452380953%\n",
      "[2, 4400], time 112.4161159992218 loss: 1.356, acc: 50.0390625%\n",
      "[2, 4600], time 113.94071578979492 loss: 1.336, acc: 50.100543478260875%\n",
      "[2, 4800], time 115.44581604003906 loss: 1.343, acc: 50.16341145833333%\n",
      "[2, 5000], time 116.96792387962341 loss: 1.338, acc: 50.223125%\n",
      "[2, 5200], time 118.46252489089966 loss: 1.340, acc: 50.30288461538461%\n",
      "[2, 5400], time 119.99462103843689 loss: 1.345, acc: 50.329282407407405%\n",
      "[2, 5600], time 121.5206458568573 loss: 1.340, acc: 50.39341517857143%\n",
      "epoch 2 TRAIN error: 1.3692290590180292, acc: 0.5038888888888889\n",
      "epoch 2 VALIDATION error: 1.1869302988052368, acc: 0.5741666666666667\n",
      "[3, 200], time 152.84326362609863 loss: 1.308, acc: 52.046875%\n",
      "[3, 400], time 154.32336950302124 loss: 1.301, acc: 52.39843749999999%\n",
      "[3, 600], time 155.81797909736633 loss: 1.320, acc: 52.40104166666667%\n",
      "[3, 800], time 157.3264000415802 loss: 1.324, acc: 52.43359374999999%\n",
      "[3, 1000], time 158.8450062274933 loss: 1.302, acc: 52.49374999999999%\n",
      "[3, 1200], time 160.34854364395142 loss: 1.315, acc: 52.526041666666664%\n",
      "[3, 1400], time 161.8366415500641 loss: 1.288, acc: 52.700892857142854%\n",
      "[3, 1600], time 163.36520147323608 loss: 1.326, acc: 52.658203125%\n",
      "[3, 1800], time 164.8913094997406 loss: 1.318, acc: 52.66319444444445%\n",
      "[3, 2000], time 166.43492031097412 loss: 1.288, acc: 52.737500000000004%\n",
      "[3, 2200], time 167.9521791934967 loss: 1.285, acc: 52.87926136363637%\n",
      "[3, 2400], time 169.48845601081848 loss: 1.298, acc: 52.9296875%\n",
      "[3, 2600], time 171.00956058502197 loss: 1.316, acc: 52.86899038461539%\n",
      "[3, 2800], time 172.53466057777405 loss: 1.297, acc: 52.900669642857146%\n",
      "[3, 3000], time 174.04826068878174 loss: 1.301, acc: 52.88125%\n",
      "[3, 3200], time 175.55885863304138 loss: 1.285, acc: 52.93457031249999%\n",
      "[3, 3400], time 177.0714647769928 loss: 1.323, acc: 52.88419117647059%\n",
      "[3, 3600], time 178.587562084198 loss: 1.269, acc: 52.97569444444444%\n",
      "[3, 3800], time 180.10573315620422 loss: 1.311, acc: 52.98684210526316%\n",
      "[3, 4000], time 181.62533235549927 loss: 1.276, acc: 53.044531250000006%\n",
      "[3, 4200], time 183.13749885559082 loss: 1.284, acc: 53.05803571428571%\n",
      "[3, 4400], time 184.6556007862091 loss: 1.288, acc: 53.09588068181819%\n",
      "[3, 4600], time 186.17020654678345 loss: 1.270, acc: 53.14470108695652%\n",
      "[3, 4800], time 187.6963701248169 loss: 1.291, acc: 53.15885416666667%\n",
      "[3, 5000], time 189.17397093772888 loss: 1.287, acc: 53.199375%\n",
      "[3, 5200], time 190.70407509803772 loss: 1.272, acc: 53.234375%\n",
      "[3, 5400], time 192.226646900177 loss: 1.252, acc: 53.31539351851852%\n",
      "[3, 5600], time 193.74217295646667 loss: 1.284, acc: 53.367745535714285%\n",
      "epoch 3 TRAIN error: 1.2958916541311476, acc: 0.5335777777777778\n",
      "epoch 3 VALIDATION error: 1.1508104801177979, acc: 0.5857555555555556\n",
      "[4, 200], time 224.9319145679474 loss: 1.234, acc: 55.703125%\n",
      "[4, 400], time 226.47575211524963 loss: 1.282, acc: 55.03125%\n",
      "[4, 600], time 228.01885056495667 loss: 1.262, acc: 54.942708333333336%\n",
      "[4, 800], time 229.54652500152588 loss: 1.254, acc: 54.8984375%\n",
      "[4, 1000], time 231.0911316871643 loss: 1.260, acc: 54.878125%\n",
      "[4, 1200], time 232.609867811203 loss: 1.248, acc: 54.93489583333333%\n",
      "[4, 1400], time 234.14447283744812 loss: 1.261, acc: 54.93080357142858%\n",
      "[4, 1600], time 235.6720838546753 loss: 1.254, acc: 54.92578125000001%\n",
      "[4, 1800], time 237.2206907272339 loss: 1.266, acc: 54.87152777777777%\n",
      "[4, 2000], time 238.75284719467163 loss: 1.261, acc: 54.823437500000004%\n",
      "[4, 2200], time 240.2969675064087 loss: 1.248, acc: 54.93181818181818%\n",
      "[4, 2400], time 241.83607029914856 loss: 1.244, acc: 55.015625%\n",
      "[4, 2600], time 243.36744570732117 loss: 1.272, acc: 54.93870192307693%\n",
      "[4, 2800], time 244.8731074333191 loss: 1.244, acc: 54.96875%\n",
      "[4, 3000], time 246.40733742713928 loss: 1.248, acc: 55.040625%\n",
      "[4, 3200], time 247.92593717575073 loss: 1.251, acc: 55.0556640625%\n",
      "[4, 3400], time 249.42715764045715 loss: 1.239, acc: 55.064338235294116%\n",
      "[4, 3600], time 250.95125317573547 loss: 1.248, acc: 55.07638888888889%\n",
      "[4, 3800], time 252.49169850349426 loss: 1.249, acc: 55.069078947368425%\n",
      "[4, 4000], time 254.0327935218811 loss: 1.242, acc: 55.118750000000006%\n",
      "[4, 4200], time 255.57639694213867 loss: 1.267, acc: 55.06473214285714%\n",
      "[4, 4400], time 257.10249376296997 loss: 1.240, acc: 55.1171875%\n",
      "[4, 4600], time 258.6275894641876 loss: 1.219, acc: 55.15489130434783%\n",
      "[4, 4800], time 260.1587800979614 loss: 1.254, acc: 55.150390625%\n",
      "[4, 5000], time 261.6668772697449 loss: 1.245, acc: 55.169999999999995%\n",
      "[4, 5200], time 263.2029790878296 loss: 1.226, acc: 55.200120192307686%\n",
      "[4, 5400], time 264.72458028793335 loss: 1.235, acc: 55.21817129629629%\n",
      "[4, 5600], time 266.26518082618713 loss: 1.235, acc: 55.21205357142858%\n",
      "epoch 4 TRAIN error: 1.2499066970083448, acc: 0.5520277777777778\n",
      "epoch 4 VALIDATION error: 1.110575795173645, acc: 0.6030666666666666\n",
      "[5, 200], time 297.1967918872833 loss: 1.205, acc: 56.84375000000001%\n",
      "[5, 400], time 298.72848320007324 loss: 1.212, acc: 56.265625%\n",
      "[5, 600], time 300.2315864562988 loss: 1.228, acc: 56.057291666666664%\n",
      "[5, 800], time 301.7518222332001 loss: 1.209, acc: 56.03515624999999%\n",
      "[5, 1000], time 303.21744203567505 loss: 1.208, acc: 56.074999999999996%\n",
      "[5, 1200], time 304.67456245422363 loss: 1.229, acc: 56.15625%\n",
      "[5, 1400], time 306.1606340408325 loss: 1.220, acc: 56.122767857142854%\n",
      "[5, 1600], time 307.6612377166748 loss: 1.211, acc: 56.13476562500001%\n",
      "[5, 1800], time 309.168842792511 loss: 1.231, acc: 56.11805555555556%\n",
      "[5, 2000], time 310.69994854927063 loss: 1.217, acc: 56.11874999999999%\n",
      "[5, 2200], time 312.2160470485687 loss: 1.200, acc: 56.20880681818182%\n",
      "[5, 2400], time 313.7356700897217 loss: 1.212, acc: 56.24869791666667%\n",
      "[5, 2600], time 315.25927233695984 loss: 1.231, acc: 56.19711538461538%\n",
      "[5, 2800], time 316.7913932800293 loss: 1.209, acc: 56.21316964285714%\n",
      "[5, 3000], time 318.31899523735046 loss: 1.217, acc: 56.256249999999994%\n",
      "[5, 3200], time 319.85518884658813 loss: 1.216, acc: 56.22070312500001%\n",
      "[5, 3400], time 321.35828709602356 loss: 1.209, acc: 56.25459558823529%\n",
      "[5, 3600], time 322.87848234176636 loss: 1.249, acc: 56.20746527777778%\n",
      "[5, 3800], time 324.37681674957275 loss: 1.214, acc: 56.20476973684211%\n",
      "[5, 4000], time 325.90161180496216 loss: 1.216, acc: 56.22265625%\n",
      "[5, 4200], time 327.38972091674805 loss: 1.222, acc: 56.19419642857143%\n",
      "[5, 4400], time 328.9133198261261 loss: 1.231, acc: 56.20241477272727%\n",
      "[5, 4600], time 330.42641854286194 loss: 1.197, acc: 56.22826086956522%\n",
      "[5, 4800], time 331.9450204372406 loss: 1.169, acc: 56.32096354166667%\n",
      "[5, 5000], time 333.46162581443787 loss: 1.214, acc: 56.335625%\n",
      "[5, 5200], time 334.97972226142883 loss: 1.231, acc: 56.33233173076923%\n",
      "[5, 5400], time 336.48232340812683 loss: 1.215, acc: 56.335648148148145%\n",
      "[5, 5600], time 337.9994297027588 loss: 1.200, acc: 56.37332589285714%\n",
      "epoch 5 TRAIN error: 1.2151318957858614, acc: 0.5636611111111111\n",
      "epoch 5 VALIDATION error: 1.0901724100112915, acc: 0.6129111111111111\n",
      "[6, 200], time 368.88707542419434 loss: 1.159, acc: 59.046875%\n",
      "[6, 400], time 370.40347123146057 loss: 1.200, acc: 57.859375%\n",
      "[6, 600], time 371.93606901168823 loss: 1.166, acc: 58.03125%\n",
      "[6, 800], time 373.44966673851013 loss: 1.184, acc: 58.08984375%\n",
      "[6, 1000], time 374.97274947166443 loss: 1.189, acc: 58.16875%\n",
      "[6, 1200], time 376.4994406700134 loss: 1.196, acc: 58.057291666666664%\n",
      "[6, 1400], time 378.0200400352478 loss: 1.187, acc: 57.91517857142857%\n",
      "[6, 1600], time 379.52267837524414 loss: 1.184, acc: 57.82031250000001%\n",
      "[6, 1800], time 381.05372953414917 loss: 1.188, acc: 57.71875%\n",
      "[6, 2000], time 382.54830837249756 loss: 1.214, acc: 57.587500000000006%\n",
      "[6, 2200], time 384.06291341781616 loss: 1.188, acc: 57.53267045454545%\n",
      "[6, 2400], time 385.57901406288147 loss: 1.180, acc: 57.59375%\n",
      "[6, 2600], time 387.08288979530334 loss: 1.182, acc: 57.64663461538462%\n",
      "[6, 2800], time 388.5889880657196 loss: 1.167, acc: 57.676339285714285%\n",
      "[6, 3000], time 390.1085948944092 loss: 1.197, acc: 57.63854166666667%\n",
      "[6, 3200], time 391.6246314048767 loss: 1.186, acc: 57.62890625%\n",
      "[6, 3400], time 393.1417279243469 loss: 1.172, acc: 57.642463235294116%\n",
      "[6, 3600], time 394.6358253955841 loss: 1.171, acc: 57.666666666666664%\n",
      "[6, 3800], time 396.1640293598175 loss: 1.174, acc: 57.678453947368425%\n",
      "[6, 4000], time 397.6786675453186 loss: 1.206, acc: 57.64296875%\n",
      "[6, 4200], time 399.2025430202484 loss: 1.179, acc: 57.70610119047619%\n",
      "[6, 4400], time 400.7161467075348 loss: 1.195, acc: 57.69673295454546%\n",
      "[6, 4600], time 402.24125027656555 loss: 1.210, acc: 57.65285326086956%\n",
      "[6, 4800], time 403.7343535423279 loss: 1.186, acc: 57.68554687500001%\n",
      "[6, 5000], time 405.27263832092285 loss: 1.182, acc: 57.66625%\n",
      "[6, 5200], time 406.79524183273315 loss: 1.191, acc: 57.65805288461539%\n",
      "[6, 5400], time 408.3968713283539 loss: 1.169, acc: 57.6869212962963%\n",
      "[6, 5600], time 410.03704047203064 loss: 1.178, acc: 57.68359374999999%\n",
      "epoch 6 TRAIN error: 1.1852963563919068, acc: 0.5768388888888889\n",
      "epoch 6 VALIDATION error: 1.0548174381256104, acc: 0.6218111111111111\n",
      "[7, 200], time 442.42599081993103 loss: 1.150, acc: 59.29687500000001%\n",
      "[7, 400], time 444.0241119861603 loss: 1.152, acc: 59.2265625%\n",
      "[7, 600], time 445.62722063064575 loss: 1.163, acc: 59.06249999999999%\n",
      "[7, 800], time 447.18682074546814 loss: 1.175, acc: 59.05859375%\n",
      "[7, 1000], time 448.7354257106781 loss: 1.168, acc: 58.803125%\n",
      "[7, 1200], time 450.30102586746216 loss: 1.139, acc: 58.8828125%\n",
      "[7, 1400], time 451.855628490448 loss: 1.192, acc: 58.78124999999999%\n",
      "[7, 1600], time 453.4347324371338 loss: 1.143, acc: 58.87109375%\n",
      "[7, 1800], time 455.0053367614746 loss: 1.160, acc: 58.82118055555555%\n",
      "[7, 2000], time 456.6299502849579 loss: 1.170, acc: 58.6421875%\n",
      "[7, 2200], time 458.2415552139282 loss: 1.151, acc: 58.649147727272734%\n",
      "[7, 2400], time 459.81916093826294 loss: 1.140, acc: 58.721354166666664%\n",
      "[7, 2600], time 461.4002733230591 loss: 1.173, acc: 58.62500000000001%\n",
      "[7, 2800], time 463.00297260284424 loss: 1.165, acc: 58.56584821428571%\n",
      "[7, 3000], time 464.61737608909607 loss: 1.160, acc: 58.55625%\n",
      "[7, 3200], time 466.2238621711731 loss: 1.162, acc: 58.548828125%\n",
      "[7, 3400], time 467.85547614097595 loss: 1.177, acc: 58.50827205882353%\n",
      "[7, 3600], time 469.46946477890015 loss: 1.166, acc: 58.49826388888889%\n",
      "[7, 3800], time 471.0690679550171 loss: 1.175, acc: 58.484375%\n",
      "[7, 4000], time 472.6700072288513 loss: 1.167, acc: 58.47656249999999%\n",
      "[7, 4200], time 474.2456121444702 loss: 1.160, acc: 58.49404761904762%\n",
      "[7, 4400], time 475.85272431373596 loss: 1.165, acc: 58.49076704545455%\n",
      "[7, 4600], time 477.4631621837616 loss: 1.172, acc: 58.497282608695656%\n",
      "[7, 4800], time 479.0722723007202 loss: 1.172, acc: 58.483723958333336%\n",
      "[7, 5000], time 480.67538261413574 loss: 1.165, acc: 58.489374999999995%\n",
      "[7, 5200], time 482.2824957370758 loss: 1.159, acc: 58.50240384615385%\n",
      "[7, 5400], time 483.8880286216736 loss: 1.161, acc: 58.50694444444444%\n",
      "[7, 5600], time 485.49764609336853 loss: 1.183, acc: 58.478794642857146%\n",
      "epoch 7 TRAIN error: 1.1637570968204074, acc: 0.5847055555555556\n",
      "epoch 7 VALIDATION error: 1.0333870649337769, acc: 0.6294111111111111\n",
      "[8, 200], time 518.7748239040375 loss: 1.148, acc: 59.21875%\n",
      "[8, 400], time 520.4344503879547 loss: 1.135, acc: 59.40625000000001%\n",
      "[8, 600], time 522.074063539505 loss: 1.105, acc: 59.703125%\n",
      "[8, 800], time 523.7071750164032 loss: 1.117, acc: 59.99609375%\n",
      "[8, 1000], time 525.3627953529358 loss: 1.123, acc: 60.021875%\n",
      "[8, 1200], time 527.0039129257202 loss: 1.142, acc: 59.997395833333336%\n",
      "[8, 1400], time 528.6435358524323 loss: 1.124, acc: 59.89955357142858%\n",
      "[8, 1600], time 530.2991518974304 loss: 1.141, acc: 59.791015625%\n",
      "[8, 1800], time 531.9577724933624 loss: 1.143, acc: 59.68750000000001%\n",
      "[8, 2000], time 533.6123957633972 loss: 1.142, acc: 59.6078125%\n",
      "[8, 2200], time 535.2630178928375 loss: 1.146, acc: 59.622159090909086%\n",
      "[8, 2400], time 536.9101331233978 loss: 1.152, acc: 59.52734374999999%\n",
      "[8, 2600], time 538.5787539482117 loss: 1.161, acc: 59.503605769230774%\n",
      "[8, 2800], time 540.1763646602631 loss: 1.158, acc: 59.455357142857146%\n",
      "[8, 3000], time 541.7639751434326 loss: 1.144, acc: 59.438541666666666%\n",
      "[8, 3200], time 543.3705840110779 loss: 1.154, acc: 59.40429687499999%\n",
      "[8, 3400], time 544.9751873016357 loss: 1.152, acc: 59.377757352941174%\n",
      "[8, 3600], time 546.5712912082672 loss: 1.149, acc: 59.36545138888889%\n",
      "[8, 3800], time 548.1653966903687 loss: 1.172, acc: 59.27713815789474%\n",
      "[8, 4000], time 549.7695031166077 loss: 1.137, acc: 59.318749999999994%\n",
      "[8, 4200], time 551.3706066608429 loss: 1.166, acc: 59.27083333333333%\n",
      "[8, 4400], time 552.9812109470367 loss: 1.130, acc: 59.29048295454545%\n",
      "[8, 4600], time 554.5818254947662 loss: 1.182, acc: 59.21942934782609%\n",
      "[8, 4800], time 556.1825847625732 loss: 1.136, acc: 59.244140625%\n",
      "[8, 5000], time 557.7791955471039 loss: 1.154, acc: 59.199999999999996%\n",
      "[8, 5200], time 559.3733086585999 loss: 1.145, acc: 59.18629807692307%\n",
      "[8, 5400], time 561.0054275989532 loss: 1.161, acc: 59.17418981481482%\n",
      "[8, 5600], time 562.6470444202423 loss: 1.152, acc: 59.161830357142854%\n",
      "epoch 8 TRAIN error: 1.145448135884603, acc: 0.5915888888888889\n",
      "epoch 8 VALIDATION error: 1.023874282836914, acc: 0.6345444444444445\n",
      "[9, 200], time 597.0996143817902 loss: 1.114, acc: 60.18750000000001%\n",
      "[9, 400], time 598.7992277145386 loss: 1.117, acc: 60.1796875%\n",
      "[9, 600], time 600.4863545894623 loss: 1.128, acc: 60.328125%\n",
      "[9, 800], time 602.1779794692993 loss: 1.114, acc: 60.37109375000001%\n",
      "[9, 1000], time 603.8046078681946 loss: 1.131, acc: 60.246875%\n",
      "[9, 1200], time 605.5227422714233 loss: 1.108, acc: 60.3125%\n",
      "[9, 1400], time 607.1853663921356 loss: 1.143, acc: 60.136160714285715%\n",
      "[9, 1600], time 608.867493391037 loss: 1.124, acc: 60.072265625%\n",
      "[9, 1800], time 610.6021153926849 loss: 1.138, acc: 60.03993055555556%\n",
      "[9, 2000], time 612.3017356395721 loss: 1.136, acc: 59.959375%\n",
      "[9, 2200], time 614.0318732261658 loss: 1.137, acc: 59.95738636363637%\n",
      "[9, 2400], time 615.7604987621307 loss: 1.129, acc: 59.95703125%\n",
      "[9, 2600], time 617.4441146850586 loss: 1.129, acc: 59.90504807692307%\n",
      "[9, 2800], time 619.1582355499268 loss: 1.099, acc: 59.93638392857142%\n",
      "[9, 3000], time 620.8843591213226 loss: 1.126, acc: 59.942708333333336%\n",
      "[9, 3200], time 622.5809798240662 loss: 1.136, acc: 59.90527343749999%\n",
      "[9, 3400], time 624.2716197967529 loss: 1.149, acc: 59.862132352941174%\n",
      "[9, 3600], time 626.000744342804 loss: 1.130, acc: 59.87065972222222%\n",
      "[9, 3800], time 627.6914026737213 loss: 1.152, acc: 59.8141447368421%\n",
      "[9, 4000], time 629.3919060230255 loss: 1.151, acc: 59.7796875%\n",
      "[9, 4200], time 631.0877118110657 loss: 1.154, acc: 59.71354166666667%\n",
      "[9, 4400], time 632.77983045578 loss: 1.127, acc: 59.71875%\n",
      "[9, 4600], time 634.5219385623932 loss: 1.109, acc: 59.74320652173913%\n",
      "[9, 4800], time 636.2395577430725 loss: 1.139, acc: 59.729166666666664%\n",
      "[9, 5000], time 637.8846740722656 loss: 1.119, acc: 59.714999999999996%\n",
      "[9, 5200], time 639.5737857818604 loss: 1.153, acc: 59.70132211538461%\n",
      "[9, 5400], time 641.3073971271515 loss: 1.141, acc: 59.67418981481482%\n",
      "[9, 5600], time 643.0275995731354 loss: 1.128, acc: 59.70479910714286%\n",
      "epoch 9 TRAIN error: 1.130971200052897, acc: 0.5969333333333333\n",
      "epoch 9 VALIDATION error: 1.027524471282959, acc: 0.6317333333333334\n",
      "[10, 200], time 675.8614552021027 loss: 1.111, acc: 60.5625%\n",
      "[10, 400], time 677.3710517883301 loss: 1.110, acc: 60.265625%\n",
      "[10, 600], time 678.8786487579346 loss: 1.115, acc: 60.369791666666664%\n",
      "[10, 800], time 680.4042489528656 loss: 1.081, acc: 60.57421875%\n",
      "[10, 1000], time 681.9128532409668 loss: 1.101, acc: 60.71875%\n",
      "[10, 1200], time 683.4224543571472 loss: 1.123, acc: 60.59895833333333%\n",
      "[10, 1400], time 684.9240500926971 loss: 1.120, acc: 60.5%\n",
      "[10, 1600], time 686.4216508865356 loss: 1.114, acc: 60.509765625%\n",
      "[10, 1800], time 687.9382576942444 loss: 1.119, acc: 60.38888888888889%\n",
      "[10, 2000], time 689.4518570899963 loss: 1.146, acc: 60.259375000000006%\n",
      "[10, 2200], time 690.970954656601 loss: 1.118, acc: 60.28835227272727%\n",
      "[10, 2400], time 692.4800581932068 loss: 1.112, acc: 60.325520833333336%\n",
      "[10, 2600], time 693.9881763458252 loss: 1.129, acc: 60.30168269230769%\n",
      "[10, 2800], time 695.5207877159119 loss: 1.120, acc: 60.30580357142858%\n",
      "[10, 3000], time 697.0229144096375 loss: 1.127, acc: 60.3%\n",
      "[10, 3200], time 698.5445735454559 loss: 1.120, acc: 60.2744140625%\n",
      "[10, 3400], time 700.0656840801239 loss: 1.109, acc: 60.3125%\n",
      "[10, 3600], time 701.587296962738 loss: 1.125, acc: 60.27517361111111%\n",
      "[10, 3800], time 703.1104030609131 loss: 1.122, acc: 60.23108552631579%\n",
      "[10, 4000], time 704.6215183734894 loss: 1.137, acc: 60.20546875%\n",
      "[10, 4200], time 706.1316435337067 loss: 1.118, acc: 60.20907738095238%\n",
      "[10, 4400], time 707.649763584137 loss: 1.106, acc: 60.22159090909091%\n",
      "[10, 4600], time 709.1773769855499 loss: 1.115, acc: 60.20040760869565%\n",
      "[10, 4800], time 710.702986240387 loss: 1.143, acc: 60.139973958333336%\n",
      "[10, 5000], time 712.2256104946136 loss: 1.133, acc: 60.114374999999995%\n",
      "[10, 5200], time 713.7527253627777 loss: 1.133, acc: 60.1045673076923%\n",
      "[10, 5400], time 715.2748408317566 loss: 1.128, acc: 60.083333333333336%\n",
      "[10, 5600], time 716.7984631061554 loss: 1.133, acc: 60.07310267857143%\n",
      "epoch 10 TRAIN error: 1.1201360771920945, acc: 0.6007111111111111\n",
      "epoch 10 VALIDATION error: 1.025870680809021, acc: 0.6347888888888888\n",
      "[11, 200], time 747.7105765342712 loss: 1.073, acc: 61.984375%\n",
      "[11, 400], time 749.2326788902283 loss: 1.103, acc: 61.32031249999999%\n",
      "[11, 600], time 750.7307770252228 loss: 1.112, acc: 60.92708333333333%\n",
      "[11, 800], time 752.2408797740936 loss: 1.089, acc: 60.98046875000001%\n",
      "[11, 1000], time 753.737478017807 loss: 1.110, acc: 60.784375000000004%\n",
      "[11, 1200], time 755.2545750141144 loss: 1.123, acc: 60.65625%\n",
      "[11, 1400], time 756.7761738300323 loss: 1.102, acc: 60.73437499999999%\n",
      "[11, 1600], time 758.2727706432343 loss: 1.100, acc: 60.7890625%\n",
      "[11, 1800], time 759.7818684577942 loss: 1.112, acc: 60.74131944444444%\n",
      "[11, 2000], time 761.2939689159393 loss: 1.125, acc: 60.68125%\n",
      "[11, 2200], time 762.809074640274 loss: 1.135, acc: 60.68181818181818%\n",
      "[11, 2400], time 764.3226759433746 loss: 1.086, acc: 60.74609375%\n",
      "[11, 2600], time 765.8072707653046 loss: 1.145, acc: 60.645432692307686%\n",
      "[11, 2800], time 767.3188674449921 loss: 1.116, acc: 60.59821428571428%\n",
      "[11, 3000], time 768.8194675445557 loss: 1.117, acc: 60.55729166666667%\n",
      "[11, 3200], time 770.3220648765564 loss: 1.121, acc: 60.521484375%\n",
      "[11, 3400], time 771.8281610012054 loss: 1.113, acc: 60.497242647058826%\n",
      "[11, 3600], time 773.3412623405457 loss: 1.130, acc: 60.454861111111114%\n",
      "[11, 3800], time 774.8633666038513 loss: 1.114, acc: 60.46052631578947%\n",
      "[11, 4000], time 776.3699786663055 loss: 1.106, acc: 60.46015625%\n",
      "[11, 4200], time 777.884081363678 loss: 1.095, acc: 60.49925595238095%\n",
      "[11, 4400], time 779.4021806716919 loss: 1.097, acc: 60.51420454545454%\n",
      "[11, 4600], time 780.9122850894928 loss: 1.102, acc: 60.54008152173913%\n",
      "[11, 4800], time 782.4168887138367 loss: 1.097, acc: 60.56445312499999%\n",
      "[11, 5000], time 783.9039921760559 loss: 1.109, acc: 60.571875%\n",
      "[11, 5200], time 785.4145908355713 loss: 1.145, acc: 60.54747596153847%\n",
      "[11, 5400], time 786.930192232132 loss: 1.121, acc: 60.539351851851855%\n",
      "[11, 5600], time 788.43678855896 loss: 1.129, acc: 60.50111607142858%\n",
      "epoch 11 TRAIN error: 1.1115979221767849, acc: 0.6050166666666666\n",
      "epoch 11 VALIDATION error: 1.0267139673233032, acc: 0.6335666666666666\n",
      "[12, 200], time 819.3283538818359 loss: 1.084, acc: 60.8125%\n",
      "[12, 400], time 820.8329529762268 loss: 1.089, acc: 61.0234375%\n",
      "[12, 600], time 822.339549779892 loss: 1.062, acc: 61.61979166666667%\n",
      "[12, 800], time 823.8591527938843 loss: 1.098, acc: 61.421875%\n",
      "[12, 1000], time 825.3712508678436 loss: 1.114, acc: 61.365625%\n",
      "[12, 1200], time 826.8773534297943 loss: 1.073, acc: 61.49479166666667%\n",
      "[12, 1400], time 828.3904585838318 loss: 1.096, acc: 61.37946428571428%\n",
      "[12, 1600], time 829.895058631897 loss: 1.076, acc: 61.447265625%\n",
      "[12, 1800], time 831.4101603031158 loss: 1.081, acc: 61.42881944444445%\n",
      "[12, 2000], time 832.9092609882355 loss: 1.082, acc: 61.481249999999996%\n",
      "[12, 2200], time 834.4143629074097 loss: 1.110, acc: 61.42755681818181%\n",
      "[12, 2400], time 835.9295601844788 loss: 1.127, acc: 61.325520833333336%\n",
      "[12, 2600], time 837.4301643371582 loss: 1.119, acc: 61.23076923076923%\n",
      "[12, 2800], time 839.0082745552063 loss: 1.090, acc: 61.27455357142857%\n",
      "[12, 3000], time 840.5153765678406 loss: 1.089, acc: 61.260416666666664%\n",
      "[12, 3200], time 842.0194780826569 loss: 1.111, acc: 61.25976562499999%\n",
      "[12, 3400], time 843.5250771045685 loss: 1.091, acc: 61.263786764705884%\n",
      "[12, 3600], time 845.0241763591766 loss: 1.101, acc: 61.25000000000001%\n",
      "[12, 3800], time 846.5207772254944 loss: 1.085, acc: 61.239309210526315%\n",
      "[12, 4000], time 848.0388872623444 loss: 1.132, acc: 61.171093750000004%\n",
      "[12, 4200], time 849.5499963760376 loss: 1.086, acc: 61.16220238095238%\n",
      "[12, 4400], time 851.0496034622192 loss: 1.103, acc: 61.14417613636364%\n",
      "[12, 4600], time 852.5667071342468 loss: 1.109, acc: 61.09986413043478%\n",
      "[12, 4800], time 854.0888123512268 loss: 1.107, acc: 61.109375%\n",
      "[12, 5000], time 855.6019160747528 loss: 1.098, acc: 61.103125000000006%\n",
      "[12, 5200], time 857.1075189113617 loss: 1.107, acc: 61.09615384615385%\n",
      "[12, 5400], time 858.707133769989 loss: 1.108, acc: 61.09548611111111%\n",
      "[12, 5600], time 860.2252407073975 loss: 1.090, acc: 61.11216517857143%\n",
      "epoch 12 TRAIN error: 1.0971868488311767, acc: 0.6109722222222222\n",
      "epoch 12 VALIDATION error: 1.0118249654769897, acc: 0.6388888888888888\n",
      "[13, 200], time 891.9158163070679 loss: 1.081, acc: 61.79687499999999%\n",
      "[13, 400], time 893.4440667629242 loss: 1.096, acc: 61.2421875%\n",
      "[13, 600], time 894.9716761112213 loss: 1.114, acc: 61.03645833333333%\n",
      "[13, 800], time 896.5005164146423 loss: 1.074, acc: 61.0859375%\n",
      "[13, 1000], time 898.087641954422 loss: 1.081, acc: 61.203125%\n",
      "[13, 1200], time 899.6227703094482 loss: 1.083, acc: 61.13802083333333%\n",
      "[13, 1400], time 901.1604058742523 loss: 1.081, acc: 61.22098214285714%\n",
      "[13, 1600], time 902.7015097141266 loss: 1.101, acc: 61.1796875%\n",
      "[13, 1800], time 904.277628660202 loss: 1.097, acc: 61.16145833333333%\n",
      "[13, 2000], time 905.8049612045288 loss: 1.084, acc: 61.235937500000006%\n",
      "[13, 2200], time 907.3575766086578 loss: 1.066, acc: 61.37073863636363%\n",
      "[13, 2400], time 908.9119951725006 loss: 1.094, acc: 61.26432291666667%\n",
      "[13, 2600], time 910.4326078891754 loss: 1.099, acc: 61.31850961538462%\n",
      "[13, 2800], time 911.956220626831 loss: 1.104, acc: 61.27678571428572%\n",
      "[13, 3000], time 913.4693224430084 loss: 1.115, acc: 61.19479166666667%\n",
      "[13, 3200], time 915.0486526489258 loss: 1.102, acc: 61.169921875%\n",
      "[13, 3400], time 916.5959181785583 loss: 1.085, acc: 61.19117647058824%\n",
      "[13, 3600], time 918.15074467659 loss: 1.108, acc: 61.14670138888889%\n",
      "[13, 3800], time 919.7020580768585 loss: 1.107, acc: 61.12746710526316%\n",
      "[13, 4000], time 921.2393891811371 loss: 1.084, acc: 61.13203125%\n",
      "[13, 4200], time 922.7516407966614 loss: 1.097, acc: 61.1547619047619%\n",
      "[13, 4400], time 924.2828967571259 loss: 1.094, acc: 61.15553977272727%\n",
      "[13, 4600], time 925.816712141037 loss: 1.113, acc: 61.124320652173914%\n",
      "[13, 4800], time 927.3720710277557 loss: 1.084, acc: 61.15234375000001%\n",
      "[13, 5000], time 929.0123355388641 loss: 1.087, acc: 61.180625%\n",
      "[13, 5200], time 930.5494439601898 loss: 1.116, acc: 61.152043269230774%\n",
      "[13, 5400], time 932.0765533447266 loss: 1.080, acc: 61.13136574074074%\n",
      "[13, 5600], time 933.6176524162292 loss: 1.103, acc: 61.12332589285714%\n",
      "epoch 13 TRAIN error: 1.094228723748525, acc: 0.6111944444444445\n",
      "epoch 13 VALIDATION error: 1.0097278356552124, acc: 0.6382\n",
      "[14, 200], time 965.5297219753265 loss: 1.099, acc: 61.35937500000001%\n",
      "[14, 400], time 967.0608260631561 loss: 1.061, acc: 62.109375%\n",
      "[14, 600], time 968.5774235725403 loss: 1.064, acc: 62.07812499999999%\n",
      "[14, 800], time 970.098302602768 loss: 1.086, acc: 61.80859374999999%\n",
      "[14, 1000], time 971.6549022197723 loss: 1.061, acc: 61.884375000000006%\n",
      "[14, 1200], time 973.1917157173157 loss: 1.089, acc: 61.807291666666664%\n",
      "[14, 1400], time 974.7503185272217 loss: 1.065, acc: 61.88392857142857%\n",
      "[14, 1600], time 976.2799384593964 loss: 1.096, acc: 61.79296875%\n",
      "[14, 1800], time 977.8380377292633 loss: 1.065, acc: 61.864583333333336%\n",
      "[14, 2000], time 979.3751513957977 loss: 1.078, acc: 61.831250000000004%\n",
      "[14, 2200], time 980.9352512359619 loss: 1.100, acc: 61.76704545454545%\n",
      "[14, 2400], time 982.4628577232361 loss: 1.088, acc: 61.765625%\n",
      "[14, 2600], time 983.9759638309479 loss: 1.081, acc: 61.72596153846154%\n",
      "[14, 2800], time 985.5070624351501 loss: 1.064, acc: 61.75111607142857%\n",
      "[14, 3000], time 987.0146543979645 loss: 1.082, acc: 61.753125%\n",
      "[14, 3200], time 988.5448424816132 loss: 1.087, acc: 61.71484375%\n",
      "[14, 3400], time 990.064234495163 loss: 1.080, acc: 61.71966911764706%\n",
      "[14, 3600], time 991.603912115097 loss: 1.106, acc: 61.63194444444444%\n",
      "[14, 3800], time 993.11501121521 loss: 1.089, acc: 61.62746710526316%\n",
      "[14, 4000], time 994.6531126499176 loss: 1.088, acc: 61.578906249999996%\n",
      "[14, 4200], time 996.1657195091248 loss: 1.085, acc: 61.55580357142857%\n",
      "[14, 4400], time 997.6788146495819 loss: 1.091, acc: 61.50142045454545%\n",
      "[14, 4600], time 999.1876990795135 loss: 1.095, acc: 61.47690217391304%\n",
      "[14, 4800], time 1000.7042920589447 loss: 1.101, acc: 61.4609375%\n",
      "[14, 5000], time 1002.2134845256805 loss: 1.108, acc: 61.456875%\n",
      "[14, 5200], time 1003.7215859889984 loss: 1.105, acc: 61.41105769230769%\n",
      "[14, 5400], time 1005.2391893863678 loss: 1.118, acc: 61.35706018518518%\n",
      "[14, 5600], time 1006.7692918777466 loss: 1.089, acc: 61.361607142857146%\n",
      "epoch 14 TRAIN error: 1.0862512157970006, acc: 0.6136444444444444\n",
      "epoch 14 VALIDATION error: 1.0060184001922607, acc: 0.6384111111111112\n",
      "[15, 200], time 1037.96315407753 loss: 1.071, acc: 61.703125%\n",
      "[15, 400], time 1039.5014951229095 loss: 1.070, acc: 61.8984375%\n",
      "[15, 600], time 1040.9720976352692 loss: 1.049, acc: 62.375%\n",
      "[15, 800], time 1042.4817025661469 loss: 1.086, acc: 62.203125%\n",
      "[15, 1000], time 1044.001329421997 loss: 1.080, acc: 62.212500000000006%\n",
      "[15, 1200], time 1045.5174379348755 loss: 1.081, acc: 62.11979166666667%\n",
      "[15, 1400], time 1047.0525465011597 loss: 1.064, acc: 62.185267857142854%\n",
      "[15, 1600], time 1048.5591614246368 loss: 1.061, acc: 62.23046875%\n",
      "[15, 1800], time 1050.0875549316406 loss: 1.060, acc: 62.282986111111114%\n",
      "[15, 2000], time 1051.6102545261383 loss: 1.098, acc: 62.15625%\n",
      "[15, 2200], time 1053.1343669891357 loss: 1.079, acc: 62.06392045454545%\n",
      "[15, 2400], time 1054.6465549468994 loss: 1.094, acc: 62.03385416666667%\n",
      "[15, 2600], time 1056.150160074234 loss: 1.050, acc: 62.05889423076923%\n",
      "[15, 2800], time 1057.6713061332703 loss: 1.070, acc: 62.07254464285714%\n",
      "[15, 3000], time 1059.1774168014526 loss: 1.079, acc: 62.00729166666667%\n",
      "[15, 3200], time 1060.6975345611572 loss: 1.104, acc: 61.9365234375%\n",
      "[15, 3400], time 1062.216635465622 loss: 1.099, acc: 61.887867647058826%\n",
      "[15, 3600], time 1063.7157363891602 loss: 1.090, acc: 61.82986111111111%\n",
      "[15, 3800], time 1065.2413353919983 loss: 1.094, acc: 61.791940789473685%\n",
      "[15, 4000], time 1066.7698330879211 loss: 1.085, acc: 61.7703125%\n",
      "[15, 4200], time 1068.2909533977509 loss: 1.089, acc: 61.736607142857146%\n",
      "[15, 4400], time 1069.830062866211 loss: 1.112, acc: 61.68181818181818%\n",
      "[15, 4600], time 1071.381172657013 loss: 1.094, acc: 61.67798913043478%\n",
      "[15, 4800], time 1072.9008564949036 loss: 1.087, acc: 61.651041666666664%\n",
      "[15, 5000], time 1074.4194602966309 loss: 1.067, acc: 61.6675%\n",
      "[15, 5200], time 1075.9601333141327 loss: 1.109, acc: 61.63401442307692%\n",
      "[15, 5400], time 1077.5067384243011 loss: 1.098, acc: 61.62037037037037%\n",
      "[15, 5600], time 1079.0454955101013 loss: 1.062, acc: 61.63727678571429%\n",
      "epoch 15 TRAIN error: 1.0813057873090108, acc: 0.6164166666666666\n",
      "epoch 15 VALIDATION error: 1.0021458864212036, acc: 0.6407333333333334\n",
      "[16, 200], time 1110.6291229724884 loss: 1.065, acc: 61.8125%\n",
      "[16, 400], time 1112.2322421073914 loss: 1.053, acc: 61.953125%\n",
      "[16, 600], time 1113.899749994278 loss: 1.071, acc: 62.109375%\n",
      "[16, 800], time 1115.4613554477692 loss: 1.059, acc: 62.10546875%\n",
      "[16, 1000], time 1117.0604650974274 loss: 1.049, acc: 62.25937499999999%\n",
      "[16, 1200], time 1118.748761177063 loss: 1.068, acc: 62.1796875%\n",
      "[16, 1400], time 1120.3564665317535 loss: 1.065, acc: 62.07812499999999%\n",
      "[16, 1600], time 1121.959687948227 loss: 1.065, acc: 62.09375%\n",
      "[16, 1800], time 1123.575799703598 loss: 1.049, acc: 62.20659722222223%\n",
      "[16, 2000], time 1125.2504448890686 loss: 1.073, acc: 62.135937500000004%\n",
      "[16, 2200], time 1126.8650577068329 loss: 1.074, acc: 62.08380681818182%\n",
      "[16, 2400], time 1128.4702954292297 loss: 1.090, acc: 62.026041666666664%\n",
      "[16, 2600], time 1130.1781978607178 loss: 1.076, acc: 62.05288461538462%\n",
      "[16, 2800], time 1131.8002648353577 loss: 1.092, acc: 61.98102678571429%\n",
      "[16, 3000], time 1133.4253206253052 loss: 1.068, acc: 61.969791666666666%\n",
      "[16, 3200], time 1135.0887546539307 loss: 1.090, acc: 61.96484375%\n",
      "[16, 3400], time 1136.7552859783173 loss: 1.077, acc: 61.935661764705884%\n",
      "[16, 3600], time 1138.3930141925812 loss: 1.055, acc: 61.94444444444444%\n",
      "[16, 3800], time 1140.0460894107819 loss: 1.091, acc: 61.88486842105263%\n",
      "[16, 4000], time 1141.718757867813 loss: 1.085, acc: 61.848437499999996%\n",
      "[16, 4200], time 1143.3378677368164 loss: 1.096, acc: 61.80654761904761%\n",
      "[16, 4400], time 1144.9852123260498 loss: 1.085, acc: 61.79048295454545%\n",
      "[16, 4600], time 1146.7178792953491 loss: 1.067, acc: 61.791440217391305%\n",
      "[16, 4800], time 1148.3699886798859 loss: 1.074, acc: 61.807291666666664%\n",
      "[16, 5000], time 1150.0347278118134 loss: 1.063, acc: 61.818125%\n",
      "[16, 5200], time 1151.6938433647156 loss: 1.059, acc: 61.83653846153846%\n",
      "[16, 5400], time 1153.4053492546082 loss: 1.113, acc: 61.802083333333336%\n",
      "[16, 5600], time 1155.0523059368134 loss: 1.072, acc: 61.84263392857143%\n",
      "epoch 16 TRAIN error: 1.0727660404523214, acc: 0.6184388888888889\n",
      "epoch 16 VALIDATION error: 1.0060745477676392, acc: 0.6419666666666667\n",
      "[17, 200], time 1187.5499498844147 loss: 1.059, acc: 62.03125000000001%\n",
      "[17, 400], time 1189.1113147735596 loss: 1.041, acc: 62.7578125%\n",
      "[17, 600], time 1190.6687316894531 loss: 1.060, acc: 62.479166666666664%\n",
      "[17, 800], time 1192.2388491630554 loss: 1.062, acc: 62.515625%\n",
      "[17, 1000], time 1193.8310408592224 loss: 1.049, acc: 62.48125%\n",
      "[17, 1200], time 1195.4112486839294 loss: 1.058, acc: 62.44531250000001%\n",
      "[17, 1400], time 1196.991277217865 loss: 1.063, acc: 62.375%\n",
      "[17, 1600], time 1198.5740418434143 loss: 1.059, acc: 62.380859375%\n",
      "[17, 1800], time 1200.1521525382996 loss: 1.048, acc: 62.41840277777778%\n",
      "[17, 2000], time 1201.7097644805908 loss: 1.091, acc: 62.29687499999999%\n",
      "[17, 2200], time 1203.308874130249 loss: 1.055, acc: 62.32528409090909%\n",
      "[17, 2400], time 1204.8444826602936 loss: 1.062, acc: 62.32682291666667%\n",
      "[17, 2600], time 1206.4030945301056 loss: 1.058, acc: 62.38221153846154%\n",
      "[17, 2800], time 1207.9367032051086 loss: 1.078, acc: 62.32142857142857%\n",
      "[17, 3000], time 1209.5188145637512 loss: 1.061, acc: 62.35520833333334%\n",
      "[17, 3200], time 1211.0507771968842 loss: 1.096, acc: 62.27441406250001%\n",
      "[17, 3400], time 1212.6088898181915 loss: 1.079, acc: 62.25367647058824%\n",
      "[17, 3600], time 1214.16699385643 loss: 1.077, acc: 62.25086805555555%\n",
      "[17, 3800], time 1215.7316045761108 loss: 1.086, acc: 62.25411184210527%\n",
      "[17, 4000], time 1217.288714647293 loss: 1.094, acc: 62.21171875%\n",
      "[17, 4200], time 1218.852830171585 loss: 1.066, acc: 62.20833333333333%\n",
      "[17, 4400], time 1220.4380342960358 loss: 1.064, acc: 62.21590909090909%\n",
      "[17, 4600], time 1221.98468542099 loss: 1.063, acc: 62.22554347826087%\n",
      "[17, 4800], time 1223.5397551059723 loss: 1.079, acc: 62.216145833333336%\n",
      "[17, 5000], time 1225.1203782558441 loss: 1.082, acc: 62.173750000000005%\n",
      "[17, 5200], time 1226.696947336197 loss: 1.050, acc: 62.18689903846154%\n",
      "[17, 5400], time 1228.271556377411 loss: 1.088, acc: 62.145833333333336%\n",
      "[17, 5600], time 1229.8402743339539 loss: 1.103, acc: 62.08147321428571%\n",
      "epoch 17 TRAIN error: 1.068715249352985, acc: 0.6209944444444444\n",
      "epoch 17 VALIDATION error: 0.9941154718399048, acc: 0.6442888888888889\n",
      "[18, 200], time 1261.8718190193176 loss: 1.023, acc: 63.890625%\n",
      "[18, 400], time 1263.3964331150055 loss: 1.049, acc: 63.1015625%\n",
      "[18, 600], time 1264.924043893814 loss: 1.055, acc: 62.984375%\n",
      "[18, 800], time 1266.4605641365051 loss: 1.030, acc: 63.1875%\n",
      "[18, 1000], time 1267.9699032306671 loss: 1.058, acc: 63.05%\n",
      "[18, 1200], time 1269.5120222568512 loss: 1.063, acc: 62.825520833333336%\n",
      "[18, 1400], time 1271.0671224594116 loss: 1.082, acc: 62.738839285714285%\n",
      "[18, 1600], time 1272.6018199920654 loss: 1.094, acc: 62.57421875%\n",
      "[18, 1800], time 1274.13942527771 loss: 1.082, acc: 62.463541666666664%\n",
      "[18, 2000], time 1275.6941285133362 loss: 1.084, acc: 62.378125%\n",
      "[18, 2200], time 1277.237307548523 loss: 1.068, acc: 62.36647727272727%\n",
      "[18, 2400], time 1278.7554121017456 loss: 1.078, acc: 62.32421875000001%\n",
      "[18, 2600], time 1280.27751994133 loss: 1.067, acc: 62.32692307692308%\n",
      "[18, 2800], time 1281.8283870220184 loss: 1.057, acc: 62.322544642857146%\n",
      "[18, 3000], time 1283.3394927978516 loss: 1.063, acc: 62.305208333333326%\n",
      "[18, 3200], time 1284.8800961971283 loss: 1.071, acc: 62.28125%\n",
      "[18, 3400], time 1286.4123747348785 loss: 1.071, acc: 62.26194852941176%\n",
      "[18, 3600], time 1287.9524779319763 loss: 1.052, acc: 62.27083333333333%\n",
      "[18, 3800], time 1289.4365670681 loss: 1.057, acc: 62.291940789473685%\n",
      "[18, 4000], time 1290.9768843650818 loss: 1.085, acc: 62.2078125%\n",
      "[18, 4200], time 1292.5054926872253 loss: 1.057, acc: 62.22544642857143%\n",
      "[18, 4400], time 1294.0240936279297 loss: 1.100, acc: 62.15482954545455%\n",
      "[18, 4600], time 1295.5566928386688 loss: 1.087, acc: 62.14673913043478%\n",
      "[18, 4800], time 1297.0852937698364 loss: 1.059, acc: 62.14127604166667%\n",
      "[18, 5000], time 1298.5825026035309 loss: 1.079, acc: 62.11687500000001%\n",
      "[18, 5200], time 1300.1031048297882 loss: 1.085, acc: 62.08834134615384%\n",
      "[18, 5400], time 1301.6434919834137 loss: 1.052, acc: 62.09606481481481%\n",
      "[18, 5600], time 1303.1560909748077 loss: 1.085, acc: 62.09040178571429%\n",
      "epoch 18 TRAIN error: 1.0674173696411982, acc: 0.6210166666666667\n",
      "epoch 18 VALIDATION error: 0.99327552318573, acc: 0.6442333333333333\n",
      "[19, 200], time 1334.585177898407 loss: 1.020, acc: 63.20312500000001%\n",
      "[19, 400], time 1336.1434335708618 loss: 1.041, acc: 63.0%\n",
      "[19, 600], time 1337.6925354003906 loss: 1.056, acc: 62.83333333333333%\n",
      "[19, 800], time 1339.2426471710205 loss: 1.065, acc: 62.55078125%\n",
      "[19, 1000], time 1340.794293165207 loss: 1.037, acc: 62.696875%\n",
      "[19, 1200], time 1342.3364038467407 loss: 1.047, acc: 62.869791666666664%\n",
      "[19, 1400], time 1343.8800065517426 loss: 1.057, acc: 62.745535714285715%\n",
      "[19, 1600], time 1345.4440038204193 loss: 1.069, acc: 62.66796875%\n",
      "[19, 1800], time 1347.0131101608276 loss: 1.054, acc: 62.635416666666664%\n",
      "[19, 2000], time 1348.5597131252289 loss: 1.043, acc: 62.7171875%\n",
      "[19, 2200], time 1350.107815027237 loss: 1.065, acc: 62.62926136363637%\n",
      "[19, 2400], time 1351.6690154075623 loss: 1.045, acc: 62.65494791666667%\n",
      "[19, 2600], time 1353.2031168937683 loss: 1.076, acc: 62.57211538461539%\n",
      "[19, 2800], time 1354.7485120296478 loss: 1.067, acc: 62.53459821428572%\n",
      "[19, 3000], time 1356.2986059188843 loss: 1.075, acc: 62.525%\n",
      "[19, 3200], time 1357.8497066497803 loss: 1.076, acc: 62.41992187499999%\n",
      "[19, 3400], time 1359.3993136882782 loss: 1.080, acc: 62.329963235294116%\n",
      "[19, 3600], time 1360.9679157733917 loss: 1.063, acc: 62.28993055555555%\n",
      "[19, 3800], time 1362.5260169506073 loss: 1.088, acc: 62.209703947368425%\n",
      "[19, 4000], time 1364.065448999405 loss: 1.065, acc: 62.19296875%\n",
      "[19, 4200], time 1365.6437804698944 loss: 1.086, acc: 62.130208333333336%\n",
      "[19, 4400], time 1367.2088372707367 loss: 1.070, acc: 62.11008522727273%\n",
      "[19, 4600], time 1368.7529418468475 loss: 1.047, acc: 62.15013586956522%\n",
      "[19, 4800], time 1370.306880235672 loss: 1.070, acc: 62.130208333333336%\n",
      "[19, 5000], time 1371.8674800395966 loss: 1.049, acc: 62.19749999999999%\n",
      "[19, 5200], time 1373.4165768623352 loss: 1.080, acc: 62.152644230769226%\n",
      "[19, 5400], time 1374.978178024292 loss: 1.048, acc: 62.15798611111111%\n",
      "[19, 5600], time 1376.5329418182373 loss: 1.065, acc: 62.16685267857143%\n",
      "epoch 19 TRAIN error: 1.061237893655565, acc: 0.6216222222222222\n",
      "epoch 19 VALIDATION error: 0.9911692142486572, acc: 0.6468888888888888\n",
      "[20, 200], time 1407.707844734192 loss: 1.041, acc: 62.78125%\n",
      "[20, 400], time 1409.2254540920258 loss: 1.051, acc: 62.89843750000001%\n",
      "[20, 600], time 1410.7734892368317 loss: 1.056, acc: 62.734375%\n",
      "[20, 800], time 1412.30109333992 loss: 1.035, acc: 62.80859375%\n",
      "[20, 1000], time 1413.852961063385 loss: 1.042, acc: 62.796875%\n",
      "[20, 1200], time 1415.3965709209442 loss: 1.050, acc: 62.81250000000001%\n",
      "[20, 1400], time 1416.9426803588867 loss: 1.064, acc: 62.69642857142858%\n",
      "[20, 1600], time 1418.483283996582 loss: 1.038, acc: 62.74999999999999%\n",
      "[20, 1800], time 1420.0113985538483 loss: 1.075, acc: 62.72916666666667%\n",
      "[20, 2000], time 1421.553092956543 loss: 1.056, acc: 62.665625000000006%\n",
      "[20, 2200], time 1423.0846915245056 loss: 1.029, acc: 62.72159090909091%\n",
      "[20, 2400], time 1424.6497025489807 loss: 1.032, acc: 62.83203125%\n",
      "[20, 2600], time 1426.194803237915 loss: 1.034, acc: 62.847355769230774%\n",
      "[20, 2800], time 1427.731205701828 loss: 1.047, acc: 62.840401785714285%\n",
      "[20, 3000], time 1429.2718014717102 loss: 1.071, acc: 62.78125%\n",
      "[20, 3200], time 1430.8161315917969 loss: 1.049, acc: 62.8037109375%\n",
      "[20, 3400], time 1432.361979484558 loss: 1.044, acc: 62.78860294117648%\n",
      "[20, 3600], time 1433.8910779953003 loss: 1.056, acc: 62.78819444444444%\n",
      "[20, 3800], time 1435.4311168193817 loss: 1.067, acc: 62.75740131578947%\n",
      "[20, 4000], time 1436.970718383789 loss: 1.041, acc: 62.75859375%\n",
      "[20, 4200], time 1438.48295712471 loss: 1.075, acc: 62.69642857142858%\n",
      "[20, 4400], time 1440.0240619182587 loss: 1.075, acc: 62.66477272727272%\n",
      "[20, 4600], time 1441.5401656627655 loss: 1.045, acc: 62.66711956521739%\n",
      "[20, 4800], time 1443.0507667064667 loss: 1.062, acc: 62.665364583333336%\n",
      "[20, 5000], time 1444.5383667945862 loss: 1.044, acc: 62.67625%\n",
      "[20, 5200], time 1446.0724625587463 loss: 1.056, acc: 62.65324519230769%\n",
      "[20, 5400], time 1447.6025607585907 loss: 1.070, acc: 62.609375%\n",
      "[20, 5600], time 1449.1416971683502 loss: 1.070, acc: 62.56529017857143%\n",
      "epoch 20 TRAIN error: 1.0527842544237773, acc: 0.6257277777777778\n",
      "epoch 20 VALIDATION error: 0.9829608798027039, acc: 0.6476666666666666\n",
      "[21, 200], time 1480.6098110675812 loss: 1.039, acc: 63.1875%\n",
      "[21, 400], time 1482.1253471374512 loss: 1.026, acc: 63.44531249999999%\n",
      "[21, 600], time 1483.6385514736176 loss: 1.048, acc: 63.11979166666667%\n",
      "[21, 800], time 1485.1815531253815 loss: 1.069, acc: 62.94140625%\n",
      "[21, 1000], time 1486.7098562717438 loss: 1.064, acc: 62.9%\n",
      "[21, 1200], time 1488.2250418663025 loss: 1.019, acc: 63.067708333333336%\n",
      "[21, 1400], time 1489.755145072937 loss: 1.048, acc: 63.015625%\n",
      "[21, 1600], time 1491.3008782863617 loss: 1.043, acc: 63.01171875%\n",
      "[21, 1800], time 1492.8079767227173 loss: 1.036, acc: 63.07118055555555%\n",
      "[21, 2000], time 1494.3430831432343 loss: 1.060, acc: 62.9953125%\n",
      "[21, 2200], time 1495.8956906795502 loss: 1.047, acc: 62.97585227272727%\n",
      "[21, 2400], time 1497.4297924041748 loss: 1.066, acc: 62.94531249999999%\n",
      "[21, 2600], time 1498.9633979797363 loss: 1.028, acc: 62.99158653846154%\n",
      "[21, 2800], time 1500.5030629634857 loss: 1.063, acc: 62.94531249999999%\n",
      "[21, 3000], time 1502.0271656513214 loss: 1.066, acc: 62.91041666666667%\n",
      "[21, 3200], time 1503.5377638339996 loss: 1.067, acc: 62.88671875%\n",
      "[21, 3400], time 1505.051608324051 loss: 1.070, acc: 62.81066176470588%\n",
      "[21, 3600], time 1506.5987045764923 loss: 1.060, acc: 62.76649305555555%\n",
      "[21, 3800], time 1508.1303021907806 loss: 1.061, acc: 62.78700657894737%\n",
      "[21, 4000], time 1509.663412809372 loss: 1.080, acc: 62.724218750000006%\n",
      "[21, 4200], time 1511.2060110569 loss: 1.052, acc: 62.73139880952381%\n",
      "[21, 4400], time 1512.7364056110382 loss: 1.060, acc: 62.72656250000001%\n",
      "[21, 4600], time 1514.243566274643 loss: 1.036, acc: 62.76086956521739%\n",
      "[21, 4800], time 1515.7796742916107 loss: 1.050, acc: 62.78255208333333%\n",
      "[21, 5000], time 1517.306307554245 loss: 1.047, acc: 62.80625%\n",
      "[21, 5200], time 1518.848328113556 loss: 1.078, acc: 62.77103365384615%\n",
      "[21, 5400], time 1520.3906602859497 loss: 1.082, acc: 62.717013888888886%\n",
      "[21, 5600], time 1521.9288828372955 loss: 1.041, acc: 62.72098214285714%\n",
      "epoch 21 TRAIN error: 1.053758873436186, acc: 0.6272833333333333\n",
      "epoch 21 VALIDATION error: 0.9896100163459778, acc: 0.6456333333333333\n",
      "[22, 200], time 1553.3110826015472 loss: 1.028, acc: 63.21875%\n",
      "[22, 400], time 1554.8511900901794 loss: 1.054, acc: 62.6796875%\n",
      "[22, 600], time 1556.3842904567719 loss: 1.011, acc: 63.036458333333336%\n",
      "[22, 800], time 1557.952548980713 loss: 1.028, acc: 63.0625%\n",
      "[22, 1000], time 1559.4746572971344 loss: 1.068, acc: 62.965625%\n",
      "[22, 1200], time 1561.0062518119812 loss: 1.037, acc: 62.979166666666664%\n",
      "[22, 1400], time 1562.5367970466614 loss: 1.047, acc: 62.99553571428571%\n",
      "[22, 1600], time 1564.0788969993591 loss: 1.047, acc: 62.978515625%\n",
      "[22, 1800], time 1565.5670001506805 loss: 1.058, acc: 62.79166666666667%\n",
      "[22, 2000], time 1567.0902059078217 loss: 1.049, acc: 62.853125%\n",
      "[22, 2200], time 1568.5966019630432 loss: 1.037, acc: 62.89630681818181%\n",
      "[22, 2400], time 1570.1224167346954 loss: 1.070, acc: 62.774739583333336%\n",
      "[22, 2600], time 1571.648515701294 loss: 1.055, acc: 62.76442307692308%\n",
      "[22, 2800], time 1573.166118144989 loss: 1.022, acc: 62.81250000000001%\n",
      "[22, 3000], time 1574.6897180080414 loss: 1.065, acc: 62.77604166666667%\n",
      "[22, 3200], time 1576.242817401886 loss: 1.081, acc: 62.7060546875%\n",
      "[22, 3400], time 1577.7676973342896 loss: 1.051, acc: 62.67922794117647%\n",
      "[22, 3600], time 1579.308604001999 loss: 1.070, acc: 62.611979166666664%\n",
      "[22, 3800], time 1580.8247096538544 loss: 1.030, acc: 62.62911184210527%\n",
      "[22, 4000], time 1582.3628220558167 loss: 1.062, acc: 62.615624999999994%\n",
      "[22, 4200], time 1583.8989267349243 loss: 1.054, acc: 62.60342261904762%\n",
      "[22, 4400], time 1585.4150321483612 loss: 1.055, acc: 62.60582386363637%\n",
      "[22, 4600], time 1586.9538474082947 loss: 1.040, acc: 62.6358695652174%\n",
      "[22, 4800], time 1588.4914581775665 loss: 1.086, acc: 62.56901041666667%\n",
      "[22, 5000], time 1590.01979470253 loss: 1.049, acc: 62.588750000000005%\n",
      "[22, 5200], time 1591.5614023208618 loss: 1.076, acc: 62.58112980769231%\n",
      "[22, 5400], time 1593.071620941162 loss: 1.056, acc: 62.58101851851852%\n",
      "[22, 5600], time 1594.561719417572 loss: 1.067, acc: 62.58872767857143%\n",
      "epoch 22 TRAIN error: 1.0518644208590189, acc: 0.6258\n",
      "epoch 22 VALIDATION error: 1.0028797388076782, acc: 0.6436888888888889\n",
      "[23, 200], time 1625.6759622097015 loss: 1.040, acc: 62.984375%\n",
      "[23, 400], time 1627.2085616588593 loss: 1.001, acc: 63.7578125%\n",
      "[23, 600], time 1628.7481615543365 loss: 1.056, acc: 63.463541666666664%\n",
      "[23, 800], time 1630.2837677001953 loss: 1.029, acc: 63.51953125000001%\n",
      "[23, 1000], time 1631.8513746261597 loss: 1.039, acc: 63.384375000000006%\n",
      "[23, 1200], time 1633.4114820957184 loss: 1.046, acc: 63.348958333333336%\n",
      "[23, 1400], time 1634.9670958518982 loss: 1.050, acc: 63.270089285714285%\n",
      "[23, 1600], time 1636.5532088279724 loss: 1.015, acc: 63.353515625%\n",
      "[23, 1800], time 1638.1458172798157 loss: 1.056, acc: 63.28472222222222%\n",
      "[23, 2000], time 1639.7149300575256 loss: 1.083, acc: 63.096875%\n",
      "[23, 2200], time 1641.2495367527008 loss: 1.036, acc: 63.10795454545455%\n",
      "[23, 2400], time 1642.7616381645203 loss: 1.047, acc: 63.04557291666667%\n",
      "[23, 2600], time 1644.3832478523254 loss: 1.033, acc: 63.09375000000001%\n",
      "[23, 2800], time 1645.9958519935608 loss: 1.046, acc: 63.09486607142857%\n",
      "[23, 3000], time 1647.6149592399597 loss: 1.054, acc: 63.05%\n",
      "[23, 3200], time 1649.2075626850128 loss: 1.050, acc: 63.02050781250001%\n",
      "[23, 3400], time 1650.738661289215 loss: 1.057, acc: 63.025735294117645%\n",
      "[23, 3600], time 1652.2522649765015 loss: 1.050, acc: 63.04253472222222%\n",
      "[23, 3800], time 1653.7783679962158 loss: 1.038, acc: 63.05838815789474%\n",
      "[23, 4000], time 1655.2995631694794 loss: 1.050, acc: 62.98359375%\n",
      "[23, 4200], time 1656.828661441803 loss: 1.057, acc: 62.95907738095238%\n",
      "[23, 4400], time 1658.3602643013 loss: 1.078, acc: 62.90482954545455%\n",
      "[23, 4600], time 1659.895869731903 loss: 1.020, acc: 62.92798913043478%\n",
      "[23, 4800], time 1661.418475151062 loss: 1.058, acc: 62.927083333333336%\n",
      "[23, 5000], time 1662.9405813217163 loss: 1.066, acc: 62.918125%\n",
      "[23, 5200], time 1664.4646861553192 loss: 1.049, acc: 62.91947115384615%\n",
      "[23, 5400], time 1665.9862854480743 loss: 1.066, acc: 62.90104166666667%\n",
      "[23, 5600], time 1667.5293834209442 loss: 1.041, acc: 62.89508928571429%\n",
      "epoch 23 TRAIN error: 1.0467372585190666, acc: 0.62895\n",
      "epoch 23 VALIDATION error: 0.9828402996063232, acc: 0.6499111111111111\n",
      "[24, 200], time 1698.7223415374756 loss: 1.028, acc: 63.4375%\n",
      "[24, 400], time 1700.2344391345978 loss: 1.034, acc: 63.171875%\n",
      "[24, 600], time 1701.7605509757996 loss: 1.039, acc: 63.265625%\n",
      "[24, 800], time 1703.2671511173248 loss: 1.009, acc: 63.51171875%\n",
      "[24, 1000], time 1704.7752571105957 loss: 1.025, acc: 63.646875%\n",
      "[24, 1200], time 1706.2803626060486 loss: 1.042, acc: 63.559895833333336%\n",
      "[24, 1400], time 1707.8214733600616 loss: 1.043, acc: 63.473214285714285%\n",
      "[24, 1600], time 1709.3800916671753 loss: 1.060, acc: 63.35937499999999%\n",
      "[24, 1800], time 1710.9252066612244 loss: 1.023, acc: 63.322916666666664%\n",
      "[24, 2000], time 1712.4333171844482 loss: 1.064, acc: 63.27343749999999%\n",
      "[24, 2200], time 1713.913433790207 loss: 1.066, acc: 63.19744318181818%\n",
      "[24, 2400], time 1715.4135701656342 loss: 1.051, acc: 63.16015625%\n",
      "[24, 2600], time 1716.9161953926086 loss: 1.040, acc: 63.161057692307686%\n",
      "[24, 2800], time 1718.4263000488281 loss: 1.039, acc: 63.16964285714286%\n",
      "[24, 3000], time 1719.9214057922363 loss: 1.042, acc: 63.18645833333333%\n",
      "[24, 3200], time 1721.4170098304749 loss: 1.037, acc: 63.18847656249999%\n",
      "[24, 3400], time 1722.9366579055786 loss: 1.048, acc: 63.184742647058826%\n",
      "[24, 3600], time 1724.4322605133057 loss: 1.030, acc: 63.21788194444444%\n",
      "[24, 3800], time 1725.9368681907654 loss: 1.044, acc: 63.20559210526315%\n",
      "[24, 4000], time 1727.4434807300568 loss: 1.068, acc: 63.100781250000004%\n",
      "[24, 4200], time 1728.949584722519 loss: 1.079, acc: 63.0014880952381%\n",
      "[24, 4400], time 1730.4466876983643 loss: 1.042, acc: 63.0%\n",
      "[24, 4600], time 1731.9527909755707 loss: 1.062, acc: 62.96739130434783%\n",
      "[24, 4800], time 1733.446897983551 loss: 1.050, acc: 62.952473958333336%\n",
      "[24, 5000], time 1734.955509185791 loss: 1.073, acc: 62.90125%\n",
      "[24, 5200], time 1736.4581100940704 loss: 1.063, acc: 62.87620192307693%\n",
      "[24, 5400], time 1737.9527080059052 loss: 1.050, acc: 62.86631944444444%\n",
      "[24, 5600], time 1739.4503061771393 loss: 1.049, acc: 62.861607142857146%\n",
      "epoch 24 TRAIN error: 1.0463880225340525, acc: 0.6286222222222222\n",
      "epoch 24 VALIDATION error: 0.9895308017730713, acc: 0.6462666666666667\n",
      "[25, 200], time 1770.267169713974 loss: 1.033, acc: 63.375%\n",
      "[25, 400], time 1771.78577709198 loss: 1.033, acc: 63.046875%\n",
      "[25, 600], time 1773.28639960289 loss: 1.011, acc: 63.526041666666664%\n",
      "[25, 800], time 1774.8055193424225 loss: 1.029, acc: 63.42578125%\n",
      "[25, 1000], time 1776.3086397647858 loss: 1.040, acc: 63.4%\n",
      "[25, 1200], time 1777.8172578811646 loss: 1.023, acc: 63.53124999999999%\n",
      "[25, 1400], time 1779.3273742198944 loss: 1.036, acc: 63.41294642857142%\n",
      "[25, 1600], time 1780.8369898796082 loss: 1.047, acc: 63.28906250000001%\n",
      "[25, 1800], time 1782.3326048851013 loss: 1.055, acc: 63.208333333333336%\n",
      "[25, 2000], time 1783.8382313251495 loss: 1.052, acc: 63.151562500000004%\n",
      "[25, 2200], time 1785.4108545780182 loss: 1.042, acc: 63.200284090909086%\n",
      "[25, 2400], time 1786.942456960678 loss: 1.038, acc: 63.21744791666667%\n",
      "[25, 2600], time 1788.4705715179443 loss: 1.038, acc: 63.20913461538461%\n",
      "[25, 2800], time 1789.9956812858582 loss: 1.053, acc: 63.22321428571429%\n",
      "[25, 3000], time 1791.5036115646362 loss: 1.042, acc: 63.215624999999996%\n",
      "[25, 3200], time 1793.0282213687897 loss: 1.049, acc: 63.20117187499999%\n",
      "[25, 3400], time 1794.543331861496 loss: 1.061, acc: 63.14705882352941%\n",
      "[25, 3600], time 1796.090876340866 loss: 1.034, acc: 63.1875%\n",
      "[25, 3800], time 1797.612975358963 loss: 1.050, acc: 63.15378289473684%\n",
      "[25, 4000], time 1799.1450731754303 loss: 1.051, acc: 63.157031249999996%\n",
      "[25, 4200], time 1800.670678138733 loss: 1.052, acc: 63.109375%\n",
      "[25, 4400], time 1802.2004601955414 loss: 1.048, acc: 63.08948863636363%\n",
      "[25, 4600], time 1803.6862952709198 loss: 1.076, acc: 63.031929347826086%\n",
      "[25, 4800], time 1805.2204005718231 loss: 1.045, acc: 63.018880208333336%\n",
      "[25, 5000], time 1806.7200002670288 loss: 1.049, acc: 63.023125%\n",
      "[25, 5200], time 1808.2470965385437 loss: 1.037, acc: 63.04026442307692%\n",
      "[25, 5400], time 1809.7804772853851 loss: 1.050, acc: 63.042245370370374%\n",
      "[25, 5600], time 1811.3025839328766 loss: 1.055, acc: 63.01339285714286%\n",
      "epoch 25 TRAIN error: 1.043713127623664, acc: 0.6302333333333333\n",
      "epoch 25 VALIDATION error: 0.9877158999443054, acc: 0.6504444444444445\n",
      "[26, 200], time 1842.6424400806427 loss: 1.019, acc: 64.78125%\n",
      "[26, 400], time 1844.149034023285 loss: 1.026, acc: 64.203125%\n",
      "[26, 600], time 1845.6540920734406 loss: 1.010, acc: 64.140625%\n",
      "[26, 800], time 1847.1781940460205 loss: 1.004, acc: 64.2109375%\n",
      "[26, 1000], time 1848.6849746704102 loss: 1.019, acc: 64.2375%\n",
      "[26, 1200], time 1850.212287902832 loss: 1.051, acc: 64.015625%\n",
      "[26, 1400], time 1851.7313935756683 loss: 1.017, acc: 64.00892857142857%\n",
      "[26, 1600], time 1853.2604928016663 loss: 1.045, acc: 63.90234375%\n",
      "[26, 1800], time 1854.786601305008 loss: 1.043, acc: 63.84722222222222%\n",
      "[26, 2000], time 1856.3236167430878 loss: 1.044, acc: 63.801562499999996%\n",
      "[26, 2200], time 1857.8512206077576 loss: 1.038, acc: 63.78693181818181%\n",
      "[26, 2400], time 1859.3808245658875 loss: 1.020, acc: 63.7578125%\n",
      "[26, 2600], time 1860.909431695938 loss: 1.056, acc: 63.61177884615385%\n",
      "[26, 2800], time 1862.4430317878723 loss: 1.047, acc: 63.50669642857143%\n",
      "[26, 3000], time 1863.930846452713 loss: 1.058, acc: 63.432291666666664%\n",
      "[26, 3200], time 1865.4619472026825 loss: 1.031, acc: 63.427734375%\n",
      "[26, 3400], time 1866.9780530929565 loss: 1.041, acc: 63.377757352941174%\n",
      "[26, 3600], time 1868.5071630477905 loss: 1.040, acc: 63.37152777777778%\n",
      "[26, 3800], time 1870.0455327033997 loss: 1.044, acc: 63.36924342105264%\n",
      "[26, 4000], time 1871.5712177753448 loss: 1.027, acc: 63.4046875%\n",
      "[26, 4200], time 1873.1028292179108 loss: 1.031, acc: 63.41592261904761%\n",
      "[26, 4400], time 1874.6268582344055 loss: 1.030, acc: 63.41122159090909%\n",
      "[26, 4600], time 1876.1434683799744 loss: 1.048, acc: 63.38790760869565%\n",
      "[26, 4800], time 1877.6655731201172 loss: 1.019, acc: 63.414713541666664%\n",
      "[26, 5000], time 1879.1892757415771 loss: 1.054, acc: 63.366875%\n",
      "[26, 5200], time 1880.7143750190735 loss: 1.049, acc: 63.30228365384616%\n",
      "[26, 5400], time 1882.2374820709229 loss: 1.045, acc: 63.25694444444444%\n",
      "[26, 5600], time 1883.7665796279907 loss: 1.054, acc: 63.23716517857143%\n",
      "epoch 26 TRAIN error: 1.0359799579726325, acc: 0.6323722222222222\n",
      "epoch 26 VALIDATION error: 0.9991299510002136, acc: 0.6442\n",
      "[27, 200], time 1915.0966346263885 loss: 1.026, acc: 63.28125%\n",
      "[27, 400], time 1916.6241517066956 loss: 1.027, acc: 63.21875%\n",
      "[27, 600], time 1918.1457543373108 loss: 1.017, acc: 63.604166666666664%\n",
      "[27, 800], time 1919.687355518341 loss: 0.987, acc: 64.07421875%\n",
      "[27, 1000], time 1921.2114655971527 loss: 1.017, acc: 63.91875%\n",
      "[27, 1200], time 1922.717574596405 loss: 1.029, acc: 63.911458333333336%\n",
      "[27, 1400], time 1924.2641665935516 loss: 1.034, acc: 63.879464285714285%\n",
      "[27, 1600], time 1925.8027787208557 loss: 1.034, acc: 63.900390625%\n",
      "[27, 1800], time 1927.345380306244 loss: 1.032, acc: 63.880208333333336%\n",
      "[27, 2000], time 1928.8657631874084 loss: 1.035, acc: 63.8171875%\n",
      "[27, 2200], time 1930.3788621425629 loss: 1.050, acc: 63.72443181818181%\n",
      "[27, 2400], time 1931.9144673347473 loss: 1.021, acc: 63.72395833333333%\n",
      "[27, 2600], time 1933.450063943863 loss: 1.044, acc: 63.68509615384615%\n",
      "[27, 2800], time 1935.003674030304 loss: 1.051, acc: 63.60044642857143%\n",
      "[27, 3000], time 1936.535496711731 loss: 1.050, acc: 63.55833333333333%\n",
      "[27, 3200], time 1938.0702955722809 loss: 1.039, acc: 63.5703125%\n",
      "[27, 3400], time 1939.6134045124054 loss: 1.042, acc: 63.49632352941177%\n",
      "[27, 3600], time 1941.148517370224 loss: 1.030, acc: 63.50520833333333%\n",
      "[27, 3800], time 1942.6787147521973 loss: 1.030, acc: 63.45394736842105%\n",
      "[27, 4000], time 1944.2078239917755 loss: 1.042, acc: 63.440625%\n",
      "[27, 4200], time 1945.7149300575256 loss: 1.045, acc: 63.414434523809526%\n",
      "[27, 4400], time 1947.2645308971405 loss: 1.048, acc: 63.34801136363636%\n",
      "[27, 4600], time 1948.8011496067047 loss: 1.036, acc: 63.34578804347826%\n",
      "[27, 4800], time 1950.353627204895 loss: 1.049, acc: 63.328125%\n",
      "[27, 5000], time 1951.891731262207 loss: 1.045, acc: 63.290625%\n",
      "[27, 5200], time 1953.3918294906616 loss: 1.047, acc: 63.27944711538461%\n",
      "[27, 5400], time 1954.9046111106873 loss: 1.045, acc: 63.24884259259259%\n",
      "[27, 5600], time 1956.4372091293335 loss: 1.060, acc: 63.21372767857143%\n",
      "epoch 27 TRAIN error: 1.036017024146186, acc: 0.6322388888888889\n",
      "epoch 27 VALIDATION error: 0.9818009734153748, acc: 0.6523\n",
      "[28, 200], time 1987.8132417201996 loss: 1.013, acc: 63.81250000000001%\n",
      "[28, 400], time 1989.3522231578827 loss: 1.035, acc: 64.015625%\n",
      "[28, 600], time 1990.85182929039 loss: 1.017, acc: 64.109375%\n",
      "[28, 800], time 1992.3754351139069 loss: 1.050, acc: 63.81250000000001%\n",
      "[28, 1000], time 1993.9015412330627 loss: 1.017, acc: 63.91875%\n",
      "[28, 1200], time 1995.4421541690826 loss: 1.049, acc: 63.734375%\n",
      "[28, 1400], time 1996.9672527313232 loss: 1.026, acc: 63.66517857142857%\n",
      "[28, 1600], time 1998.501359462738 loss: 1.022, acc: 63.5859375%\n",
      "[28, 1800], time 2000.0404715538025 loss: 1.040, acc: 63.527777777777786%\n",
      "[28, 2000], time 2001.564579963684 loss: 1.003, acc: 63.5953125%\n",
      "[28, 2200], time 2003.1157977581024 loss: 1.058, acc: 63.482954545454554%\n",
      "[28, 2400], time 2004.6473948955536 loss: 1.024, acc: 63.52734375%\n",
      "[28, 2600], time 2006.1824934482574 loss: 1.024, acc: 63.50480769230769%\n",
      "[28, 2800], time 2007.6795988082886 loss: 1.022, acc: 63.494419642857146%\n",
      "[28, 3000], time 2009.21129488945 loss: 1.025, acc: 63.53541666666666%\n",
      "[28, 3200], time 2010.7303948402405 loss: 1.034, acc: 63.5263671875%\n",
      "[28, 3400], time 2012.2614994049072 loss: 1.051, acc: 63.502757352941174%\n",
      "[28, 3600], time 2013.7955975532532 loss: 1.026, acc: 63.51909722222222%\n",
      "[28, 3800], time 2015.3492212295532 loss: 1.048, acc: 63.45723684210526%\n",
      "[28, 4000], time 2016.8933250904083 loss: 1.030, acc: 63.48437500000001%\n",
      "[28, 4200], time 2018.4380679130554 loss: 1.040, acc: 63.480654761904766%\n",
      "[28, 4400], time 2019.961166858673 loss: 1.040, acc: 63.43323863636363%\n",
      "[28, 4600], time 2021.504677772522 loss: 1.035, acc: 63.41372282608696%\n",
      "[28, 4800], time 2023.0402827262878 loss: 1.048, acc: 63.397786458333336%\n",
      "[28, 5000], time 2024.581995010376 loss: 1.041, acc: 63.403125%\n",
      "[28, 5200], time 2026.132108449936 loss: 1.048, acc: 63.39182692307692%\n",
      "[28, 5400], time 2027.684892654419 loss: 1.052, acc: 63.386574074074076%\n",
      "[28, 5600], time 2029.231190919876 loss: 1.058, acc: 63.340401785714285%\n",
      "epoch 28 TRAIN error: 1.034920558834076, acc: 0.6333277777777778\n",
      "epoch 28 VALIDATION error: 0.982128918170929, acc: 0.6486222222222222\n",
      "[29, 200], time 2060.7445566654205 loss: 1.017, acc: 64.875%\n",
      "[29, 400], time 2062.271656036377 loss: 1.002, acc: 64.5%\n",
      "[29, 600], time 2063.813757419586 loss: 1.003, acc: 64.515625%\n",
      "[29, 800], time 2065.3548719882965 loss: 1.015, acc: 64.328125%\n",
      "[29, 1000], time 2066.9414784908295 loss: 1.024, acc: 64.165625%\n",
      "[29, 1200], time 2068.5355894565582 loss: 1.056, acc: 63.86197916666667%\n",
      "[29, 1400], time 2070.116700410843 loss: 1.029, acc: 63.910714285714285%\n",
      "[29, 1600], time 2071.6938095092773 loss: 1.040, acc: 63.83593749999999%\n",
      "[29, 1800], time 2073.2269158363342 loss: 1.010, acc: 63.83680555555556%\n",
      "[29, 2000], time 2074.762511730194 loss: 1.017, acc: 63.800000000000004%\n",
      "[29, 2200], time 2076.3251202106476 loss: 1.021, acc: 63.83806818181819%\n",
      "[29, 2400], time 2077.828727245331 loss: 1.022, acc: 63.81510416666667%\n",
      "[29, 2600], time 2079.329826593399 loss: 1.044, acc: 63.74399038461539%\n",
      "[29, 2800], time 2080.834925174713 loss: 1.049, acc: 63.72321428571428%\n",
      "[29, 3000], time 2082.336524963379 loss: 1.012, acc: 63.76145833333333%\n",
      "[29, 3200], time 2083.837630033493 loss: 1.023, acc: 63.71484375000001%\n",
      "[29, 3400], time 2085.335734128952 loss: 1.027, acc: 63.6875%\n",
      "[29, 3600], time 2086.8333382606506 loss: 1.023, acc: 63.6484375%\n",
      "[29, 3800], time 2088.330945253372 loss: 1.052, acc: 63.611842105263165%\n",
      "[29, 4000], time 2089.831046104431 loss: 1.043, acc: 63.567968750000006%\n",
      "[29, 4200], time 2091.336654186249 loss: 1.056, acc: 63.494791666666664%\n",
      "[29, 4400], time 2092.846258163452 loss: 1.049, acc: 63.45454545454545%\n",
      "[29, 4600], time 2094.342360496521 loss: 1.024, acc: 63.485054347826086%\n",
      "[29, 4800], time 2095.8504588603973 loss: 1.045, acc: 63.439453125%\n",
      "[29, 5000], time 2097.351063489914 loss: 1.039, acc: 63.42250000000001%\n",
      "[29, 5200], time 2098.8576698303223 loss: 1.068, acc: 63.38341346153846%\n",
      "[29, 5400], time 2100.352771997452 loss: 1.043, acc: 63.38773148148148%\n",
      "[29, 5600], time 2101.847371339798 loss: 1.056, acc: 63.35993303571429%\n",
      "epoch 29 TRAIN error: 1.0326983816305797, acc: 0.6334055555555556\n",
      "epoch 29 VALIDATION error: 0.9806821346282959, acc: 0.6496888888888889\n",
      "[30, 200], time 2133.358677148819 loss: 0.998, acc: 64.78125%\n",
      "[30, 400], time 2134.9154024124146 loss: 0.998, acc: 64.7421875%\n",
      "[30, 600], time 2136.5011262893677 loss: 1.017, acc: 64.359375%\n",
      "[30, 800], time 2138.07088804245 loss: 1.027, acc: 64.18359375%\n",
      "[30, 1000], time 2139.6375739574432 loss: 1.037, acc: 63.98437500000001%\n",
      "[30, 1200], time 2141.2192537784576 loss: 1.019, acc: 64.046875%\n",
      "[30, 1400], time 2142.7924365997314 loss: 1.020, acc: 64.02455357142857%\n",
      "[30, 1600], time 2144.3636145591736 loss: 1.037, acc: 63.888671875%\n",
      "[30, 1800], time 2145.9122910499573 loss: 1.037, acc: 63.79340277777777%\n",
      "[30, 2000], time 2147.4769706726074 loss: 1.023, acc: 63.7765625%\n",
      "[30, 2200], time 2149.046148300171 loss: 1.025, acc: 63.75994318181818%\n",
      "[30, 2400], time 2150.606322288513 loss: 1.044, acc: 63.697916666666664%\n",
      "[30, 2600], time 2152.1605501174927 loss: 1.027, acc: 63.76442307692308%\n",
      "[30, 2800], time 2153.712289094925 loss: 1.043, acc: 63.705357142857146%\n",
      "[30, 3000], time 2155.266968727112 loss: 1.035, acc: 63.65%\n",
      "[30, 3200], time 2156.822144508362 loss: 1.015, acc: 63.671875%\n",
      "[30, 3400], time 2158.381836414337 loss: 1.016, acc: 63.71231617647059%\n",
      "[30, 3600], time 2159.9435081481934 loss: 1.028, acc: 63.717013888888886%\n",
      "[30, 3800], time 2161.507745742798 loss: 1.055, acc: 63.66529605263158%\n",
      "[30, 4000], time 2163.068925857544 loss: 1.060, acc: 63.589062500000004%\n",
      "[30, 4200], time 2164.6221175193787 loss: 1.063, acc: 63.526041666666664%\n",
      "[30, 4400], time 2166.175854444504 loss: 1.068, acc: 63.458096590909086%\n",
      "[30, 4600], time 2167.7356791496277 loss: 1.022, acc: 63.491168478260875%\n",
      "[30, 4800], time 2169.291852712631 loss: 1.056, acc: 63.4609375%\n",
      "[30, 5000], time 2170.8390486240387 loss: 1.065, acc: 63.435624999999995%\n",
      "[30, 5200], time 2172.380260705948 loss: 1.037, acc: 63.43930288461539%\n",
      "[30, 5400], time 2173.9504630565643 loss: 1.017, acc: 63.43981481481481%\n",
      "[30, 5600], time 2175.4976511001587 loss: 1.050, acc: 63.38616071428571%\n",
      "epoch 30 TRAIN error: 1.0333620156976913, acc: 0.6339055555555556\n",
      "epoch 30 VALIDATION error: 0.9796306490898132, acc: 0.6495555555555556\n",
      "[31, 200], time 2207.8428313732147 loss: 1.007, acc: 63.625%\n",
      "[31, 400], time 2209.3594329357147 loss: 1.020, acc: 63.89843750000001%\n",
      "[31, 600], time 2210.9284620285034 loss: 1.006, acc: 63.92187499999999%\n",
      "[31, 800], time 2212.5325725078583 loss: 1.012, acc: 64.0859375%\n",
      "[31, 1000], time 2214.1426787376404 loss: 1.003, acc: 64.1875%\n",
      "[31, 1200], time 2215.7487885951996 loss: 1.029, acc: 64.08854166666667%\n",
      "[31, 1400], time 2217.364894390106 loss: 1.013, acc: 64.015625%\n",
      "[31, 1600], time 2218.9399995803833 loss: 1.028, acc: 63.97265625000001%\n",
      "[31, 1800], time 2220.481101036072 loss: 1.032, acc: 63.888888888888886%\n",
      "[31, 2000], time 2222.0627148151398 loss: 1.038, acc: 63.86875%\n",
      "[31, 2200], time 2223.6293182373047 loss: 1.007, acc: 63.92045454545454%\n",
      "[31, 2400], time 2225.166428565979 loss: 1.048, acc: 63.80859375%\n",
      "[31, 2600], time 2226.733357191086 loss: 1.032, acc: 63.753605769230774%\n",
      "[31, 2800], time 2228.3229656219482 loss: 1.025, acc: 63.75111607142857%\n",
      "[31, 3000], time 2229.911074399948 loss: 1.023, acc: 63.74895833333334%\n",
      "[31, 3200], time 2231.498188972473 loss: 1.050, acc: 63.6708984375%\n",
      "[31, 3400], time 2233.095301628113 loss: 1.046, acc: 63.58639705882353%\n",
      "[31, 3600], time 2234.6975572109222 loss: 1.021, acc: 63.6015625%\n",
      "[31, 3800], time 2236.305168390274 loss: 1.034, acc: 63.577302631578945%\n",
      "[31, 4000], time 2237.9057760238647 loss: 1.033, acc: 63.590625%\n",
      "[31, 4200], time 2239.518889904022 loss: 1.055, acc: 63.520089285714285%\n",
      "[31, 4400], time 2241.1084990501404 loss: 1.051, acc: 63.50426136363636%\n",
      "[31, 4600], time 2242.621105194092 loss: 1.046, acc: 63.44497282608695%\n",
      "[31, 4800], time 2244.2567172050476 loss: 1.027, acc: 63.45898437500001%\n",
      "[31, 5000], time 2245.8899297714233 loss: 1.033, acc: 63.449999999999996%\n",
      "[31, 5200], time 2247.5071861743927 loss: 1.044, acc: 63.45072115384616%\n",
      "[31, 5400], time 2249.118804216385 loss: 1.031, acc: 63.44444444444445%\n",
      "[31, 5600], time 2250.703704595566 loss: 1.039, acc: 63.44419642857143%\n",
      "epoch 31 TRAIN error: 1.029825754653083, acc: 0.6344722222222222\n",
      "epoch 31 VALIDATION error: 0.9749615788459778, acc: 0.6524333333333333\n",
      "[32, 200], time 2283.4667127132416 loss: 1.009, acc: 64.3125%\n",
      "[32, 400], time 2285.0498225688934 loss: 1.028, acc: 63.98437500000001%\n",
      "[32, 600], time 2286.624420642853 loss: 1.006, acc: 64.44270833333333%\n",
      "[32, 800], time 2288.242036819458 loss: 1.019, acc: 64.20703125%\n",
      "[32, 1000], time 2289.8561573028564 loss: 1.037, acc: 63.9875%\n",
      "[32, 1200], time 2291.4592797756195 loss: 1.006, acc: 64.10677083333334%\n",
      "[32, 1400], time 2293.048407793045 loss: 1.033, acc: 63.93973214285714%\n",
      "[32, 1600], time 2294.648342847824 loss: 1.008, acc: 63.931640625%\n",
      "[32, 1800], time 2296.2569534778595 loss: 1.021, acc: 63.87847222222223%\n",
      "[32, 2000], time 2297.8745613098145 loss: 1.024, acc: 63.85937499999999%\n",
      "[32, 2200], time 2299.5136053562164 loss: 1.030, acc: 63.772727272727266%\n",
      "[32, 2400], time 2301.094211578369 loss: 1.025, acc: 63.755208333333336%\n",
      "[32, 2600], time 2302.719181537628 loss: 1.022, acc: 63.75120192307693%\n",
      "[32, 2800], time 2304.331789255142 loss: 1.021, acc: 63.74888392857143%\n",
      "[32, 3000], time 2305.938455104828 loss: 1.036, acc: 63.735416666666666%\n",
      "[32, 3200], time 2307.560038089752 loss: 1.054, acc: 63.6923828125%\n",
      "[32, 3400], time 2309.184150457382 loss: 1.032, acc: 63.64338235294118%\n",
      "[32, 3600], time 2310.805256843567 loss: 1.036, acc: 63.62152777777778%\n",
      "[32, 3800], time 2312.430876493454 loss: 1.046, acc: 63.57483552631579%\n",
      "[32, 4000], time 2314.041489124298 loss: 1.048, acc: 63.509375%\n",
      "[32, 4200], time 2315.6275992393494 loss: 1.040, acc: 63.50223214285714%\n",
      "[32, 4400], time 2317.2017171382904 loss: 1.056, acc: 63.48366477272728%\n",
      "[32, 4600], time 2318.8287692070007 loss: 1.018, acc: 63.466711956521735%\n",
      "[32, 4800], time 2320.4173843860626 loss: 1.059, acc: 63.43554687500001%\n",
      "[32, 5000], time 2322.0580010414124 loss: 1.051, acc: 63.436875%\n",
      "[32, 5200], time 2323.642109632492 loss: 1.020, acc: 63.43870192307692%\n",
      "[32, 5400], time 2325.274726629257 loss: 1.004, acc: 63.48495370370371%\n",
      "[32, 5600], time 2326.9063351154327 loss: 1.003, acc: 63.531808035714285%\n",
      "epoch 32 TRAIN error: 1.0282099054018656, acc: 0.6354611111111111\n",
      "epoch 32 VALIDATION error: 0.9965946078300476, acc: 0.6470555555555556\n",
      "[33, 200], time 2359.2324435710907 loss: 1.006, acc: 63.546875%\n",
      "[33, 400], time 2360.7239730358124 loss: 1.018, acc: 63.5703125%\n",
      "[33, 600], time 2362.2415766716003 loss: 1.003, acc: 63.83854166666667%\n",
      "[33, 800], time 2363.7696857452393 loss: 1.009, acc: 63.80859375%\n",
      "[33, 1000], time 2365.2912878990173 loss: 1.021, acc: 63.9%\n",
      "[33, 1200], time 2366.7928943634033 loss: 1.021, acc: 63.848958333333336%\n",
      "[33, 1400], time 2368.324498653412 loss: 1.027, acc: 63.810267857142854%\n",
      "[33, 1600], time 2369.846599817276 loss: 1.002, acc: 63.916015625%\n",
      "[33, 1800], time 2371.3918087482452 loss: 0.997, acc: 63.97222222222222%\n",
      "[33, 2000], time 2372.8809099197388 loss: 1.063, acc: 63.86875%\n",
      "[33, 2200], time 2374.40300822258 loss: 1.018, acc: 63.85227272727273%\n",
      "[33, 2400], time 2375.9141144752502 loss: 1.011, acc: 63.81640625%\n",
      "[33, 2600], time 2377.440217733383 loss: 1.038, acc: 63.734375%\n",
      "[33, 2800], time 2378.9698293209076 loss: 1.031, acc: 63.737723214285715%\n",
      "[33, 3000], time 2380.5084319114685 loss: 1.029, acc: 63.688541666666666%\n",
      "[33, 3200], time 2382.0315396785736 loss: 1.056, acc: 63.587890625%\n",
      "[33, 3400], time 2383.571647644043 loss: 1.028, acc: 63.599264705882355%\n",
      "[33, 3600], time 2385.1071469783783 loss: 1.037, acc: 63.55208333333333%\n",
      "[33, 3800], time 2386.6287615299225 loss: 1.029, acc: 63.575657894736835%\n",
      "[33, 4000], time 2388.163874387741 loss: 1.029, acc: 63.57890625%\n",
      "[33, 4200], time 2389.693982362747 loss: 1.029, acc: 63.593005952380956%\n",
      "[33, 4400], time 2391.224511861801 loss: 1.030, acc: 63.56534090909091%\n",
      "[33, 4600], time 2392.745176553726 loss: 1.058, acc: 63.491168478260875%\n",
      "[33, 4800], time 2394.274781703949 loss: 1.020, acc: 63.532552083333336%\n",
      "[33, 5000], time 2395.8048799037933 loss: 1.048, acc: 63.485%\n",
      "[33, 5200], time 2397.347877264023 loss: 1.045, acc: 63.44350961538462%\n",
      "[33, 5400], time 2398.8966658115387 loss: 1.022, acc: 63.442129629629626%\n",
      "[33, 5600], time 2400.4108476638794 loss: 1.023, acc: 63.44363839285714%\n",
      "epoch 33 TRAIN error: 1.0269070994906955, acc: 0.6343333333333333\n",
      "epoch 33 VALIDATION error: 0.9848552942276001, acc: 0.6490111111111111\n",
      "[34, 200], time 2431.7353138923645 loss: 1.007, acc: 63.87500000000001%\n",
      "[34, 400], time 2433.25141453743 loss: 1.012, acc: 63.9921875%\n",
      "[34, 600], time 2434.791749238968 loss: 1.025, acc: 63.869791666666664%\n",
      "[34, 800], time 2436.29784989357 loss: 1.045, acc: 63.72656249999999%\n",
      "[34, 1000], time 2437.818447828293 loss: 1.000, acc: 63.775000000000006%\n",
      "[34, 1200], time 2439.310165166855 loss: 0.992, acc: 63.9375%\n",
      "[34, 1400], time 2440.846710920334 loss: 1.045, acc: 63.78125%\n",
      "[34, 1600], time 2442.368814468384 loss: 1.026, acc: 63.73828124999999%\n",
      "[34, 1800], time 2443.9094133377075 loss: 1.021, acc: 63.74652777777777%\n",
      "[34, 2000], time 2445.413016796112 loss: 1.012, acc: 63.721875%\n",
      "[34, 2200], time 2446.9296164512634 loss: 1.017, acc: 63.68607954545455%\n",
      "[34, 2400], time 2448.4517233371735 loss: 1.024, acc: 63.70312500000001%\n",
      "[34, 2600], time 2449.9653310775757 loss: 1.046, acc: 63.628605769230774%\n",
      "[34, 2800], time 2451.466438770294 loss: 1.029, acc: 63.57142857142857%\n",
      "[34, 3000], time 2452.995037317276 loss: 1.014, acc: 63.610416666666666%\n",
      "[34, 3200], time 2454.531138896942 loss: 1.025, acc: 63.63964843749999%\n",
      "[34, 3400], time 2456.12703704834 loss: 1.029, acc: 63.63694852941176%\n",
      "[34, 3600], time 2457.8461780548096 loss: 1.017, acc: 63.63454861111111%\n",
      "[34, 3800], time 2459.5573103427887 loss: 1.027, acc: 63.61513157894737%\n",
      "[34, 4000], time 2461.127936601639 loss: 1.017, acc: 63.646875%\n",
      "[34, 4200], time 2462.719600915909 loss: 1.011, acc: 63.66220238095238%\n",
      "[34, 4400], time 2464.27184343338 loss: 1.018, acc: 63.66193181818181%\n",
      "[34, 4600], time 2465.7904419898987 loss: 1.050, acc: 63.62364130434782%\n",
      "[34, 4800], time 2467.32465839386 loss: 1.031, acc: 63.638671875%\n",
      "[34, 5000], time 2468.8642859458923 loss: 1.044, acc: 63.616249999999994%\n",
      "[34, 5200], time 2470.399883747101 loss: 1.014, acc: 63.630408653846146%\n",
      "[34, 5400], time 2471.9219813346863 loss: 1.027, acc: 63.61863425925925%\n",
      "[34, 5600], time 2473.4490778446198 loss: 1.037, acc: 63.59709821428572%\n",
      "epoch 34 TRAIN error: 1.0234805615001255, acc: 0.6360333333333333\n",
      "epoch 34 VALIDATION error: 0.9836317896842957, acc: 0.6495555555555556\n",
      "[35, 200], time 2504.5570414066315 loss: 1.005, acc: 64.015625%\n",
      "[35, 400], time 2506.094249725342 loss: 1.006, acc: 63.734375%\n",
      "[35, 600], time 2507.603492498398 loss: 1.015, acc: 63.890625%\n",
      "[35, 800], time 2509.1335928440094 loss: 1.007, acc: 63.87500000000001%\n",
      "[35, 1000], time 2510.66286444664 loss: 0.997, acc: 64.1%\n",
      "[35, 1200], time 2512.1726331710815 loss: 0.992, acc: 64.18229166666667%\n",
      "[35, 1400], time 2513.710005760193 loss: 1.024, acc: 64.10491071428571%\n",
      "[35, 1600], time 2515.2528166770935 loss: 1.029, acc: 63.98828125%\n",
      "[35, 1800], time 2516.7769362926483 loss: 1.028, acc: 63.953125%\n",
      "[35, 2000], time 2518.306036233902 loss: 1.054, acc: 63.8046875%\n",
      "[35, 2200], time 2519.8446412086487 loss: 1.022, acc: 63.80113636363637%\n",
      "[35, 2400], time 2521.378752231598 loss: 1.024, acc: 63.782552083333336%\n",
      "[35, 2600], time 2522.884664773941 loss: 1.062, acc: 63.668269230769226%\n",
      "[35, 2800], time 2524.3904945850372 loss: 1.019, acc: 63.660714285714285%\n",
      "[35, 3000], time 2525.877200603485 loss: 1.030, acc: 63.66354166666667%\n",
      "[35, 3200], time 2527.4137964248657 loss: 1.021, acc: 63.6689453125%\n",
      "[35, 3400], time 2528.9355051517487 loss: 1.003, acc: 63.732536764705884%\n",
      "[35, 3600], time 2530.47110915184 loss: 1.031, acc: 63.697916666666664%\n",
      "[35, 3800], time 2531.996206521988 loss: 1.022, acc: 63.68009868421053%\n",
      "[35, 4000], time 2533.524304628372 loss: 1.012, acc: 63.675000000000004%\n",
      "[35, 4200], time 2535.0095431804657 loss: 1.019, acc: 63.654761904761905%\n",
      "[35, 4400], time 2536.5282378196716 loss: 1.017, acc: 63.66832386363637%\n",
      "[35, 4600], time 2538.0487201213837 loss: 1.030, acc: 63.67663043478261%\n",
      "[35, 4800], time 2539.5788235664368 loss: 1.048, acc: 63.62890624999999%\n",
      "[35, 5000], time 2541.1064240932465 loss: 1.040, acc: 63.63999999999999%\n",
      "[35, 5200], time 2542.6190292835236 loss: 1.027, acc: 63.65865384615385%\n",
      "[35, 5400], time 2544.132131099701 loss: 1.040, acc: 63.65046296296296%\n",
      "[35, 5600], time 2545.670173883438 loss: 1.037, acc: 63.64341517857143%\n",
      "epoch 35 TRAIN error: 1.0240379447195265, acc: 0.6362777777777778\n",
      "epoch 35 VALIDATION error: 0.9706594944000244, acc: 0.6531777777777777\n",
      "[36, 200], time 2576.6715059280396 loss: 0.987, acc: 65.140625%\n",
      "[36, 400], time 2578.196615934372 loss: 1.011, acc: 64.59375%\n",
      "[36, 600], time 2579.719216108322 loss: 1.021, acc: 64.4375%\n",
      "[36, 800], time 2581.2388207912445 loss: 1.009, acc: 64.4296875%\n",
      "[36, 1000], time 2582.765434026718 loss: 1.034, acc: 64.15625%\n",
      "[36, 1200], time 2584.278046607971 loss: 1.023, acc: 64.1015625%\n",
      "[36, 1400], time 2585.809611558914 loss: 1.005, acc: 64.22098214285714%\n",
      "[36, 1600], time 2587.313723564148 loss: 1.003, acc: 64.2109375%\n",
      "[36, 1800], time 2588.8388562202454 loss: 1.007, acc: 64.20833333333333%\n",
      "[36, 2000], time 2590.351461648941 loss: 1.023, acc: 64.14687500000001%\n",
      "[36, 2200], time 2591.8680713176727 loss: 1.016, acc: 64.04829545454545%\n",
      "[36, 2400], time 2593.394673347473 loss: 1.018, acc: 64.04166666666666%\n",
      "[36, 2600], time 2594.9202737808228 loss: 1.050, acc: 63.97115384615385%\n",
      "[36, 2800], time 2596.441874265671 loss: 1.042, acc: 63.89843750000001%\n",
      "[36, 3000], time 2597.970984697342 loss: 1.011, acc: 63.9%\n",
      "[36, 3200], time 2599.462672472 loss: 1.019, acc: 63.8564453125%\n",
      "[36, 3400], time 2600.984781265259 loss: 1.039, acc: 63.80422794117647%\n",
      "[36, 3600], time 2602.505576133728 loss: 1.016, acc: 63.802951388888886%\n",
      "[36, 3800], time 2604.0461823940277 loss: 1.023, acc: 63.83470394736842%\n",
      "[36, 4000], time 2605.5893001556396 loss: 1.032, acc: 63.7953125%\n",
      "[36, 4200], time 2607.1174037456512 loss: 1.024, acc: 63.78125%\n",
      "[36, 4400], time 2608.6490070819855 loss: 1.021, acc: 63.75639204545455%\n",
      "[36, 4600], time 2610.1566162109375 loss: 1.002, acc: 63.74388586956522%\n",
      "[36, 4800], time 2611.6750228405 loss: 1.038, acc: 63.72395833333333%\n",
      "[36, 5000], time 2613.2046349048615 loss: 1.052, acc: 63.695%\n",
      "[36, 5200], time 2614.7476732730865 loss: 1.021, acc: 63.70072115384615%\n",
      "[36, 5400], time 2616.272275686264 loss: 1.019, acc: 63.74537037037037%\n",
      "[36, 5600], time 2617.8143649101257 loss: 1.056, acc: 63.71484375000001%\n",
      "epoch 36 TRAIN error: 1.0224102493921916, acc: 0.6370555555555556\n",
      "epoch 36 VALIDATION error: 0.9797599911689758, acc: 0.6510666666666667\n",
      "[37, 200], time 2649.6209387779236 loss: 1.014, acc: 64.359375%\n",
      "[37, 400], time 2651.1385452747345 loss: 0.988, acc: 64.5546875%\n",
      "[37, 600], time 2652.6676535606384 loss: 1.006, acc: 64.296875%\n",
      "[37, 800], time 2654.260278224945 loss: 1.024, acc: 64.0234375%\n",
      "[37, 1000], time 2655.8563978672028 loss: 0.993, acc: 64.19375000000001%\n",
      "[37, 1200], time 2657.45951795578 loss: 0.970, acc: 64.3984375%\n",
      "[37, 1400], time 2659.014132499695 loss: 1.005, acc: 64.43080357142857%\n",
      "[37, 1600], time 2660.528228521347 loss: 1.021, acc: 64.322265625%\n",
      "[37, 1800], time 2662.0763297080994 loss: 1.018, acc: 64.22395833333333%\n",
      "[37, 2000], time 2663.6129302978516 loss: 1.007, acc: 64.265625%\n",
      "[37, 2200], time 2665.1745319366455 loss: 1.027, acc: 64.203125%\n",
      "[37, 2400], time 2666.708637714386 loss: 1.034, acc: 64.12109375%\n",
      "[37, 2600], time 2668.300246477127 loss: 1.012, acc: 64.11778846153847%\n",
      "[37, 2800], time 2669.8803510665894 loss: 1.023, acc: 64.109375%\n",
      "[37, 3000], time 2671.4654564857483 loss: 1.019, acc: 64.08749999999999%\n",
      "[37, 3200], time 2673.05206656456 loss: 1.033, acc: 64.021484375%\n",
      "[37, 3400], time 2674.6226785182953 loss: 1.038, acc: 64.01930147058823%\n",
      "[37, 3600], time 2676.143785715103 loss: 1.050, acc: 63.98697916666667%\n",
      "[37, 3800], time 2677.640387535095 loss: 1.018, acc: 64.00904605263158%\n",
      "[37, 4000], time 2679.125988483429 loss: 1.034, acc: 63.9875%\n",
      "[37, 4200], time 2680.6145901679993 loss: 1.042, acc: 63.92782738095239%\n",
      "[37, 4400], time 2682.098691701889 loss: 0.997, acc: 63.97443181818182%\n",
      "[37, 4600], time 2683.585790157318 loss: 1.044, acc: 63.9273097826087%\n",
      "[37, 4800], time 2685.075891017914 loss: 1.015, acc: 63.939453125%\n",
      "[37, 5000], time 2686.565989971161 loss: 1.027, acc: 63.938125%\n",
      "[37, 5200], time 2688.0610988140106 loss: 1.038, acc: 63.90024038461538%\n",
      "[37, 5400], time 2689.5526995658875 loss: 1.053, acc: 63.86516203703704%\n",
      "[37, 5600], time 2691.038803577423 loss: 1.012, acc: 63.88839285714286%\n",
      "epoch 37 TRAIN error: 1.0202054678705004, acc: 0.6388111111111111\n",
      "epoch 37 VALIDATION error: 0.9849787354469299, acc: 0.6501333333333333\n",
      "[38, 200], time 2722.2525725364685 loss: 1.014, acc: 64.3125%\n",
      "[38, 400], time 2723.8406817913055 loss: 1.025, acc: 63.85937499999999%\n",
      "[38, 600], time 2725.414788246155 loss: 1.016, acc: 63.734375%\n",
      "[38, 800], time 2726.9843888282776 loss: 1.007, acc: 63.78125%\n",
      "[38, 1000], time 2728.5649933815002 loss: 1.003, acc: 63.86875%\n",
      "[38, 1200], time 2730.124604701996 loss: 0.993, acc: 64.09375%\n",
      "[38, 1400], time 2731.683708667755 loss: 1.014, acc: 64.08258928571429%\n",
      "[38, 1600], time 2733.2313129901886 loss: 1.006, acc: 64.041015625%\n",
      "[38, 1800], time 2734.759414434433 loss: 1.018, acc: 63.99652777777778%\n",
      "[38, 2000], time 2736.29052066803 loss: 1.015, acc: 63.9546875%\n",
      "[38, 2200], time 2737.826131105423 loss: 1.020, acc: 63.90482954545455%\n",
      "[38, 2400], time 2739.3647360801697 loss: 1.031, acc: 63.86197916666667%\n",
      "[38, 2600], time 2740.9118478298187 loss: 1.054, acc: 63.777644230769226%\n",
      "[38, 2800], time 2742.4464559555054 loss: 1.018, acc: 63.76674107142857%\n",
      "[38, 3000], time 2743.977066755295 loss: 1.041, acc: 63.755208333333336%\n",
      "[38, 3200], time 2745.5151822566986 loss: 1.015, acc: 63.7900390625%\n",
      "[38, 3400], time 2747.0687832832336 loss: 1.036, acc: 63.77022058823529%\n",
      "[38, 3600], time 2748.5998935699463 loss: 1.033, acc: 63.72916666666667%\n",
      "[38, 3800], time 2750.1439933776855 loss: 1.021, acc: 63.749177631578945%\n",
      "[38, 4000], time 2751.6971020698547 loss: 1.011, acc: 63.7765625%\n",
      "[38, 4200], time 2753.2412025928497 loss: 1.011, acc: 63.79315476190476%\n",
      "[38, 4400], time 2754.7823045253754 loss: 1.032, acc: 63.796875%\n",
      "[38, 4600], time 2756.322957277298 loss: 1.007, acc: 63.82065217391304%\n",
      "[38, 4800], time 2757.89155960083 loss: 1.041, acc: 63.80989583333333%\n",
      "[38, 5000], time 2759.4386699199677 loss: 1.036, acc: 63.783125%\n",
      "[38, 5200], time 2760.988771677017 loss: 1.022, acc: 63.8155048076923%\n",
      "[38, 5400], time 2762.5303807258606 loss: 1.021, acc: 63.81944444444444%\n",
      "[38, 5600], time 2764.0739982128143 loss: 1.047, acc: 63.789620535714285%\n",
      "epoch 38 TRAIN error: 1.0217388504346212, acc: 0.6379444444444444\n",
      "epoch 38 VALIDATION error: 0.9730169177055359, acc: 0.6524222222222222\n",
      "[39, 200], time 2795.8930773735046 loss: 1.005, acc: 64.484375%\n",
      "[39, 400], time 2797.4051814079285 loss: 0.986, acc: 65.1796875%\n",
      "[39, 600], time 2798.952778816223 loss: 0.998, acc: 65.05208333333333%\n",
      "[39, 800], time 2800.5068905353546 loss: 0.982, acc: 65.078125%\n",
      "[39, 1000], time 2802.0519890785217 loss: 1.009, acc: 64.86562500000001%\n",
      "[39, 1200], time 2803.602092027664 loss: 1.018, acc: 64.74739583333333%\n",
      "[39, 1400], time 2805.150188446045 loss: 0.997, acc: 64.71875%\n",
      "[39, 1600], time 2806.6547853946686 loss: 0.996, acc: 64.740234375%\n",
      "[39, 1800], time 2808.171937942505 loss: 1.027, acc: 64.66493055555556%\n",
      "[39, 2000], time 2809.7020564079285 loss: 1.004, acc: 64.6953125%\n",
      "[39, 2200], time 2811.2501628398895 loss: 1.037, acc: 64.50852272727273%\n",
      "[39, 2400], time 2812.80326461792 loss: 1.004, acc: 64.52213541666667%\n",
      "[39, 2600], time 2814.348381757736 loss: 1.028, acc: 64.48677884615385%\n",
      "[39, 2800], time 2815.878988265991 loss: 1.024, acc: 64.42633928571428%\n",
      "[39, 3000], time 2817.3886003494263 loss: 1.013, acc: 64.42812500000001%\n",
      "[39, 3200], time 2818.8872179985046 loss: 1.028, acc: 64.3349609375%\n",
      "[39, 3400], time 2820.4063532352448 loss: 1.016, acc: 64.35569852941177%\n",
      "[39, 3600], time 2821.961479663849 loss: 1.017, acc: 64.33767361111111%\n",
      "[39, 3800], time 2823.5190992355347 loss: 1.023, acc: 64.31003289473685%\n",
      "[39, 4000], time 2825.085204601288 loss: 1.036, acc: 64.2890625%\n",
      "[39, 4200], time 2826.637809753418 loss: 1.031, acc: 64.25520833333334%\n",
      "[39, 4400], time 2828.144912958145 loss: 1.003, acc: 64.2627840909091%\n",
      "[39, 4600], time 2829.662519454956 loss: 1.007, acc: 64.265625%\n",
      "[39, 4800], time 2831.172121524811 loss: 1.022, acc: 64.23567708333333%\n",
      "[39, 5000], time 2832.6917378902435 loss: 1.040, acc: 64.18625%\n",
      "[39, 5200], time 2834.2458424568176 loss: 1.018, acc: 64.1905048076923%\n",
      "[39, 5400], time 2835.8184583187103 loss: 1.012, acc: 64.16608796296296%\n",
      "[39, 5600], time 2837.3800587654114 loss: 1.054, acc: 64.10602678571429%\n",
      "epoch 39 TRAIN error: 1.015769368569056, acc: 0.6409222222222222\n",
      "epoch 39 VALIDATION error: 0.9792535901069641, acc: 0.6523333333333333\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "train_errors, val_errors = train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b12cf6a-4b15-47a3-bc36-ad7f1d93eeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m val_errors \u001b[38;5;241m=\u001b[39m [error\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m val_errors]\n\u001b[1;32m----> 2\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_errors\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m val_errors \u001b[38;5;241m=\u001b[39m [error\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m val_errors]\n\u001b[1;32m----> 2\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m [\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m train_errors]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "val_errors = [error.item() for error in val_errors]\n",
    "train_errors = [error.item() for error in train_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1603718c-57d0-496b-b31e-6e4e42b8c760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3372588157653809,\n",
       " 1.2188230752944946,\n",
       " 1.1649056673049927,\n",
       " 1.1328580379486084,\n",
       " 1.10721755027771,\n",
       " 1.0929135084152222,\n",
       " 1.0857011079788208,\n",
       " 1.0723203420639038,\n",
       " 1.0667293071746826,\n",
       " 1.0526394844055176,\n",
       " 1.039095163345337,\n",
       " 1.0307964086532593,\n",
       " 1.0336157083511353,\n",
       " 1.0272157192230225,\n",
       " 1.0201568603515625,\n",
       " 1.0201719999313354,\n",
       " 1.0115723609924316,\n",
       " 1.0105035305023193,\n",
       " 1.0077552795410156,\n",
       " 1.0063811540603638,\n",
       " 1.0138052701950073,\n",
       " 1.0019172430038452,\n",
       " 1.0003180503845215,\n",
       " 1.0071711540222168,\n",
       " 1.0108094215393066,\n",
       " 1.0055571794509888,\n",
       " 0.9953855872154236,\n",
       " 0.9930347204208374,\n",
       " 1.0016943216323853,\n",
       " 0.9979351758956909,\n",
       " 1.0014150142669678,\n",
       " 0.9990295171737671]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c754d8b-1b0d-4107-ae2c-3ec3903d595c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6338206401718987,\n",
       " 1.415222422027588,\n",
       " 1.335942827712165,\n",
       " 1.283900398657057,\n",
       " 1.2487130006684197,\n",
       " 1.2187324887805515,\n",
       " 1.1958250019921197,\n",
       " 1.1740234076393976,\n",
       " 1.155056654294332,\n",
       " 1.1395169295840792,\n",
       " 1.1264583301756117,\n",
       " 1.1116882113456725,\n",
       " 1.104477496094174,\n",
       " 1.0917700447718302,\n",
       " 1.084641329966651,\n",
       " 1.0777994940121969,\n",
       " 1.066557710366779,\n",
       " 1.059576832962036,\n",
       " 1.0532014403661092,\n",
       " 1.047639622285631,\n",
       " 1.0390196873876782,\n",
       " 1.0360867286576165,\n",
       " 1.027599328162935,\n",
       " 1.0248288424597847,\n",
       " 1.0199344754748874,\n",
       " 1.014524180730184,\n",
       " 1.0121251190927294,\n",
       " 1.0085033401330312,\n",
       " 1.005666101635827,\n",
       " 0.9994403559896681,\n",
       " 0.9960463191244338,\n",
       " 0.9927381872442034]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "932281c6-3fba-40f2-8f90-2f0b86d4508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        # BN-ReLU-Conv(1x1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "        # BN-ReLU-Conv(3x3)\n",
    "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        new_features = self.conv1(self.relu1(self.bn1(x)))\n",
    "        new_features = self.conv2(self.relu2(self.bn2(new_features)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.add_module('denselayer%d' % (i + 1),\n",
    "                                  DenseLayer(in_channels + i * growth_rate, growth_rate, bn_size, drop_rate))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = x\n",
    "        for layer in self.layers:\n",
    "            features = layer(features)\n",
    "        return features\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, block_config=(6, 12, 8), \n",
    "                 num_init_features=32, bn_size=4, drop_rate=0.2, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            # Add a transition layer between dense blocks (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(in_channels=num_features, out_channels=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.features.add_module('relu5', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# Create an instance of the model\n",
    "model = DenseNet(growth_rate=12, block_config=(6, 12, 8), num_classes=10)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.95, weight_decay=0.0005)\n",
    "# Alternative: optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# Load and preprocess data (using your existing setup)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder('../../data/processed/train_augmented_rotated/', transform=transform)\n",
    "valset = torchvision.datasets.ImageFolder('../../data/raw/valid/', transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "ES = EarlyStopper(patience=4, min_delta=0)\n",
    "\n",
    "def train(epochs=5):\n",
    "    time_started = time.time()\n",
    "    _train_errors = []\n",
    "    _val_errors = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct_train = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        total_batches = 0\n",
    "        \n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct_train += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            total += len(outputs)\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            \n",
    "            if i % 200 == 199:    # Print every 200 mini-batches\n",
    "                acc = correct_train / total\n",
    "                print(f'[{epoch + 1}, {i + 1}], time {time.time() - time_started:.2f}s loss: {running_loss / 200:.3f}, acc: {acc * 100:.2f}%')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        _train_errors.append(epoch_loss / total_batches)\n",
    "        val_error = 0\n",
    "        correct_val = 0\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "                correct_val += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            val_error = val_error / len(valloader.dataset)\n",
    "            _val_errors.append(val_error)\n",
    "            print(f'epoch {epoch + 1} TRAIN error: {epoch_loss / total_batches:.4f}, acc: {correct_train/total:.4f}')\n",
    "            print(f'epoch {epoch + 1} VALIDATION error: {val_error:.4f}, acc: {correct_val/len(valloader.dataset):.4f}')\n",
    "\n",
    "        if ES.check(val_error, model):\n",
    "            print('Early stopping triggered!')\n",
    "            return _train_errors, _val_errors\n",
    "    \n",
    "    return _train_errors, _val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b42916-9825-4cc5-b2e9-3c3aa0c614ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200], time 10.94s loss: 2.185, acc: 19.14%\n",
      "[1, 400], time 17.63s loss: 2.017, acc: 22.30%\n",
      "[1, 600], time 24.40s loss: 1.924, acc: 24.10%\n",
      "[1, 800], time 31.57s loss: 1.854, acc: 25.42%\n",
      "[1, 1000], time 38.44s loss: 1.810, acc: 26.56%\n",
      "[1, 1200], time 45.36s loss: 1.799, acc: 27.48%\n",
      "[1, 1400], time 52.29s loss: 1.749, acc: 28.39%\n",
      "[1, 1600], time 59.04s loss: 1.724, acc: 29.17%\n",
      "[1, 1800], time 66.07s loss: 1.692, acc: 29.89%\n",
      "[1, 2000], time 72.99s loss: 1.655, acc: 30.66%\n",
      "[1, 2200], time 79.88s loss: 1.644, acc: 31.41%\n",
      "[1, 2400], time 86.66s loss: 1.629, acc: 32.07%\n",
      "[1, 2600], time 93.54s loss: 1.613, acc: 32.66%\n",
      "[1, 2800], time 100.52s loss: 1.587, acc: 33.21%\n",
      "[1, 3000], time 107.11s loss: 1.597, acc: 33.66%\n",
      "[1, 3200], time 113.57s loss: 1.559, acc: 34.22%\n",
      "[1, 3400], time 120.33s loss: 1.538, acc: 34.72%\n",
      "[1, 3600], time 126.91s loss: 1.532, acc: 35.18%\n",
      "[1, 3800], time 133.47s loss: 1.534, acc: 35.61%\n",
      "[1, 4000], time 140.11s loss: 1.484, acc: 36.04%\n",
      "[1, 4200], time 146.75s loss: 1.533, acc: 36.39%\n",
      "[1, 4400], time 153.57s loss: 1.514, acc: 36.76%\n",
      "[1, 4600], time 160.06s loss: 1.483, acc: 37.11%\n",
      "[1, 4800], time 166.67s loss: 1.469, acc: 37.47%\n",
      "[1, 5000], time 173.17s loss: 1.472, acc: 37.78%\n",
      "[1, 5200], time 179.96s loss: 1.447, acc: 38.13%\n",
      "[1, 5400], time 186.42s loss: 1.458, acc: 38.44%\n",
      "[1, 5600], time 193.20s loss: 1.449, acc: 38.74%\n",
      "epoch 1 TRAIN error: 1.6402, acc: 0.3879\n",
      "epoch 1 VALIDATION error: 1.3990, acc: 0.4871\n",
      "[2, 200], time 242.02s loss: 1.427, acc: 47.58%\n",
      "[2, 400], time 248.68s loss: 1.432, acc: 47.55%\n",
      "[2, 600], time 255.40s loss: 1.396, acc: 47.89%\n",
      "[2, 800], time 262.08s loss: 1.423, acc: 47.80%\n",
      "[2, 1000], time 268.76s loss: 1.400, acc: 47.80%\n",
      "[2, 1200], time 275.34s loss: 1.412, acc: 47.83%\n",
      "[2, 1400], time 281.91s loss: 1.370, acc: 47.99%\n",
      "[2, 1600], time 288.48s loss: 1.380, acc: 48.16%\n",
      "[2, 1800], time 295.02s loss: 1.357, acc: 48.32%\n",
      "[2, 2000], time 301.59s loss: 1.367, acc: 48.44%\n",
      "[2, 2200], time 308.15s loss: 1.368, acc: 48.52%\n",
      "[2, 2400], time 314.71s loss: 1.336, acc: 48.81%\n",
      "[2, 2600], time 321.24s loss: 1.353, acc: 48.91%\n",
      "[2, 2800], time 327.78s loss: 1.365, acc: 49.01%\n",
      "[2, 3000], time 334.18s loss: 1.374, acc: 48.99%\n",
      "[2, 3200], time 340.69s loss: 1.334, acc: 49.08%\n",
      "[2, 3400], time 347.36s loss: 1.340, acc: 49.21%\n",
      "[2, 3600], time 353.89s loss: 1.312, acc: 49.34%\n",
      "[2, 3800], time 360.55s loss: 1.307, acc: 49.48%\n",
      "[2, 4000], time 366.73s loss: 1.323, acc: 49.58%\n",
      "[2, 4200], time 373.44s loss: 1.329, acc: 49.66%\n",
      "[2, 4400], time 379.87s loss: 1.313, acc: 49.74%\n",
      "[2, 4600], time 386.39s loss: 1.322, acc: 49.83%\n",
      "[2, 4800], time 393.05s loss: 1.301, acc: 49.93%\n",
      "[2, 5000], time 399.67s loss: 1.302, acc: 50.03%\n",
      "[2, 5200], time 406.23s loss: 1.318, acc: 50.10%\n",
      "[2, 5400], time 412.57s loss: 1.302, acc: 50.21%\n",
      "[2, 5600], time 419.24s loss: 1.295, acc: 50.30%\n",
      "epoch 2 TRAIN error: 1.3515, acc: 0.5032\n",
      "epoch 2 VALIDATION error: 1.2330, acc: 0.5502\n",
      "[3, 200], time 467.15s loss: 1.257, acc: 53.39%\n",
      "[3, 400], time 473.79s loss: 1.258, acc: 53.64%\n",
      "[3, 600], time 480.33s loss: 1.272, acc: 53.49%\n",
      "[3, 800], time 486.83s loss: 1.265, acc: 53.63%\n",
      "[3, 1000], time 493.41s loss: 1.287, acc: 53.62%\n",
      "[3, 1200], time 500.00s loss: 1.286, acc: 53.52%\n",
      "[3, 1400], time 506.70s loss: 1.268, acc: 53.65%\n",
      "[3, 1600], time 513.33s loss: 1.239, acc: 53.81%\n",
      "[3, 1800], time 520.02s loss: 1.262, acc: 53.82%\n",
      "[3, 2000], time 526.54s loss: 1.238, acc: 53.93%\n",
      "[3, 2200], time 533.04s loss: 1.246, acc: 54.02%\n",
      "[3, 2400], time 539.77s loss: 1.230, acc: 54.12%\n",
      "[3, 2600], time 546.35s loss: 1.226, acc: 54.27%\n",
      "[3, 2800], time 553.00s loss: 1.236, acc: 54.34%\n",
      "[3, 3000], time 559.72s loss: 1.210, acc: 54.43%\n",
      "[3, 3200], time 566.33s loss: 1.242, acc: 54.47%\n",
      "[3, 3400], time 573.33s loss: 1.220, acc: 54.56%\n",
      "[3, 3600], time 580.20s loss: 1.235, acc: 54.58%\n",
      "[3, 3800], time 587.11s loss: 1.230, acc: 54.63%\n",
      "[3, 4000], time 593.93s loss: 1.207, acc: 54.72%\n",
      "[3, 4200], time 600.72s loss: 1.233, acc: 54.71%\n",
      "[3, 4400], time 607.52s loss: 1.207, acc: 54.76%\n",
      "[3, 4600], time 614.50s loss: 1.204, acc: 54.84%\n",
      "[3, 4800], time 621.49s loss: 1.188, acc: 54.92%\n",
      "[3, 5000], time 628.29s loss: 1.215, acc: 54.93%\n",
      "[3, 5200], time 634.98s loss: 1.207, acc: 54.97%\n",
      "[3, 5400], time 641.65s loss: 1.204, acc: 55.03%\n",
      "[3, 5600], time 648.26s loss: 1.194, acc: 55.08%\n",
      "epoch 3 TRAIN error: 1.2344, acc: 0.5507\n",
      "epoch 3 VALIDATION error: 1.1466, acc: 0.5829\n",
      "[4, 200], time 696.59s loss: 1.173, acc: 57.28%\n",
      "[4, 400], time 703.28s loss: 1.174, acc: 57.36%\n",
      "[4, 600], time 709.96s loss: 1.145, acc: 57.71%\n",
      "[4, 800], time 716.56s loss: 1.160, acc: 57.94%\n",
      "[4, 1000], time 723.23s loss: 1.185, acc: 57.83%\n",
      "[4, 1200], time 729.92s loss: 1.169, acc: 57.78%\n",
      "[4, 1400], time 736.42s loss: 1.150, acc: 57.91%\n",
      "[4, 1600], time 743.12s loss: 1.160, acc: 57.89%\n",
      "[4, 1800], time 749.62s loss: 1.169, acc: 57.87%\n",
      "[4, 2000], time 756.22s loss: 1.172, acc: 57.80%\n",
      "[4, 2200], time 762.65s loss: 1.171, acc: 57.76%\n",
      "[4, 2400], time 769.14s loss: 1.173, acc: 57.73%\n",
      "[4, 2600], time 775.82s loss: 1.174, acc: 57.71%\n",
      "[4, 2800], time 782.54s loss: 1.156, acc: 57.78%\n",
      "[4, 3000], time 789.23s loss: 1.173, acc: 57.74%\n",
      "[4, 3200], time 795.96s loss: 1.146, acc: 57.81%\n",
      "[4, 3400], time 802.60s loss: 1.152, acc: 57.84%\n",
      "[4, 3600], time 809.34s loss: 1.133, acc: 57.88%\n",
      "[4, 3800], time 816.09s loss: 1.128, acc: 57.96%\n",
      "[4, 4000], time 822.86s loss: 1.136, acc: 58.03%\n",
      "[4, 4200], time 829.83s loss: 1.149, acc: 58.00%\n",
      "[4, 4400], time 836.66s loss: 1.148, acc: 57.99%\n",
      "[4, 4600], time 843.63s loss: 1.144, acc: 58.01%\n",
      "[4, 4800], time 850.55s loss: 1.143, acc: 58.02%\n",
      "[4, 5000], time 857.28s loss: 1.146, acc: 58.04%\n",
      "[4, 5200], time 863.88s loss: 1.102, acc: 58.16%\n",
      "[4, 5400], time 870.45s loss: 1.112, acc: 58.23%\n",
      "[4, 5600], time 877.18s loss: 1.150, acc: 58.24%\n",
      "epoch 4 TRAIN error: 1.1528, acc: 0.5826\n",
      "epoch 4 VALIDATION error: 1.0714, acc: 0.6144\n",
      "[5, 200], time 925.97s loss: 1.113, acc: 59.95%\n",
      "[5, 400], time 932.97s loss: 1.103, acc: 59.80%\n",
      "[5, 600], time 939.72s loss: 1.111, acc: 59.61%\n",
      "[5, 800], time 946.45s loss: 1.122, acc: 59.69%\n",
      "[5, 1000], time 953.17s loss: 1.091, acc: 59.87%\n",
      "[5, 1200], time 959.88s loss: 1.098, acc: 59.90%\n",
      "[5, 1400], time 966.40s loss: 1.105, acc: 59.94%\n",
      "[5, 1600], time 973.11s loss: 1.091, acc: 60.01%\n",
      "[5, 1800], time 979.58s loss: 1.091, acc: 60.10%\n",
      "[5, 2000], time 986.34s loss: 1.076, acc: 60.23%\n",
      "[5, 2200], time 993.37s loss: 1.092, acc: 60.21%\n",
      "[5, 2400], time 999.97s loss: 1.111, acc: 60.19%\n",
      "[5, 2600], time 1006.52s loss: 1.114, acc: 60.20%\n",
      "[5, 2800], time 1013.07s loss: 1.101, acc: 60.27%\n",
      "[5, 3000], time 1019.62s loss: 1.079, acc: 60.33%\n",
      "[5, 3200], time 1026.29s loss: 1.094, acc: 60.35%\n",
      "[5, 3400], time 1032.93s loss: 1.084, acc: 60.44%\n",
      "[5, 3600], time 1039.57s loss: 1.075, acc: 60.47%\n",
      "[5, 3800], time 1046.13s loss: 1.104, acc: 60.44%\n",
      "[5, 4000], time 1052.53s loss: 1.076, acc: 60.46%\n",
      "[5, 4200], time 1059.17s loss: 1.074, acc: 60.49%\n",
      "[5, 4400], time 1065.66s loss: 1.093, acc: 60.50%\n",
      "[5, 4600], time 1072.45s loss: 1.077, acc: 60.54%\n",
      "[5, 4800], time 1079.21s loss: 1.068, acc: 60.60%\n",
      "[5, 5000], time 1085.64s loss: 1.077, acc: 60.63%\n",
      "[5, 5200], time 1092.62s loss: 1.057, acc: 60.72%\n",
      "[5, 5400], time 1099.84s loss: 1.075, acc: 60.74%\n",
      "[5, 5600], time 1106.26s loss: 1.058, acc: 60.81%\n",
      "epoch 5 TRAIN error: 1.0896, acc: 0.6082\n",
      "epoch 5 VALIDATION error: 1.0172, acc: 0.6327\n",
      "[6, 200], time 1154.09s loss: 1.047, acc: 62.42%\n",
      "[6, 400], time 1160.63s loss: 1.051, acc: 62.41%\n",
      "[6, 600], time 1167.17s loss: 1.058, acc: 62.28%\n",
      "[6, 800], time 1173.48s loss: 1.027, acc: 62.47%\n",
      "[6, 1000], time 1179.68s loss: 1.057, acc: 62.38%\n",
      "[6, 1200], time 1186.21s loss: 1.046, acc: 62.55%\n",
      "[6, 1400], time 1192.74s loss: 1.021, acc: 62.68%\n",
      "[6, 1600], time 1199.39s loss: 1.036, acc: 62.79%\n",
      "[6, 1800], time 1205.98s loss: 1.046, acc: 62.73%\n",
      "[6, 2000], time 1212.75s loss: 1.054, acc: 62.67%\n",
      "[6, 2200], time 1219.50s loss: 1.043, acc: 62.64%\n",
      "[6, 2400], time 1226.20s loss: 1.017, acc: 62.77%\n",
      "[6, 2600], time 1232.83s loss: 1.048, acc: 62.71%\n",
      "[6, 2800], time 1239.52s loss: 1.025, acc: 62.73%\n",
      "[6, 3000], time 1246.18s loss: 1.017, acc: 62.80%\n",
      "[6, 3200], time 1252.88s loss: 1.027, acc: 62.85%\n",
      "[6, 3400], time 1259.25s loss: 1.062, acc: 62.78%\n",
      "[6, 3600], time 1265.88s loss: 1.025, acc: 62.80%\n",
      "[6, 3800], time 1272.45s loss: 1.010, acc: 62.85%\n",
      "[6, 4000], time 1279.17s loss: 1.043, acc: 62.83%\n",
      "[6, 4200], time 1285.68s loss: 1.052, acc: 62.83%\n",
      "[6, 4400], time 1292.21s loss: 1.031, acc: 62.83%\n",
      "[6, 4600], time 1298.89s loss: 1.032, acc: 62.86%\n",
      "[6, 4800], time 1305.62s loss: 1.012, acc: 62.89%\n",
      "[6, 5000], time 1312.34s loss: 0.994, acc: 62.96%\n",
      "[6, 5200], time 1319.04s loss: 1.036, acc: 62.96%\n",
      "[6, 5400], time 1325.67s loss: 1.031, acc: 62.97%\n",
      "[6, 5600], time 1332.30s loss: 1.036, acc: 62.96%\n",
      "epoch 6 TRAIN error: 1.0351, acc: 0.6296\n",
      "epoch 6 VALIDATION error: 0.9601, acc: 0.6576\n",
      "[7, 200], time 1380.40s loss: 1.001, acc: 63.75%\n",
      "[7, 400], time 1386.93s loss: 1.013, acc: 63.73%\n",
      "[7, 600], time 1393.65s loss: 1.020, acc: 63.58%\n",
      "[7, 800], time 1400.26s loss: 1.005, acc: 63.75%\n",
      "[7, 1000], time 1406.99s loss: 1.011, acc: 63.81%\n",
      "[7, 1200], time 1413.64s loss: 0.985, acc: 64.07%\n",
      "[7, 1400], time 1420.35s loss: 0.994, acc: 64.22%\n",
      "[7, 1600], time 1427.01s loss: 1.007, acc: 64.21%\n",
      "[7, 1800], time 1433.72s loss: 0.975, acc: 64.32%\n",
      "[7, 2000], time 1440.37s loss: 1.004, acc: 64.34%\n",
      "[7, 2200], time 1446.90s loss: 0.999, acc: 64.38%\n",
      "[7, 2400], time 1453.53s loss: 0.995, acc: 64.36%\n",
      "[7, 2600], time 1460.20s loss: 1.005, acc: 64.32%\n",
      "[7, 2800], time 1466.90s loss: 0.977, acc: 64.37%\n",
      "[7, 3000], time 1473.44s loss: 1.005, acc: 64.35%\n",
      "[7, 3200], time 1479.90s loss: 0.990, acc: 64.40%\n",
      "[7, 3400], time 1486.39s loss: 0.985, acc: 64.48%\n",
      "[7, 3600], time 1493.05s loss: 1.014, acc: 64.45%\n",
      "[7, 3800], time 1499.71s loss: 0.978, acc: 64.48%\n",
      "[7, 4000], time 1506.16s loss: 0.992, acc: 64.50%\n",
      "[7, 4200], time 1512.87s loss: 1.001, acc: 64.45%\n",
      "[7, 4400], time 1519.65s loss: 0.958, acc: 64.53%\n",
      "[7, 4600], time 1526.39s loss: 1.000, acc: 64.49%\n",
      "[7, 4800], time 1533.11s loss: 0.982, acc: 64.51%\n",
      "[7, 5000], time 1539.85s loss: 0.970, acc: 64.55%\n",
      "[7, 5200], time 1546.35s loss: 0.990, acc: 64.54%\n",
      "[7, 5400], time 1552.99s loss: 0.970, acc: 64.57%\n",
      "[7, 5600], time 1559.50s loss: 0.965, acc: 64.60%\n",
      "epoch 7 TRAIN error: 0.9925, acc: 0.6460\n",
      "epoch 7 VALIDATION error: 0.9596, acc: 0.6592\n",
      "[8, 200], time 1607.58s loss: 0.936, acc: 66.88%\n",
      "[8, 400], time 1614.26s loss: 0.982, acc: 65.46%\n",
      "[8, 600], time 1620.90s loss: 0.988, acc: 65.23%\n",
      "[8, 800], time 1627.58s loss: 0.966, acc: 65.22%\n",
      "[8, 1000], time 1634.27s loss: 0.954, acc: 65.35%\n",
      "[8, 1200], time 1640.95s loss: 0.961, acc: 65.45%\n",
      "[8, 1400], time 1647.69s loss: 0.945, acc: 65.69%\n",
      "[8, 1600], time 1654.37s loss: 0.958, acc: 65.75%\n",
      "[8, 1800], time 1661.06s loss: 0.955, acc: 65.76%\n",
      "[8, 2000], time 1667.71s loss: 0.944, acc: 65.81%\n",
      "[8, 2200], time 1674.40s loss: 0.947, acc: 65.84%\n",
      "[8, 2400], time 1681.14s loss: 0.979, acc: 65.76%\n",
      "[8, 2600], time 1687.81s loss: 0.952, acc: 65.82%\n",
      "[8, 2800], time 1694.70s loss: 0.956, acc: 65.79%\n",
      "[8, 3000], time 1701.42s loss: 0.949, acc: 65.81%\n",
      "[8, 3200], time 1708.10s loss: 0.969, acc: 65.71%\n",
      "[8, 3400], time 1714.60s loss: 0.975, acc: 65.68%\n",
      "[8, 3600], time 1721.32s loss: 0.977, acc: 65.65%\n",
      "[8, 3800], time 1727.95s loss: 0.948, acc: 65.69%\n",
      "[8, 4000], time 1734.56s loss: 0.946, acc: 65.71%\n",
      "[8, 4200], time 1741.17s loss: 0.930, acc: 65.78%\n",
      "[8, 4400], time 1747.68s loss: 0.948, acc: 65.79%\n",
      "[8, 4600], time 1754.36s loss: 0.951, acc: 65.80%\n",
      "[8, 4800], time 1760.94s loss: 0.954, acc: 65.81%\n",
      "[8, 5000], time 1767.60s loss: 0.944, acc: 65.84%\n",
      "[8, 5200], time 1774.16s loss: 0.963, acc: 65.83%\n",
      "[8, 5400], time 1780.73s loss: 0.927, acc: 65.89%\n",
      "[8, 5600], time 1787.34s loss: 0.954, acc: 65.89%\n",
      "epoch 8 TRAIN error: 0.9556, acc: 0.6589\n",
      "epoch 8 VALIDATION error: 0.9200, acc: 0.6739\n",
      "[9, 200], time 1835.40s loss: 0.904, acc: 67.77%\n",
      "[9, 400], time 1842.01s loss: 0.899, acc: 67.77%\n",
      "[9, 600], time 1848.60s loss: 0.953, acc: 67.04%\n",
      "[9, 800], time 1855.26s loss: 0.908, acc: 67.21%\n",
      "[9, 1000], time 1862.01s loss: 0.907, acc: 67.35%\n",
      "[9, 1200], time 1868.29s loss: 0.922, acc: 67.39%\n",
      "[9, 1400], time 1874.97s loss: 0.947, acc: 67.24%\n",
      "[9, 1600], time 1881.87s loss: 0.915, acc: 67.27%\n",
      "[9, 1800], time 1888.69s loss: 0.915, acc: 67.30%\n",
      "[9, 2000], time 1895.55s loss: 0.932, acc: 67.28%\n",
      "[9, 2200], time 1902.60s loss: 0.928, acc: 67.23%\n",
      "[9, 2400], time 1909.63s loss: 0.936, acc: 67.16%\n",
      "[9, 2600], time 1916.61s loss: 0.907, acc: 67.21%\n",
      "[9, 2800], time 1923.67s loss: 0.926, acc: 67.18%\n",
      "[9, 3000], time 1930.69s loss: 0.914, acc: 67.14%\n",
      "[9, 3200], time 1937.60s loss: 0.915, acc: 67.14%\n",
      "[9, 3400], time 1944.46s loss: 0.940, acc: 67.11%\n",
      "[9, 3600], time 1951.27s loss: 0.931, acc: 67.12%\n",
      "[9, 3800], time 1958.23s loss: 0.914, acc: 67.14%\n",
      "[9, 4000], time 1964.91s loss: 0.925, acc: 67.15%\n",
      "[9, 4200], time 1971.81s loss: 0.917, acc: 67.19%\n",
      "[9, 4400], time 1978.66s loss: 0.940, acc: 67.16%\n",
      "[9, 4600], time 1985.42s loss: 0.944, acc: 67.12%\n",
      "[9, 4800], time 1992.36s loss: 0.944, acc: 67.06%\n",
      "[9, 5000], time 1999.11s loss: 0.944, acc: 67.03%\n",
      "[9, 5200], time 2005.91s loss: 0.923, acc: 67.04%\n",
      "[9, 5400], time 2012.68s loss: 0.902, acc: 67.07%\n",
      "[9, 5600], time 2019.47s loss: 0.895, acc: 67.13%\n",
      "epoch 9 TRAIN error: 0.9232, acc: 0.6713\n",
      "epoch 9 VALIDATION error: 0.9028, acc: 0.6785\n",
      "[10, 200], time 2067.80s loss: 0.875, acc: 68.69%\n",
      "[10, 400], time 2074.47s loss: 0.878, acc: 68.84%\n",
      "[10, 600], time 2081.17s loss: 0.918, acc: 68.64%\n",
      "[10, 800], time 2087.87s loss: 0.908, acc: 68.47%\n",
      "[10, 1000], time 2094.55s loss: 0.906, acc: 68.27%\n",
      "[10, 1200], time 2101.17s loss: 0.911, acc: 68.19%\n",
      "[10, 1400], time 2107.88s loss: 0.894, acc: 68.20%\n",
      "[10, 1600], time 2114.47s loss: 0.869, acc: 68.31%\n",
      "[10, 1800], time 2121.19s loss: 0.884, acc: 68.32%\n",
      "[10, 2000], time 2127.91s loss: 0.904, acc: 68.29%\n",
      "[10, 2200], time 2134.54s loss: 0.896, acc: 68.26%\n",
      "[10, 2400], time 2141.18s loss: 0.882, acc: 68.29%\n",
      "[10, 2600], time 2147.85s loss: 0.893, acc: 68.29%\n",
      "[10, 2800], time 2154.47s loss: 0.928, acc: 68.08%\n",
      "[10, 3000], time 2161.09s loss: 0.890, acc: 68.11%\n",
      "[10, 3200], time 2167.75s loss: 0.887, acc: 68.14%\n",
      "[10, 3400], time 2174.41s loss: 0.905, acc: 68.11%\n",
      "[10, 3600], time 2181.11s loss: 0.919, acc: 68.05%\n",
      "[10, 3800], time 2187.78s loss: 0.902, acc: 68.04%\n",
      "[10, 4000], time 2194.52s loss: 0.907, acc: 68.02%\n",
      "[10, 4200], time 2201.22s loss: 0.891, acc: 68.04%\n",
      "[10, 4400], time 2207.88s loss: 0.870, acc: 68.09%\n",
      "[10, 4600], time 2214.40s loss: 0.906, acc: 68.09%\n",
      "[10, 4800], time 2220.85s loss: 0.895, acc: 68.09%\n",
      "[10, 5000], time 2227.54s loss: 0.889, acc: 68.11%\n",
      "[10, 5200], time 2234.09s loss: 0.906, acc: 68.09%\n",
      "[10, 5400], time 2240.79s loss: 0.888, acc: 68.11%\n",
      "[10, 5600], time 2247.48s loss: 0.896, acc: 68.12%\n",
      "epoch 10 TRAIN error: 0.8963, acc: 0.6813\n",
      "epoch 10 VALIDATION error: 0.9044, acc: 0.6794\n",
      "[11, 200], time 2295.93s loss: 0.850, acc: 69.55%\n",
      "[11, 400], time 2302.51s loss: 0.862, acc: 69.36%\n",
      "[11, 600], time 2309.19s loss: 0.878, acc: 68.92%\n",
      "[11, 800], time 2315.94s loss: 0.905, acc: 68.75%\n",
      "[11, 1000], time 2322.66s loss: 0.864, acc: 68.79%\n",
      "[11, 1200], time 2329.51s loss: 0.852, acc: 68.88%\n",
      "[11, 1400], time 2336.50s loss: 0.882, acc: 68.86%\n",
      "[11, 1600], time 2343.39s loss: 0.869, acc: 68.94%\n",
      "[11, 1800], time 2350.29s loss: 0.876, acc: 68.89%\n",
      "[11, 2000], time 2357.19s loss: 0.882, acc: 68.87%\n",
      "[11, 2200], time 2364.18s loss: 0.872, acc: 68.87%\n",
      "[11, 2400], time 2370.98s loss: 0.862, acc: 68.90%\n",
      "[11, 2600], time 2377.78s loss: 0.873, acc: 68.90%\n",
      "[11, 2800], time 2384.66s loss: 0.871, acc: 68.91%\n",
      "[11, 3000], time 2391.53s loss: 0.863, acc: 68.92%\n",
      "[11, 3200], time 2398.46s loss: 0.865, acc: 68.91%\n",
      "[11, 3400], time 2405.45s loss: 0.873, acc: 68.89%\n",
      "[11, 3600], time 2412.41s loss: 0.867, acc: 68.91%\n",
      "[11, 3800], time 2419.26s loss: 0.860, acc: 68.94%\n",
      "[11, 4000], time 2425.97s loss: 0.882, acc: 68.95%\n",
      "[11, 4200], time 2432.75s loss: 0.883, acc: 68.95%\n",
      "[11, 4400], time 2439.56s loss: 0.878, acc: 68.92%\n",
      "[11, 4600], time 2446.29s loss: 0.886, acc: 68.89%\n",
      "[11, 4800], time 2452.90s loss: 0.850, acc: 68.93%\n",
      "[11, 5000], time 2459.58s loss: 0.883, acc: 68.90%\n",
      "[11, 5200], time 2466.33s loss: 0.871, acc: 68.91%\n",
      "[11, 5400], time 2473.12s loss: 0.866, acc: 68.95%\n",
      "[11, 5600], time 2479.79s loss: 0.863, acc: 68.96%\n",
      "epoch 11 TRAIN error: 0.8708, acc: 0.6896\n",
      "epoch 11 VALIDATION error: 0.8841, acc: 0.6883\n",
      "[12, 200], time 2529.21s loss: 0.854, acc: 70.03%\n",
      "[12, 400], time 2535.82s loss: 0.827, acc: 70.38%\n",
      "[12, 600], time 2542.62s loss: 0.857, acc: 70.14%\n",
      "[12, 800], time 2549.43s loss: 0.853, acc: 69.82%\n",
      "[12, 1000], time 2556.04s loss: 0.831, acc: 69.90%\n",
      "[12, 1200], time 2563.01s loss: 0.821, acc: 70.10%\n",
      "[12, 1400], time 2569.87s loss: 0.834, acc: 70.06%\n",
      "[12, 1600], time 2576.44s loss: 0.842, acc: 70.05%\n",
      "[12, 1800], time 2583.07s loss: 0.854, acc: 70.03%\n",
      "[12, 2000], time 2589.67s loss: 0.842, acc: 70.08%\n",
      "[12, 2200], time 2596.54s loss: 0.833, acc: 70.13%\n",
      "[12, 2400], time 2603.08s loss: 0.860, acc: 70.02%\n",
      "[12, 2600], time 2609.76s loss: 0.840, acc: 69.99%\n",
      "[12, 2800], time 2616.23s loss: 0.833, acc: 70.03%\n",
      "[12, 3000], time 2622.83s loss: 0.878, acc: 69.97%\n",
      "[12, 3200], time 2629.54s loss: 0.854, acc: 69.96%\n",
      "[12, 3400], time 2636.33s loss: 0.846, acc: 69.94%\n",
      "[12, 3600], time 2643.04s loss: 0.880, acc: 69.88%\n",
      "[12, 3800], time 2649.60s loss: 0.827, acc: 69.91%\n",
      "[12, 4000], time 2656.23s loss: 0.864, acc: 69.89%\n",
      "[12, 4200], time 2662.89s loss: 0.856, acc: 69.88%\n",
      "[12, 4400], time 2669.66s loss: 0.853, acc: 69.88%\n",
      "[12, 4600], time 2676.19s loss: 0.856, acc: 69.87%\n",
      "[12, 4800], time 2682.89s loss: 0.856, acc: 69.87%\n",
      "[12, 5000], time 2689.59s loss: 0.869, acc: 69.84%\n",
      "[12, 5200], time 2696.24s loss: 0.858, acc: 69.84%\n",
      "[12, 5400], time 2702.86s loss: 0.837, acc: 69.84%\n",
      "[12, 5600], time 2709.45s loss: 0.870, acc: 69.82%\n",
      "epoch 12 TRAIN error: 0.8495, acc: 0.6981\n",
      "epoch 12 VALIDATION error: 0.8429, acc: 0.7008\n",
      "[13, 200], time 2758.00s loss: 0.812, acc: 71.34%\n",
      "[13, 400], time 2765.09s loss: 0.815, acc: 71.12%\n",
      "[13, 600], time 2771.98s loss: 0.818, acc: 71.08%\n",
      "[13, 800], time 2778.60s loss: 0.827, acc: 70.95%\n",
      "[13, 1000], time 2785.20s loss: 0.837, acc: 70.77%\n",
      "[13, 1200], time 2791.83s loss: 0.811, acc: 70.84%\n",
      "[13, 1400], time 2798.48s loss: 0.813, acc: 70.85%\n",
      "[13, 1600], time 2805.19s loss: 0.836, acc: 70.82%\n",
      "[13, 1800], time 2811.79s loss: 0.841, acc: 70.74%\n",
      "[13, 2000], time 2818.56s loss: 0.846, acc: 70.65%\n",
      "[13, 2200], time 2825.32s loss: 0.849, acc: 70.58%\n",
      "[13, 2400], time 2832.04s loss: 0.818, acc: 70.59%\n",
      "[13, 2600], time 2839.65s loss: 0.828, acc: 70.59%\n",
      "[13, 2800], time 2846.49s loss: 0.833, acc: 70.58%\n",
      "[13, 3000], time 2853.01s loss: 0.831, acc: 70.57%\n",
      "[13, 3200], time 2859.63s loss: 0.830, acc: 70.54%\n",
      "[13, 3400], time 2866.26s loss: 0.811, acc: 70.55%\n",
      "[13, 3600], time 2872.96s loss: 0.841, acc: 70.48%\n",
      "[13, 3800], time 2879.66s loss: 0.845, acc: 70.48%\n",
      "[13, 4000], time 2886.34s loss: 0.817, acc: 70.55%\n",
      "[13, 4200], time 2893.06s loss: 0.817, acc: 70.56%\n",
      "[13, 4400], time 2899.67s loss: 0.835, acc: 70.52%\n",
      "[13, 4600], time 2906.38s loss: 0.832, acc: 70.50%\n",
      "[13, 4800], time 2913.11s loss: 0.835, acc: 70.50%\n",
      "[13, 5000], time 2919.76s loss: 0.825, acc: 70.54%\n",
      "[13, 5200], time 2926.27s loss: 0.823, acc: 70.55%\n",
      "[13, 5400], time 2932.98s loss: 0.829, acc: 70.54%\n",
      "[13, 5600], time 2939.41s loss: 0.818, acc: 70.57%\n",
      "epoch 13 TRAIN error: 0.8277, acc: 0.7056\n",
      "epoch 13 VALIDATION error: 0.8844, acc: 0.6919\n",
      "[14, 200], time 2986.99s loss: 0.794, acc: 71.80%\n",
      "[14, 400], time 2993.66s loss: 0.816, acc: 71.27%\n",
      "[14, 600], time 3000.25s loss: 0.787, acc: 71.43%\n",
      "[14, 800], time 3006.94s loss: 0.789, acc: 71.57%\n",
      "[14, 1000], time 3013.62s loss: 0.793, acc: 71.71%\n",
      "[14, 1200], time 3020.24s loss: 0.800, acc: 71.66%\n",
      "[14, 1400], time 3026.80s loss: 0.813, acc: 71.56%\n",
      "[14, 1600], time 3033.39s loss: 0.812, acc: 71.48%\n",
      "[14, 1800], time 3040.04s loss: 0.801, acc: 71.53%\n",
      "[14, 2000], time 3046.73s loss: 0.807, acc: 71.51%\n",
      "[14, 2200], time 3053.46s loss: 0.807, acc: 71.48%\n",
      "[14, 2400], time 3060.07s loss: 0.816, acc: 71.43%\n",
      "[14, 2600], time 3066.72s loss: 0.798, acc: 71.46%\n",
      "[14, 2800], time 3073.40s loss: 0.796, acc: 71.46%\n",
      "[14, 3000], time 3080.09s loss: 0.832, acc: 71.40%\n",
      "[14, 3200], time 3086.87s loss: 0.831, acc: 71.34%\n",
      "[14, 3400], time 3093.61s loss: 0.809, acc: 71.35%\n",
      "[14, 3600], time 3100.32s loss: 0.830, acc: 71.29%\n",
      "[14, 3800], time 3106.87s loss: 0.809, acc: 71.29%\n",
      "[14, 4000], time 3113.37s loss: 0.822, acc: 71.26%\n",
      "[14, 4200], time 3120.04s loss: 0.824, acc: 71.26%\n",
      "[14, 4400], time 3126.60s loss: 0.801, acc: 71.27%\n",
      "[14, 4600], time 3133.37s loss: 0.827, acc: 71.25%\n",
      "[14, 4800], time 3139.97s loss: 0.824, acc: 71.22%\n",
      "[14, 5000], time 3146.62s loss: 0.820, acc: 71.20%\n",
      "[14, 5200], time 3153.36s loss: 0.831, acc: 71.17%\n",
      "[14, 5400], time 3160.00s loss: 0.804, acc: 71.17%\n",
      "[14, 5600], time 3166.66s loss: 0.802, acc: 71.17%\n",
      "epoch 14 TRAIN error: 0.8105, acc: 0.7117\n",
      "epoch 14 VALIDATION error: 0.8372, acc: 0.7050\n",
      "[15, 200], time 3214.89s loss: 0.780, acc: 72.89%\n",
      "[15, 400], time 3221.37s loss: 0.811, acc: 72.22%\n",
      "[15, 600], time 3227.96s loss: 0.788, acc: 72.26%\n",
      "[15, 800], time 3234.61s loss: 0.759, acc: 72.54%\n",
      "[15, 1000], time 3241.03s loss: 0.814, acc: 72.24%\n",
      "[15, 1200], time 3247.55s loss: 0.796, acc: 72.21%\n",
      "[15, 1400], time 3253.97s loss: 0.783, acc: 72.15%\n",
      "[15, 1600], time 3260.55s loss: 0.796, acc: 72.07%\n",
      "[15, 1800], time 3266.87s loss: 0.802, acc: 72.00%\n",
      "[15, 2000], time 3273.36s loss: 0.782, acc: 72.01%\n",
      "[15, 2200], time 3279.82s loss: 0.789, acc: 71.98%\n",
      "[15, 2400], time 3286.38s loss: 0.782, acc: 72.00%\n",
      "[15, 2600], time 3293.02s loss: 0.804, acc: 71.96%\n",
      "[15, 2800], time 3299.55s loss: 0.791, acc: 71.94%\n",
      "[15, 3000], time 3306.10s loss: 0.769, acc: 72.00%\n",
      "[15, 3200], time 3312.70s loss: 0.796, acc: 72.02%\n",
      "[15, 3400], time 3319.16s loss: 0.808, acc: 71.97%\n",
      "[15, 3600], time 3325.62s loss: 0.795, acc: 71.95%\n",
      "[15, 3800], time 3332.21s loss: 0.771, acc: 71.99%\n",
      "[15, 4000], time 3338.67s loss: 0.811, acc: 71.96%\n",
      "[15, 4200], time 3345.23s loss: 0.793, acc: 71.95%\n",
      "[15, 4400], time 3351.81s loss: 0.773, acc: 71.96%\n",
      "[15, 4600], time 3358.41s loss: 0.795, acc: 71.95%\n",
      "[15, 4800], time 3365.02s loss: 0.779, acc: 71.99%\n",
      "[15, 5000], time 3371.69s loss: 0.794, acc: 71.97%\n",
      "[15, 5200], time 3378.39s loss: 0.816, acc: 71.95%\n",
      "[15, 5400], time 3384.83s loss: 0.795, acc: 71.93%\n",
      "[15, 5600], time 3391.44s loss: 0.793, acc: 71.92%\n",
      "epoch 15 TRAIN error: 0.7915, acc: 0.7192\n",
      "epoch 15 VALIDATION error: 0.8333, acc: 0.7106\n",
      "[16, 200], time 3439.92s loss: 0.775, acc: 72.11%\n",
      "[16, 400], time 3446.57s loss: 0.758, acc: 72.32%\n",
      "[16, 600], time 3453.31s loss: 0.781, acc: 72.48%\n",
      "[16, 800], time 3459.94s loss: 0.783, acc: 72.39%\n",
      "[16, 1000], time 3466.62s loss: 0.768, acc: 72.54%\n",
      "[16, 1200], time 3473.23s loss: 0.765, acc: 72.55%\n",
      "[16, 1400], time 3479.85s loss: 0.783, acc: 72.48%\n",
      "[16, 1600], time 3486.18s loss: 0.787, acc: 72.42%\n",
      "[16, 1800], time 3492.99s loss: 0.773, acc: 72.44%\n",
      "[16, 2000], time 3499.76s loss: 0.788, acc: 72.43%\n",
      "[16, 2200], time 3506.49s loss: 0.797, acc: 72.37%\n",
      "[16, 2400], time 3513.20s loss: 0.776, acc: 72.32%\n",
      "[16, 2600], time 3519.73s loss: 0.781, acc: 72.37%\n",
      "[16, 2800], time 3526.38s loss: 0.761, acc: 72.37%\n",
      "[16, 3000], time 3533.01s loss: 0.767, acc: 72.39%\n",
      "[16, 3200], time 3539.32s loss: 0.745, acc: 72.45%\n",
      "[16, 3400], time 3545.99s loss: 0.781, acc: 72.45%\n",
      "[16, 3600], time 3552.64s loss: 0.798, acc: 72.41%\n",
      "[16, 3800], time 3559.39s loss: 0.758, acc: 72.40%\n",
      "[16, 4000], time 3565.93s loss: 0.769, acc: 72.41%\n",
      "[16, 4200], time 3572.42s loss: 0.782, acc: 72.41%\n",
      "[16, 4400], time 3578.87s loss: 0.778, acc: 72.39%\n",
      "[16, 4600], time 3585.64s loss: 0.771, acc: 72.40%\n",
      "[16, 4800], time 3592.31s loss: 0.760, acc: 72.41%\n",
      "[16, 5000], time 3598.91s loss: 0.798, acc: 72.38%\n",
      "[16, 5200], time 3605.54s loss: 0.767, acc: 72.39%\n",
      "[16, 5400], time 3612.25s loss: 0.774, acc: 72.37%\n",
      "[16, 5600], time 3618.88s loss: 0.793, acc: 72.34%\n",
      "epoch 16 TRAIN error: 0.7759, acc: 0.7233\n",
      "epoch 16 VALIDATION error: 0.8150, acc: 0.7157\n",
      "[17, 200], time 3667.20s loss: 0.762, acc: 72.80%\n",
      "[17, 400], time 3673.94s loss: 0.747, acc: 73.29%\n",
      "[17, 600], time 3680.57s loss: 0.741, acc: 73.55%\n",
      "[17, 800], time 3687.19s loss: 0.735, acc: 73.60%\n",
      "[17, 1000], time 3693.72s loss: 0.756, acc: 73.63%\n",
      "[17, 1200], time 3700.15s loss: 0.753, acc: 73.54%\n",
      "[17, 1400], time 3706.78s loss: 0.777, acc: 73.43%\n",
      "[17, 1600], time 3713.37s loss: 0.757, acc: 73.37%\n",
      "[17, 1800], time 3719.98s loss: 0.766, acc: 73.34%\n",
      "[17, 2000], time 3726.69s loss: 0.741, acc: 73.41%\n",
      "[17, 2200], time 3733.40s loss: 0.766, acc: 73.36%\n",
      "[17, 2400], time 3740.24s loss: 0.774, acc: 73.25%\n",
      "[17, 2600], time 3746.85s loss: 0.743, acc: 73.25%\n",
      "[17, 2800], time 3753.55s loss: 0.753, acc: 73.26%\n",
      "[17, 3000], time 3760.28s loss: 0.754, acc: 73.19%\n",
      "[17, 3200], time 3766.99s loss: 0.770, acc: 73.19%\n",
      "[17, 3400], time 3773.63s loss: 0.764, acc: 73.18%\n",
      "[17, 3600], time 3780.38s loss: 0.752, acc: 73.21%\n",
      "[17, 3800], time 3787.17s loss: 0.783, acc: 73.13%\n",
      "[17, 4000], time 3793.87s loss: 0.768, acc: 73.14%\n",
      "[17, 4200], time 3800.51s loss: 0.783, acc: 73.06%\n",
      "[17, 4400], time 3807.07s loss: 0.762, acc: 73.07%\n",
      "[17, 4600], time 3813.66s loss: 0.770, acc: 73.05%\n",
      "[17, 4800], time 3820.20s loss: 0.772, acc: 73.05%\n",
      "[17, 5000], time 3826.82s loss: 0.770, acc: 73.03%\n",
      "[17, 5200], time 3833.53s loss: 0.796, acc: 73.00%\n",
      "[17, 5400], time 3840.12s loss: 0.768, acc: 73.00%\n",
      "[17, 5600], time 3846.76s loss: 0.783, acc: 72.95%\n",
      "epoch 17 TRAIN error: 0.7629, acc: 0.7295\n",
      "epoch 17 VALIDATION error: 0.8463, acc: 0.7078\n",
      "[18, 200], time 3895.14s loss: 0.727, acc: 74.52%\n",
      "[18, 400], time 3901.72s loss: 0.739, acc: 74.19%\n",
      "[18, 600], time 3908.40s loss: 0.736, acc: 74.03%\n",
      "[18, 800], time 3915.18s loss: 0.741, acc: 73.95%\n",
      "[18, 1000], time 3921.83s loss: 0.747, acc: 73.74%\n",
      "[18, 1200], time 3928.23s loss: 0.762, acc: 73.58%\n",
      "[18, 1400], time 3934.79s loss: 0.795, acc: 73.37%\n",
      "[18, 1600], time 3941.29s loss: 0.736, acc: 73.43%\n",
      "[18, 1800], time 3947.95s loss: 0.755, acc: 73.40%\n",
      "[18, 2000], time 3954.50s loss: 0.762, acc: 73.36%\n",
      "[18, 2200], time 3960.85s loss: 0.763, acc: 73.36%\n",
      "[18, 2400], time 3967.45s loss: 0.752, acc: 73.35%\n",
      "[18, 2600], time 3974.11s loss: 0.749, acc: 73.36%\n",
      "[18, 2800], time 3980.52s loss: 0.744, acc: 73.41%\n",
      "[18, 3000], time 3987.09s loss: 0.744, acc: 73.43%\n",
      "[18, 3200], time 3993.78s loss: 0.750, acc: 73.37%\n",
      "[18, 3400], time 4000.26s loss: 0.775, acc: 73.30%\n",
      "[18, 3600], time 4006.76s loss: 0.745, acc: 73.31%\n",
      "[18, 3800], time 4013.23s loss: 0.742, acc: 73.34%\n",
      "[18, 4000], time 4019.70s loss: 0.748, acc: 73.35%\n",
      "[18, 4200], time 4026.34s loss: 0.757, acc: 73.36%\n",
      "[18, 4400], time 4032.94s loss: 0.746, acc: 73.36%\n",
      "[18, 4600], time 4039.45s loss: 0.767, acc: 73.32%\n",
      "[18, 4800], time 4046.06s loss: 0.773, acc: 73.28%\n",
      "[18, 5000], time 4052.67s loss: 0.766, acc: 73.27%\n",
      "[18, 5200], time 4059.33s loss: 0.742, acc: 73.29%\n",
      "[18, 5400], time 4066.04s loss: 0.743, acc: 73.29%\n",
      "[18, 5600], time 4072.54s loss: 0.755, acc: 73.31%\n",
      "epoch 18 TRAIN error: 0.7520, acc: 0.7331\n",
      "epoch 18 VALIDATION error: 0.8446, acc: 0.7077\n",
      "[19, 200], time 4120.58s loss: 0.723, acc: 74.64%\n",
      "[19, 400], time 4127.25s loss: 0.724, acc: 74.39%\n",
      "[19, 600], time 4133.96s loss: 0.703, acc: 74.52%\n",
      "[19, 800], time 4140.68s loss: 0.731, acc: 74.39%\n",
      "[19, 1000], time 4147.34s loss: 0.738, acc: 74.23%\n",
      "[19, 1200], time 4154.06s loss: 0.727, acc: 74.18%\n",
      "[19, 1400], time 4160.77s loss: 0.714, acc: 74.23%\n",
      "[19, 1600], time 4167.55s loss: 0.726, acc: 74.24%\n",
      "[19, 1800], time 4174.15s loss: 0.718, acc: 74.20%\n",
      "[19, 2000], time 4180.79s loss: 0.737, acc: 74.12%\n",
      "[19, 2200], time 4187.48s loss: 0.722, acc: 74.08%\n",
      "[19, 2400], time 4194.19s loss: 0.738, acc: 74.03%\n",
      "[19, 2600], time 4200.89s loss: 0.738, acc: 74.02%\n",
      "[19, 2800], time 4207.57s loss: 0.759, acc: 73.97%\n",
      "[19, 3000], time 4214.16s loss: 0.765, acc: 73.89%\n",
      "[19, 3200], time 4220.71s loss: 0.747, acc: 73.88%\n",
      "[19, 3400], time 4227.14s loss: 0.756, acc: 73.80%\n",
      "[19, 3600], time 4233.79s loss: 0.729, acc: 73.83%\n",
      "[19, 3800], time 4240.53s loss: 0.730, acc: 73.82%\n",
      "[19, 4000], time 4247.19s loss: 0.718, acc: 73.85%\n",
      "[19, 4200], time 4253.80s loss: 0.734, acc: 73.91%\n",
      "[19, 4400], time 4260.53s loss: 0.745, acc: 73.89%\n",
      "[19, 4600], time 4267.21s loss: 0.756, acc: 73.85%\n",
      "[19, 4800], time 4273.73s loss: 0.747, acc: 73.82%\n",
      "[19, 5000], time 4280.49s loss: 0.740, acc: 73.84%\n",
      "[19, 5200], time 4286.96s loss: 0.749, acc: 73.82%\n",
      "[19, 5400], time 4293.58s loss: 0.741, acc: 73.81%\n",
      "[19, 5600], time 4300.24s loss: 0.763, acc: 73.79%\n",
      "epoch 19 TRAIN error: 0.7365, acc: 0.7378\n",
      "epoch 19 VALIDATION error: 0.8478, acc: 0.7120\n",
      "[20, 200], time 4348.04s loss: 0.729, acc: 73.91%\n",
      "[20, 400], time 4354.52s loss: 0.716, acc: 74.01%\n",
      "[20, 600], time 4361.19s loss: 0.717, acc: 74.05%\n",
      "[20, 800], time 4367.74s loss: 0.729, acc: 73.97%\n",
      "[20, 1000], time 4374.27s loss: 0.728, acc: 73.97%\n",
      "[20, 1200], time 4380.70s loss: 0.725, acc: 74.05%\n",
      "[20, 1400], time 4387.28s loss: 0.733, acc: 73.99%\n",
      "[20, 1600], time 4394.07s loss: 0.701, acc: 74.13%\n",
      "[20, 1800], time 4400.69s loss: 0.744, acc: 74.04%\n",
      "[20, 2000], time 4407.35s loss: 0.738, acc: 74.07%\n",
      "[20, 2200], time 4414.09s loss: 0.719, acc: 74.08%\n",
      "[20, 2400], time 4420.73s loss: 0.692, acc: 74.21%\n",
      "[20, 2600], time 4427.39s loss: 0.718, acc: 74.25%\n",
      "[20, 2800], time 4434.08s loss: 0.719, acc: 74.25%\n",
      "[20, 3000], time 4440.74s loss: 0.724, acc: 74.26%\n",
      "[20, 3200], time 4447.39s loss: 0.713, acc: 74.26%\n",
      "[20, 3400], time 4454.03s loss: 0.746, acc: 74.25%\n",
      "[20, 3600], time 4460.67s loss: 0.755, acc: 74.21%\n",
      "[20, 3800], time 4467.24s loss: 0.746, acc: 74.18%\n",
      "[20, 4000], time 4473.88s loss: 0.715, acc: 74.22%\n",
      "[20, 4200], time 4480.59s loss: 0.726, acc: 74.22%\n",
      "[20, 4400], time 4487.31s loss: 0.714, acc: 74.25%\n",
      "[20, 4600], time 4494.01s loss: 0.722, acc: 74.26%\n",
      "[20, 4800], time 4500.64s loss: 0.724, acc: 74.25%\n",
      "[20, 5000], time 4507.28s loss: 0.742, acc: 74.21%\n",
      "[20, 5200], time 4513.95s loss: 0.719, acc: 74.23%\n",
      "[20, 5400], time 4520.64s loss: 0.726, acc: 74.23%\n",
      "[20, 5600], time 4527.27s loss: 0.709, acc: 74.26%\n",
      "epoch 20 TRAIN error: 0.7246, acc: 0.7426\n",
      "epoch 20 VALIDATION error: 0.7899, acc: 0.7271\n",
      "[21, 200], time 4575.04s loss: 0.704, acc: 74.72%\n",
      "[21, 400], time 4581.70s loss: 0.700, acc: 74.55%\n",
      "[21, 600], time 4588.27s loss: 0.674, acc: 75.13%\n",
      "[21, 800], time 4594.92s loss: 0.700, acc: 75.03%\n",
      "[21, 1000], time 4601.54s loss: 0.696, acc: 75.05%\n",
      "[21, 1200], time 4608.16s loss: 0.712, acc: 74.97%\n",
      "[21, 1400], time 4614.71s loss: 0.724, acc: 74.95%\n",
      "[21, 1600], time 4621.38s loss: 0.723, acc: 74.86%\n",
      "[21, 1800], time 4627.97s loss: 0.721, acc: 74.75%\n",
      "[21, 2000], time 4634.43s loss: 0.709, acc: 74.78%\n",
      "[21, 2200], time 4641.06s loss: 0.717, acc: 74.77%\n",
      "[21, 2400], time 4647.71s loss: 0.725, acc: 74.69%\n",
      "[21, 2600], time 4654.21s loss: 0.712, acc: 74.68%\n",
      "[21, 2800], time 4660.77s loss: 0.730, acc: 74.64%\n",
      "[21, 3000], time 4667.36s loss: 0.731, acc: 74.60%\n",
      "[21, 3200], time 4673.98s loss: 0.710, acc: 74.61%\n",
      "[21, 3400], time 4680.36s loss: 0.717, acc: 74.62%\n",
      "[21, 3600], time 4687.03s loss: 0.713, acc: 74.63%\n",
      "[21, 3800], time 4694.37s loss: 0.717, acc: 74.62%\n",
      "[21, 4000], time 4701.32s loss: 0.704, acc: 74.65%\n",
      "[21, 4200], time 4707.77s loss: 0.712, acc: 74.67%\n",
      "[21, 4400], time 4714.27s loss: 0.726, acc: 74.66%\n",
      "[21, 4600], time 4720.96s loss: 0.751, acc: 74.59%\n",
      "[21, 4800], time 4727.47s loss: 0.723, acc: 74.60%\n",
      "[21, 5000], time 4734.05s loss: 0.721, acc: 74.57%\n",
      "[21, 5200], time 4740.66s loss: 0.733, acc: 74.54%\n",
      "[21, 5400], time 4747.31s loss: 0.711, acc: 74.55%\n",
      "[21, 5600], time 4753.70s loss: 0.712, acc: 74.56%\n",
      "epoch 21 TRAIN error: 0.7151, acc: 0.7457\n",
      "epoch 21 VALIDATION error: 0.7733, acc: 0.7340\n",
      "[22, 200], time 4801.40s loss: 0.679, acc: 75.58%\n",
      "[22, 400], time 4808.12s loss: 0.685, acc: 75.44%\n",
      "[22, 600], time 4814.74s loss: 0.705, acc: 75.35%\n",
      "[22, 800], time 4821.46s loss: 0.681, acc: 75.53%\n",
      "[22, 1000], time 4828.14s loss: 0.688, acc: 75.59%\n",
      "[22, 1200], time 4834.80s loss: 0.707, acc: 75.55%\n",
      "[22, 1400], time 4841.40s loss: 0.705, acc: 75.59%\n",
      "[22, 1600], time 4848.08s loss: 0.695, acc: 75.59%\n",
      "[22, 1800], time 4854.75s loss: 0.710, acc: 75.47%\n",
      "[22, 2000], time 4861.38s loss: 0.703, acc: 75.37%\n",
      "[22, 2200], time 4868.01s loss: 0.716, acc: 75.31%\n",
      "[22, 2400], time 4874.69s loss: 0.703, acc: 75.27%\n",
      "[22, 2600], time 4881.41s loss: 0.687, acc: 75.24%\n",
      "[22, 2800], time 4888.14s loss: 0.695, acc: 75.24%\n",
      "[22, 3000], time 4894.81s loss: 0.683, acc: 75.26%\n",
      "[22, 3200], time 4901.50s loss: 0.723, acc: 75.20%\n",
      "[22, 3400], time 4908.11s loss: 0.706, acc: 75.18%\n",
      "[22, 3600], time 4914.78s loss: 0.718, acc: 75.13%\n",
      "[22, 3800], time 4921.46s loss: 0.720, acc: 75.08%\n",
      "[22, 4000], time 4928.05s loss: 0.710, acc: 75.05%\n",
      "[22, 4200], time 4934.67s loss: 0.694, acc: 75.04%\n",
      "[22, 4400], time 4941.37s loss: 0.703, acc: 75.02%\n",
      "[22, 4600], time 4948.03s loss: 0.726, acc: 74.98%\n",
      "[22, 4800], time 4954.71s loss: 0.706, acc: 74.97%\n",
      "[22, 5000], time 4961.36s loss: 0.711, acc: 74.94%\n",
      "[22, 5200], time 4968.02s loss: 0.703, acc: 74.96%\n",
      "[22, 5400], time 4974.75s loss: 0.732, acc: 74.94%\n",
      "[22, 5600], time 4981.37s loss: 0.705, acc: 74.92%\n",
      "epoch 22 TRAIN error: 0.7037, acc: 0.7491\n",
      "epoch 22 VALIDATION error: 0.7804, acc: 0.7289\n",
      "[23, 200], time 5029.71s loss: 0.677, acc: 76.14%\n",
      "[23, 400], time 5036.31s loss: 0.690, acc: 75.69%\n",
      "[23, 600], time 5042.94s loss: 0.665, acc: 75.88%\n",
      "[23, 800], time 5049.42s loss: 0.704, acc: 75.76%\n",
      "[23, 1000], time 5055.96s loss: 0.685, acc: 75.73%\n",
      "[23, 1200], time 5062.41s loss: 0.681, acc: 75.73%\n",
      "[23, 1400], time 5069.01s loss: 0.688, acc: 75.60%\n",
      "[23, 1600], time 5075.40s loss: 0.696, acc: 75.51%\n",
      "[23, 1800], time 5082.07s loss: 0.706, acc: 75.44%\n",
      "[23, 2000], time 5088.57s loss: 0.674, acc: 75.48%\n",
      "[23, 2200], time 5095.24s loss: 0.699, acc: 75.44%\n",
      "[23, 2400], time 5101.87s loss: 0.704, acc: 75.42%\n",
      "[23, 2600], time 5108.48s loss: 0.719, acc: 75.36%\n",
      "[23, 2800], time 5115.10s loss: 0.678, acc: 75.42%\n",
      "[23, 3000], time 5121.62s loss: 0.690, acc: 75.44%\n",
      "[23, 3200], time 5128.22s loss: 0.689, acc: 75.43%\n",
      "[23, 3400], time 5134.83s loss: 0.704, acc: 75.43%\n",
      "[23, 3600], time 5141.42s loss: 0.682, acc: 75.46%\n",
      "[23, 3800], time 5147.89s loss: 0.680, acc: 75.50%\n",
      "[23, 4000], time 5154.54s loss: 0.701, acc: 75.49%\n",
      "[23, 4200], time 5161.14s loss: 0.715, acc: 75.47%\n",
      "[23, 4400], time 5167.62s loss: 0.700, acc: 75.45%\n",
      "[23, 4600], time 5174.29s loss: 0.728, acc: 75.39%\n",
      "[23, 4800], time 5180.90s loss: 0.700, acc: 75.39%\n",
      "[23, 5000], time 5187.59s loss: 0.725, acc: 75.35%\n",
      "[23, 5200], time 5194.17s loss: 0.696, acc: 75.34%\n",
      "[23, 5400], time 5200.82s loss: 0.700, acc: 75.33%\n",
      "[23, 5600], time 5207.35s loss: 0.710, acc: 75.32%\n",
      "epoch 23 TRAIN error: 0.6960, acc: 0.7532\n",
      "epoch 23 VALIDATION error: 0.7987, acc: 0.7246\n",
      "[24, 200], time 5255.32s loss: 0.669, acc: 76.41%\n",
      "[24, 400], time 5261.97s loss: 0.695, acc: 75.84%\n",
      "[24, 600], time 5268.50s loss: 0.661, acc: 76.08%\n",
      "[24, 800], time 5275.18s loss: 0.668, acc: 76.11%\n",
      "[24, 1000], time 5281.76s loss: 0.693, acc: 75.83%\n",
      "[24, 1200], time 5288.39s loss: 0.676, acc: 75.91%\n",
      "[24, 1400], time 5295.17s loss: 0.684, acc: 75.83%\n",
      "[24, 1600], time 5301.77s loss: 0.663, acc: 75.95%\n",
      "[24, 1800], time 5308.43s loss: 0.692, acc: 75.93%\n",
      "[24, 2000], time 5315.04s loss: 0.695, acc: 75.81%\n",
      "[24, 2200], time 5321.49s loss: 0.667, acc: 75.81%\n",
      "[24, 2400], time 5328.15s loss: 0.668, acc: 75.86%\n",
      "[24, 2600], time 5334.72s loss: 0.681, acc: 75.84%\n",
      "[24, 2800], time 5341.24s loss: 0.695, acc: 75.79%\n",
      "[24, 3000], time 5347.86s loss: 0.706, acc: 75.73%\n",
      "[24, 3200], time 5354.49s loss: 0.708, acc: 75.69%\n",
      "[24, 3400], time 5361.07s loss: 0.695, acc: 75.67%\n",
      "[24, 3600], time 5367.66s loss: 0.678, acc: 75.69%\n",
      "[24, 3800], time 5374.23s loss: 0.711, acc: 75.63%\n",
      "[24, 4000], time 5380.87s loss: 0.687, acc: 75.62%\n",
      "[24, 4200], time 5387.43s loss: 0.672, acc: 75.65%\n",
      "[24, 4400], time 5393.92s loss: 0.682, acc: 75.64%\n",
      "[24, 4600], time 5400.39s loss: 0.689, acc: 75.63%\n",
      "[24, 4800], time 5406.80s loss: 0.717, acc: 75.60%\n",
      "[24, 5000], time 5413.30s loss: 0.702, acc: 75.56%\n",
      "[24, 5200], time 5419.74s loss: 0.697, acc: 75.55%\n",
      "[24, 5400], time 5426.16s loss: 0.709, acc: 75.52%\n",
      "[24, 5600], time 5432.74s loss: 0.702, acc: 75.50%\n",
      "epoch 24 TRAIN error: 0.6880, acc: 0.7549\n",
      "epoch 24 VALIDATION error: 0.7967, acc: 0.7264\n",
      "[25, 200], time 5479.53s loss: 0.650, acc: 76.48%\n",
      "[25, 400], time 5485.99s loss: 0.649, acc: 76.75%\n",
      "[25, 600], time 5492.37s loss: 0.668, acc: 76.53%\n",
      "[25, 800], time 5498.88s loss: 0.664, acc: 76.54%\n",
      "[25, 1000], time 5505.45s loss: 0.674, acc: 76.42%\n",
      "[25, 1200], time 5511.93s loss: 0.685, acc: 76.27%\n",
      "[25, 1400], time 5518.41s loss: 0.666, acc: 76.30%\n",
      "[25, 1600], time 5524.85s loss: 0.674, acc: 76.25%\n",
      "[25, 1800], time 5531.31s loss: 0.697, acc: 76.12%\n",
      "[25, 2000], time 5537.81s loss: 0.664, acc: 76.20%\n",
      "[25, 2200], time 5544.37s loss: 0.691, acc: 76.12%\n",
      "[25, 2400], time 5550.90s loss: 0.658, acc: 76.16%\n",
      "[25, 2600], time 5557.31s loss: 0.697, acc: 76.13%\n",
      "[25, 2800], time 5563.76s loss: 0.692, acc: 76.09%\n",
      "[25, 3000], time 5570.27s loss: 0.712, acc: 75.96%\n",
      "[25, 3200], time 5576.86s loss: 0.687, acc: 75.95%\n",
      "[25, 3400], time 5583.54s loss: 0.683, acc: 75.94%\n",
      "[25, 3600], time 5590.37s loss: 0.667, acc: 75.94%\n",
      "[25, 3800], time 5597.29s loss: 0.694, acc: 75.89%\n",
      "[25, 4000], time 5604.04s loss: 0.692, acc: 75.88%\n",
      "[25, 4200], time 5610.75s loss: 0.690, acc: 75.85%\n",
      "[25, 4400], time 5617.28s loss: 0.676, acc: 75.84%\n",
      "[25, 4600], time 5624.12s loss: 0.694, acc: 75.83%\n",
      "[25, 4800], time 5630.83s loss: 0.696, acc: 75.82%\n",
      "[25, 5000], time 5637.50s loss: 0.670, acc: 75.85%\n",
      "[25, 5200], time 5644.04s loss: 0.690, acc: 75.83%\n",
      "[25, 5400], time 5650.77s loss: 0.704, acc: 75.80%\n",
      "[25, 5600], time 5657.39s loss: 0.706, acc: 75.77%\n",
      "epoch 25 TRAIN error: 0.6818, acc: 0.7577\n",
      "epoch 25 VALIDATION error: 0.8039, acc: 0.7280\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "train_errors, val_errors = train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2c178d8-3ee6-4364-9c86-72e109659da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m val_errors \u001b[38;5;241m=\u001b[39m [error\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m val_errors]\n\u001b[1;32m----> 2\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_errors\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m val_errors \u001b[38;5;241m=\u001b[39m [error\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m val_errors]\n\u001b[1;32m----> 2\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m [\u001b[43merror\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m train_errors]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "val_errors = [error.item() for error in val_errors]\n",
    "train_errors = [error.item() for error in train_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34c68e32-2c12-46b4-8d7c-e4996d7e1caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3990191221237183,\n",
       " 1.2330001592636108,\n",
       " 1.1466305255889893,\n",
       " 1.071427345275879,\n",
       " 1.0172206163406372,\n",
       " 0.9600944519042969,\n",
       " 0.9596141576766968,\n",
       " 0.9200401306152344,\n",
       " 0.9028377532958984,\n",
       " 0.9043552279472351,\n",
       " 0.884101390838623,\n",
       " 0.8428972959518433,\n",
       " 0.8844390511512756,\n",
       " 0.8371550440788269,\n",
       " 0.8333303928375244,\n",
       " 0.8150157332420349,\n",
       " 0.8462653160095215,\n",
       " 0.8445577621459961,\n",
       " 0.8477894067764282,\n",
       " 0.7899123430252075,\n",
       " 0.7733379602432251,\n",
       " 0.7804141640663147,\n",
       " 0.7987033128738403,\n",
       " 0.7967380285263062,\n",
       " 0.8039102554321289]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3676e3a-a5d2-43ce-9227-ae815cb351ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6401697391721937,\n",
       " 1.3514976713498434,\n",
       " 1.234424605295393,\n",
       " 1.1527789666811625,\n",
       " 1.0896000926441616,\n",
       " 1.0351025211811065,\n",
       " 0.9925279830349816,\n",
       " 0.9556316237396664,\n",
       " 0.923247714334064,\n",
       " 0.8963217977205912,\n",
       " 0.8708394589477115,\n",
       " 0.8495361325475904,\n",
       " 0.827725604391098,\n",
       " 0.8104668157524533,\n",
       " 0.7915131389723884,\n",
       " 0.7758552071041531,\n",
       " 0.7629392334037357,\n",
       " 0.7520453106827206,\n",
       " 0.7365402299722036,\n",
       " 0.7245674634271198,\n",
       " 0.7150805357297262,\n",
       " 0.7037374505996704,\n",
       " 0.6959792173438603,\n",
       " 0.6879621674034331,\n",
       " 0.6817698967933655]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4198d-1953-400a-80aa-fb5b0c872f17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf99e9d8-ab48-4c28-a916-65aebb449c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_14556\\1559688700.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6790635-d811-4f45-a058-bb64096ec78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch NONE val error: 0.00020540178229566664, acc: 0.5511888888888888\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9378cb-13db-4ede-95f1-7184e9fec962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df1812b-6cf9-4869-a17a-f5d985d3a229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, labels = imgs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed978fd2-bad4-459f-9b69-6ff69ece7dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = model(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dee69b-3f4e-4db0-8181-673332054000",
   "metadata": {},
   "source": [
    "---\n",
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9d938757-7f39-4a9d-a5cb-8cd3e6bbb734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d34befa3-36e7-441c-b34c-21c30527918e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    #tensor_image = transform(Image.fromarray(img)).unsqueeze(0)\n",
    "    tensor_image = img_tensor.unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_image)\n",
    "\n",
    "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Denormalize if your transform normalized the images\n",
    "    img_np = img_np * 0.5 + 0.5  # Adjust based on your normalization values\n",
    "    img_np = np.clip(img_np, 0, 1)    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    \n",
    "    ax1.imshow(img_np)\n",
    "    proba = torch.nn.functional.softmax(output, dim=1)[0].tolist()\n",
    "    sns.barplot(x=trainset.classes, y=proba, ax = ax2)\n",
    "    ax2.set_xticks(ax2.get_xticks(), ax2.get_xticklabels(), rotation=45)\n",
    "    plt.show()\n",
    "    print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c352ef70-f811-46aa-84ce-f3e2e64a0216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09197390079498291, 0.13637155294418335, 0.1987992525100708, 0.12512052059173584, 0.14453724026679993, 0.05299113318324089, 0.06924711912870407, 0.007855124771595001, 0.14874224364757538, 0.024361854419112206]\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = valloader.dataset[29000]\n",
    "predict(img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (intotoml)",
   "language": "python",
   "name": "introtoml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
