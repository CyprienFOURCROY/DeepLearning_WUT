{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e0bf32a-c70c-4a97-8680-0d13f617bed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65308b86-9c3a-4094-ac29-d374c13b0572",
   "metadata": {},
   "source": [
    "# DenseNet_model14 brightness_contrast augmentation architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d827583-b0c0-4ff3-9925-0589cd7a873e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseLayer_model14(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer_model14, self).__init__()\n",
    "        # BN-ReLU-Conv(1x1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "        # BN-ReLU-Conv(3x3)\n",
    "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        new_features = self.conv1(self.relu1(self.bn1(x)))\n",
    "        new_features = self.conv2(self.relu2(self.bn2(new_features)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class DenseBlock_model14(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseBlock_model14, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.add_module('DenseLayer_model14%d' % (i + 1),\n",
    "                                  DenseLayer_model14(in_channels + i * growth_rate, growth_rate, bn_size, drop_rate))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = x\n",
    "        for layer in self.layers:\n",
    "            features = layer(features)\n",
    "        return features\n",
    "\n",
    "class Transition_model14(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transition_model14, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DenseNet_model14(nn.Module):\n",
    "    def __init__(self, growth_rate=12, block_config=(6, 12, 8), \n",
    "                 num_init_features=32, bn_size=4, drop_rate=0.3, num_classes=10):\n",
    "        super(DenseNet_model14, self).__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Each DenseBlock_model14\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock_model14(\n",
    "                num_layers=num_layers,\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.features.add_module('DenseBlock_model14%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            # Add a Transition_model14 layer between dense blocks (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition_model14(in_channels=num_features, out_channels=num_features // 2)\n",
    "                self.features.add_module('Transition_model14%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.features.add_module('relu5', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0689e7d-816f-4086-af1a-b73f54d663cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model\n",
    "model_brightness_contrast = torch.load(\"../DenseNet/model_id_14_pelny_chyba.pth\", weights_only=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_brightness_contrast.eval()\n",
    "\n",
    "# Define preprocessing transformations (adjust based on how your model was trained)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load and preprocess your image\n",
    "image_path = \"../../data/raw/valid/airplane/cifar10-train-10031.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Handle device compatibility (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_brightness_contrast = model_brightness_contrast.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output = model_brightness_contrast(input_batch)\n",
    "\n",
    "# Process the output based on your model type\n",
    "# For classification:\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f408291-23b3-4916-99fd-d994a1fe2759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "trainset_raw = torchvision.datasets.ImageFolder('../../data/raw/train/', transform=transform)\n",
    "trainloader_raw = torch.utils.data.DataLoader(trainset_raw, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valset = torchvision.datasets.ImageFolder('../../data/raw/valid/', transform=preprocess)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06adc890-5716-40a1-8344-0fd41e9a3146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000270D33A84A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micha\\Envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\micha\\Envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1582, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 109, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    model_brightness_contrast.eval()\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_brightness_contrast(images)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e2ba1-9720-4888-9cf9-328f8d964553",
   "metadata": {},
   "source": [
    "# DenseNet rotation augmentation architecture (model 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24425533-753e-4875-b94b-fbab7af107ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        # BN-ReLU-Conv(1x1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "        # BN-ReLU-Conv(3x3)\n",
    "        self.bn2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        new_features = self.conv1(self.relu1(self.bn1(x)))\n",
    "        new_features = self.conv2(self.relu2(self.bn2(new_features)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.add_module('denselayer%d' % (i + 1),\n",
    "                                  DenseLayer(in_channels + i * growth_rate, growth_rate, bn_size, drop_rate))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = x\n",
    "        for layer in self.layers:\n",
    "            features = layer(features)\n",
    "        return features\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=12, block_config=(6, 12, 8), \n",
    "                 num_init_features=32, bn_size=4, drop_rate=0.2, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            # Add a dense block\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            # Add a transition layer between dense blocks (except after the last block)\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(in_channels=num_features, out_channels=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "        self.features.add_module('relu5', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b808809-1875-4f94-9738-5fc3762ade48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model\n",
    "model_rotation = DenseNet(growth_rate=12, block_config=(6, 12, 8), num_classes=10)\n",
    "state_dict  = torch.load(\"../basic_CNN/model_id_12_DenseNET.pth\")\n",
    "model_rotation.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_rotation.eval()\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "model_rotation = model_rotation.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output = model_rotation(input_batch)\n",
    "\n",
    "# Process the output based on your model type\n",
    "# For classification:\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcd733-7d90-4934-a417-e898f37ea455",
   "metadata": {},
   "source": [
    "# Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8030d3c-4e0f-4697-a5a3-764392a3c016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(image, model1, model2):\n",
    "    # Get predictions from both models\n",
    "    with torch.no_grad():\n",
    "        output1 = model1(image)\n",
    "        output2 = model2(image)\n",
    "    \n",
    "    # For classification (voting)\n",
    "    if output1.shape[1] > 1:  # Multi-class\n",
    "        # Average the probabilities\n",
    "        avg_output = (output1 + output2) / 2\n",
    "        # Or use hard voting\n",
    "        # pred1 = output1.argmax(dim=1)\n",
    "        # pred2 = output2.argmax(dim=1)\n",
    "        # Use most common prediction\n",
    "    \n",
    "    # For regression\n",
    "    else:\n",
    "        avg_output = (output1 + output2) / 2\n",
    "        \n",
    "    return avg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8ec9c2c-f5a9-4889-9bb8-fea6685aab87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(image_path).convert('RGB')\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output = ensemble_predict(input_batch, model_brightness_contrast, model_rotation)\n",
    "\n",
    "# Process the output based on your model type\n",
    "# For classification:\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "print(f\"Predicted class: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3bbc75d-aa1b-47ef-98d3-901dad35a4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch NONE val error: 9.175079321721569e-05, acc: 0.7689333333333334\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ensemble_predict(images, model_brightness_contrast, model_rotation)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7be797d-692c-4ff0-9125-27d7a89a18ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict_hard_voting(image, model1, model2):\n",
    "    # Get predictions from both models\n",
    "    with torch.no_grad():\n",
    "        output1 = model1(image)\n",
    "        output2 = model2(image)\n",
    "    \n",
    "    # Get the predicted class from each model\n",
    "    pred1 = output1.argmax(dim=1)\n",
    "    pred2 = output2.argmax(dim=1)\n",
    "    \n",
    "    batch_size = image.shape[0]\n",
    "    num_classes = output1.shape[1]\n",
    "    \n",
    "    # Initialize output tensor with zeros\n",
    "    ensemble_output = torch.zeros((batch_size, num_classes), device=image.device)\n",
    "    \n",
    "    # Implement hard voting for each sample in the batch\n",
    "    for i in range(batch_size):\n",
    "        # Get confidence scores for both models\n",
    "        conf1 = output1[i, pred1[i]]\n",
    "        conf2 = output2[i, pred2[i]]\n",
    "        \n",
    "        # If models agree, use their prediction\n",
    "        if pred1[i] == pred2[i]:\n",
    "            chosen_class = pred1[i]\n",
    "        else:\n",
    "            # If models disagree, choose the one with higher confidence\n",
    "            chosen_class = pred1[i] if conf1 > conf2 else pred2[i]\n",
    "        \n",
    "        # Use the probabilities from the model with higher confidence\n",
    "        ensemble_output[i] = output1[i] if conf1 > conf2 else output2[i]\n",
    "    \n",
    "    return ensemble_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "647623ef-3444-4ff2-bd71-bcd7541fa078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch NONE val error: 0.00011848946451209486, acc: 0.7558555555555555\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ensemble_predict_hard_voting(images, model_brightness_contrast, model_rotation)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eede8-2136-47f2-83a8-89f99cc74aad",
   "metadata": {},
   "source": [
    "# Hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78e46c-7b03-4f37-9902-e0f59b4b4d6e",
   "metadata": {},
   "source": [
    "# Weighted ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49c5a82f-8816-4690-ba9f-077d4984b7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_ensemble(image, model1, model2, weight1=0.6, weight2=0.4):\n",
    "    with torch.no_grad():\n",
    "        output1 = model1(image)\n",
    "        output2 = model2(image)\n",
    "    \n",
    "    # Weight the outputs based on model performance\n",
    "    weighted_output = weight1 * output1 + weight2 * output2\n",
    "    return weighted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33cada88-62ed-43b2-b5f9-e7cebfad8528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch NONE val error: 9.493881952948868e-05, acc: 0.7687111111111111\n"
     ]
    }
   ],
   "source": [
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = weighted_ensemble(images, model_brightness_contrast, model_rotation, weight1=0.55, weight2=0.45)\n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "        val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch NONE val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e8d43-33f6-414f-971f-0fffb28051db",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c2bdd29-70ba-426a-9a75-5a52b68fd6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=2, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_error = float('inf')\n",
    "    \n",
    "    def check(self, val_error, model):\n",
    "        if val_error < self.min_error - self.min_delta:\n",
    "            self.min_error = val_error\n",
    "            # Save state_dict instead of entire model\n",
    "            torch.save(model.state_dict(), 'best_ensemble_model.pth')\n",
    "            self.counter = 0\n",
    "            return (False, True)  # Not stopping, but saved new best model\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return (True, False)  # Stopping, no new best model\n",
    "            return (False, False)  # Not stopping, no new best model\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2, meta_layer_size=64):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        \n",
    "        # Freeze base models\n",
    "        for param in self.model1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model2.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Determine output size of the models\n",
    "        try:\n",
    "            # Try to get output size from the final layer\n",
    "            last_layer1 = list(model_brightness_contrast.modules())[-1]\n",
    "            while not isinstance(last_layer1, nn.Linear) and len(list(last_layer1.children())) == 0:\n",
    "                last_layer1 = list(model_brightness_contrast.modules())[-2]\n",
    "            \n",
    "            last_layer2 = list(model_rotation.modules())[-1]\n",
    "            while not isinstance(last_layer2, nn.Linear) and len(list(last_layer2.children())) == 0:\n",
    "                last_layer2 = list(model_rotation.modules())[-2]\n",
    "                \n",
    "            output_size1 = last_layer1.out_features\n",
    "            output_size2 = last_layer2.out_features\n",
    "            \n",
    "            # Check if outputs have same dimensions\n",
    "            if output_size1 != output_size2:\n",
    "                raise ValueError(\"Models have different output dimensions\")\n",
    "            \n",
    "            output_size = output_size1\n",
    "        except:\n",
    "            # Fallback - you'll need to specify this manually if automatic detection fails\n",
    "            output_size = 1000  # replace with your actual output size\n",
    "            print(\"Couldn't detect output size automatically. Using default:\", output_size)\n",
    "        \n",
    "        # Create meta-learning layers\n",
    "        self.meta_layer = nn.Sequential(\n",
    "            nn.Linear(output_size * 2, meta_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Add dropout for regularization\n",
    "            nn.Linear(meta_layer_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get outputs from both models\n",
    "        with torch.no_grad():  # Ensure base models aren't updated\n",
    "            out1 = self.model1(x)\n",
    "            out2 = self.model2(x)\n",
    "        \n",
    "        # Concatenate outputs\n",
    "        combined = torch.cat((out1, out2), dim=1)\n",
    "        \n",
    "        # Pass through meta-layer\n",
    "        final_output = self.meta_layer(combined)\n",
    "        return final_output\n",
    "\n",
    "# Now you need to train the meta-learner on a validation set\n",
    "def train_ensemble(ensemble_model, train_loader, val_loader, num_epochs=10, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()  # Change to appropriate loss function\n",
    "    # Only train the meta-layers\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, ensemble_model.parameters()), lr=0.001)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ensemble_model = ensemble_model.to(device)\n",
    "    \n",
    "    # Initialize early stopper\n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta=0.001)\n",
    "    \n",
    "    # Track best metrics\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs on {device}\")\n",
    "    print(f\"Early stopping patience: {patience}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"| Epoch | Train Loss | Val Loss  | Val Acc | Time (s) | Best Model |\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        ensemble_model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = ensemble_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print batch progress every 10 batches\n",
    "            if (i+1) % 10 == 0:\n",
    "                batch_loss = running_loss / batch_count\n",
    "                batch_acc = 100 * correct_train / total_train\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Batch {i+1}/{len(train_loader)}: Loss: {batch_loss:.4f}, Acc: {batch_acc:.2f}%\", end='\\r')\n",
    "        \n",
    "        # Calculate average training loss and accuracy\n",
    "        epoch_loss = running_loss / batch_count\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # Validation phase\n",
    "        ensemble_model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = ensemble_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        is_best = \"\"\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            is_best = \"✓ (loss)\"\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if is_best == \"\":\n",
    "                is_best = \"✓ (acc)\"\n",
    "            else:\n",
    "                is_best = \"✓ (both)\"\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"| {epoch+1:5d} | {epoch_loss:.6f} | {val_epoch_loss:.6f} | {val_acc:6.2f}% | {epoch_time:7.2f} | {is_best:10s} |\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        should_stop, new_best = early_stopper.check(val_epoch_loss, ensemble_model)\n",
    "        if new_best:\n",
    "            print(f\"New best model saved with validation loss: {val_epoch_loss:.6f}\")\n",
    "        \n",
    "        if should_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.')\n",
    "            break\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%, Best validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Load the best model\n",
    "    if os.path.exists('best_ensemble_model.pth'):\n",
    "        print(\"Loading best model weights...\")\n",
    "        ensemble_model.load_state_dict(torch.load('best_ensemble_model.pth'))\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "def predict_with_ensemble(ensemble_model, image):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ensemble_model = ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        output = ensemble_model(image)\n",
    "        \n",
    "        # For classification\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        # For regression\n",
    "        # predicted = output\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd174244-c9cd-4fbe-a3b5-0e207284fd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble = EnsembleModel(model_brightness_contrast, model_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc336236-488b-4ebb-a097-6231d4714afd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 100 epochs on cuda\n",
      "Early stopping patience: 5\n",
      "--------------------------------------------------------------------------------\n",
      "| Epoch | Train Loss | Val Loss  | Val Acc | Time (s) | Best Model |\n",
      "--------------------------------------------------------------------------------\n",
      "|     1 | 0.642540 | 0.706942 |  76.85% |  144.92 | ✓ (both)   |\n",
      "New best model saved with validation loss: 0.706942\n",
      "|     2 | 0.575266 | 0.698058 |  77.00% |  147.08 | ✓ (both)   |\n",
      "New best model saved with validation loss: 0.698058\n",
      "|     3 | 0.568090 | 0.709668 |  76.78% |  144.86 |            |\n",
      "|     4 | 0.565016 | 0.711150 |  76.84% |  147.74 |            |\n",
      "|     5 | 0.564080 | 0.699611 |  77.01% |  148.42 | ✓ (acc)    |\n",
      "|     6 | 0.562507 | 0.716086 |  76.87% |  145.89 |            |\n",
      "|     7 | 0.560734 | 0.704827 |  77.05% |  146.23 | ✓ (acc)    |\n",
      "Early stopping triggered at epoch 7. No improvement for 5 epochs.\n",
      "--------------------------------------------------------------------------------\n",
      "Training completed. Best validation accuracy: 77.05%, Best validation loss: 0.698058\n",
      "Loading best model weights...\n"
     ]
    }
   ],
   "source": [
    "ensemle_model = train_ensemble(ensemble, train_loader=trainloader_raw, val_loader=valloader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb47da-f862-4cb6-bf07-6c8fa15ce01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf7a2579-78ee-42c2-8650-27a0f0de9379",
   "metadata": {},
   "source": [
    "# Hierarchical stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07bbe449-fb98-4041-ad20-b3cdf1d1d9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch HIER val error: 0.7796308994293213, acc: 0.7232888888888889\n"
     ]
    }
   ],
   "source": [
    "# First, let's define the augmentation functions\n",
    "def apply_brightness_contrast(images, brightness_factor=1.2, contrast_factor=1.2):\n",
    "    \"\"\"Apply brightness and contrast augmentation to the images\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    augmented_images = images.clone()\n",
    "    # Apply brightness (multiply)\n",
    "    augmented_images = augmented_images * brightness_factor\n",
    "    # Apply contrast (subtract mean, multiply by contrast factor, add mean back)\n",
    "    mean = torch.mean(augmented_images, dim=[1, 2, 3], keepdim=True)\n",
    "    augmented_images = (augmented_images - mean) * contrast_factor + mean\n",
    "    # Clip values to be in valid range [0, 1]\n",
    "    augmented_images = torch.clamp(augmented_images, 0, 1)\n",
    "    return augmented_images\n",
    "\n",
    "# Define the weighted ensemble function\n",
    "def weighted_ensemble(images, model1, model2, weight1=0.5, weight2=0.5):\n",
    "    \"\"\"Perform weighted soft voting between two models\"\"\"\n",
    "    outputs1 = model1(images)\n",
    "    outputs2 = model2(images)\n",
    "    return outputs1 * weight1 + outputs2 * weight2\n",
    "\n",
    "# Define the hierarchical ensemble function\n",
    "def hierarchical_ensemble(images, model_brightness_contrast, model_rotation, \n",
    "                         bc_weight1=0.55, bc_weight2=0.45,\n",
    "                         final_weight_normal=0.5, final_weight_augmented=0.5):\n",
    "    \"\"\"\n",
    "    Implement hierarchical ensembling:\n",
    "    1. Soft voting for normal image\n",
    "    2. Soft voting for brightness/contrast-augmented image\n",
    "    3. Soft voting between results from steps 1 and 2\n",
    "    \"\"\"\n",
    "    # Step 1: Soft voting for normal image\n",
    "    normal_ensemble = weighted_ensemble(\n",
    "        images, \n",
    "        model_brightness_contrast, \n",
    "        model_rotation, \n",
    "        bc_weight1, \n",
    "        bc_weight2\n",
    "    )\n",
    "    \n",
    "    # Step 2: Apply brightness/contrast augmentation and perform soft voting\n",
    "    augmented_images = apply_brightness_contrast(images)\n",
    "    augmented_ensemble = weighted_ensemble(\n",
    "        augmented_images, \n",
    "        model_brightness_contrast, \n",
    "        model_rotation, \n",
    "        bc_weight1, \n",
    "        bc_weight2\n",
    "    )\n",
    "    \n",
    "    # Step 3: Combine predictions from normal and augmented images\n",
    "    final_ensemble = normal_ensemble * final_weight_normal + augmented_ensemble * final_weight_augmented\n",
    "    \n",
    "    return final_ensemble\n",
    "\n",
    "# Modified evaluation code\n",
    "val_error = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Use the hierarchical ensemble for predictions\n",
    "        outputs = hierarchical_ensemble(\n",
    "            images, \n",
    "            model_brightness_contrast, \n",
    "            model_rotation, \n",
    "            bc_weight1=0.5, \n",
    "            bc_weight2=0.5,\n",
    "            final_weight_normal=0.5,  \n",
    "            final_weight_augmented=0.5\n",
    "        )\n",
    "        \n",
    "        val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "        correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "    \n",
    "    val_error = val_error / len(valloader.dataset)\n",
    "    print(f'epoch HIER val error: {val_error}, acc: {correct/len(valloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755224fe-d851-48c9-8cd5-7ecb74c25ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to experiment with different weights for the final ensemble\n",
    "# Here's code to find optimal weights on the validation set\n",
    "def find_optimal_weights(valloader, model_brightness_contrast, model_rotation, device, criterion,\n",
    "                        bc_weight1=0.55, bc_weight2=0.45):\n",
    "    best_acc = 0\n",
    "    best_weights = (0.5, 0.5)\n",
    "    \n",
    "    for final_weight_normal in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        final_weight_augmented = 1.0 - final_weight_normal\n",
    "        val_error = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = hierarchical_ensemble(\n",
    "                    images, \n",
    "                    model_brightness_contrast, \n",
    "                    model_rotation, \n",
    "                    bc_weight1, \n",
    "                    bc_weight2,\n",
    "                    final_weight_normal,  \n",
    "                    final_weight_augmented\n",
    "                )\n",
    "                \n",
    "                val_error = val_error + criterion(outputs, labels) * images.size(0)\n",
    "                correct += (torch.argmax(outputs, 1) == labels).float().sum().item()\n",
    "            \n",
    "            val_error = val_error / len(valloader.dataset)\n",
    "            accuracy = correct / len(valloader.dataset)\n",
    "            \n",
    "            print(f'Weights normal={final_weight_normal:.1f}, aug={final_weight_augmented:.1f} - '\n",
    "                  f'val error: {val_error:.4f}, acc: {accuracy:.4f}')\n",
    "            \n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_weights = (final_weight_normal, final_weight_augmented)\n",
    "    \n",
    "    print(f'Best weights: normal={best_weights[0]:.1f}, aug={best_weights[1]:.1f}, accuracy: {best_acc:.4f}')\n",
    "    return best_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "dlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
